{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6514db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "#ham_spam csv\n",
    "\n",
    "ham_spam_pd = pd.read_csv('ham-spam.csv')\n",
    "    \n",
    "#read in wiki texts\n",
    "wiki_text = []\n",
    "path = './wikitext-2/'\n",
    "fileList = os.listdir(path)\n",
    "for i in fileList:\n",
    "    #Need a way to skip the blank lines\n",
    "    file = open(os.path.join('wikitext-2/'+ i), 'r')\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            wiki_text.append(line)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0217805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text)) #Type String\n",
    "print(type(ham_spam_pd)) #Type pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(len(wiki_text))\n",
    "for line in wiki_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Reusing code from assignment3\n",
    "def preprocessing(text_list: list[str]):\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Get rid of the stop words in the wiki_text\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "            \n",
    "            #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "    \n",
    "wiki_text_dataset: list[list[str]] = preprocessing(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text_dataset))\n",
    "for list in wiki_text_dataset:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the ham_spam dataframe\n",
    "import re\n",
    "def preprocess_dataframe(text):\n",
    "    regex = '[^A-Za-z0-9]+'\n",
    "    try:\n",
    "        if not isinstance(text,str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        #Tokenize the words, lower the words, and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if isinstance(word, str) and word.isalpha()]\n",
    "        tokens = [re.sub(regex,'', word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(\"Error processing {text}, Error: {e}\")\n",
    "\n",
    "ham_spam_pd[\"Text\"] = ham_spam_pd[\"Text\"].apply(preprocess_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df461e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e71a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply word2vec to the ham spam csv\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "#I need to apply this to the second column ONLY!!!\n",
    "#first column is just labels\n",
    "\n",
    "#I used the skip-gram model because the assignment didn't specify using CBOW or Skip-gram\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#function to get the vectors\n",
    "def get_word_vectors(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        #If the word can be found in the model, append it\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            #Otherwise, make it a 0\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    result = np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)\n",
    "    return result\n",
    "\n",
    "ham_spam_pd[\"average email embedding\"] = ham_spam_pd[\"Text\"].apply(lambda x: get_word_vectors(x, wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what the size of the input is\n",
    "embedding_lengths = ham_spam_pd[\"average email embedding\"].apply(len)\n",
    "print(f\"Total length of email word embeddings: {embedding_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the dataset where the independent variable is the average email embeddings and the dependent variable is the spam classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(ham_spam_pd[\"average email embedding\"], ham_spam_pd[\"IsSpam\"], test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "        \n",
    "def intialize_parameters(input_vector_size, hidden_size, output_size):\n",
    "    W1 = np.random.randn(input_vector_size,hidden_size)\n",
    "    b1 = np.zeros(1, input_vector_size)\n",
    "    W2 = np.random.rand(input_vector_size,hidden_size)\n",
    "    b2 = np.zeros(1, output_size)\n",
    "    \n",
    "def feedforward():\n",
    "    return\n",
    "def back_propagation():\n",
    "    return\n",
    "def train(X_train, y_train, input_size, hidden_size, output_size, epochs, learning_rate, dropout):\n",
    "    \n",
    "    return \n",
    "\n",
    "def prediction(X_test, w1, b1, w2, b2):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the network \n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 256\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "weight1, bias1, weight2, bias2 = train(X_train, y_train, input_size, hidden_size, output_size, 30, 0.0025,0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy():\n",
    "    accuracy = 0\n",
    "    return accuracy\n",
    "\n",
    "def model_precision():\n",
    "    precision = 0\n",
    "    return precision\n",
    "\n",
    "def model_recall():\n",
    "    recall = 0 \n",
    "    return recall\n",
    "\n",
    "def model_f1_score():\n",
    "    f1_score = 0\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985ce28",
   "metadata": {},
   "source": [
    "This is where the wiki-text dataset sent through a neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vocab for the wiki-text dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self, batch_size = 64, learning_rate = 0.01, epochs = 10):\n",
    "        self.batch = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epochs\n",
    "        \n",
    "    def softmax():\n",
    "        return 0\n",
    "    #Model training\n",
    "    def train_model():\n",
    "        return 0\n",
    "    #Finds the perplexity of the neural network   \n",
    "    def perplexity():\n",
    "        model_perplexity = 0\n",
    "        return model_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d54099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a78c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
