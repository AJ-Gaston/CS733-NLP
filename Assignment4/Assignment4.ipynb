{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4066c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6514db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "#ham_spam csv\n",
    "\n",
    "ham_spam_pd = pd.read_csv('ham-spam.csv')\n",
    "    \n",
    "#read in wiki texts\n",
    "wiki_text = []\n",
    "path = './wikitext-2/'\n",
    "fileList = os.listdir(path)\n",
    "for i in fileList:\n",
    "    #Need a way to skip the blank lines\n",
    "    file = open(os.path.join('wikitext-2/'+ i), 'r')\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            wiki_text.append(line)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0217805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text)) #Type String\n",
    "print(type(ham_spam_pd)) #Type pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(len(wiki_text))\n",
    "for line in wiki_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Reusing code from assignment3\n",
    "def preprocessing(text_list: list[str]):\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Get rid of the stop words in the wiki_text\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "            \n",
    "            #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "    \n",
    "wiki_text_dataset: list[list[str]] = preprocessing(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text_dataset))\n",
    "for list in wiki_text_dataset:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the ham_spam dataframe\n",
    "import re\n",
    "def preprocess_dataframe(text):\n",
    "    regex = '[^A-Za-z0-9]+'\n",
    "    try:\n",
    "        if not isinstance(text,str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        #Tokenize the words, lower the words, and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if isinstance(word, str) and word.isalpha()]\n",
    "        tokens = [re.sub(regex,'', word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(\"Error processing {text}, Error: {e}\")\n",
    "\n",
    "ham_spam_pd[\"Text\"] = ham_spam_pd[\"Text\"].apply(preprocess_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df461e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e71a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply word2vec to the ham spam csv\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "#I need to apply this to the second column ONLY!!!\n",
    "#first column is just labels\n",
    "\n",
    "#I used the skip-gram model because the assignment didn't specify using CBOW or Skip-gram\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#function to get the vectors\n",
    "def get_word_vectors(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        #If the word can be found in the model, append it\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            #Otherwise, make it a 0\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    result = np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)\n",
    "    return result\n",
    "\n",
    "ham_spam_pd[\"average email embedding\"] = ham_spam_pd[\"Text\"].apply(lambda x: get_word_vectors(x, wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what the size of the input is\n",
    "embedding_lengths = ham_spam_pd[\"average email embedding\"].apply(len)\n",
    "print(f\"Total length of email word embeddings: {embedding_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc48202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the dataset where the independent variable is the average email embeddings and the dependent variable is the spam classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(ham_spam_pd[\"average email embedding\"], ham_spam_pd[\"IsSpam\"], test_size=0.2,random_state=42)\n",
    "\n",
    "#I'm getting into issues here\n",
    "#Make it into a numpy array before splitting to make it easier\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        #Initialization of the weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        # Simple forward pass - no dropout initially\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU\n",
    "        \n",
    "        # DROPOUT HERE - Only to hidden layer (A1)\n",
    "        if training and self.dropout_rate > 0:\n",
    "            self.D1 = (np.random.rand(*self.A1.shape) > self.dropout_rate).astype(float)\n",
    "            self.A1 = self.A1 * self.D1  # Apply dropout mask\n",
    "            self.A1 = self.A1 / (1 - self.dropout_rate)  # Inverted dropout scaling\n",
    "        else:\n",
    "            self.D1 = np.ones_like(self.A1)  # No dropout during inference\n",
    "            \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid\n",
    "        return self.A2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Gradient calculations\n",
    "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        # APPLYING DROPOUT DURING BACKPROP\n",
    "        if self.dropout_rate > 0:\n",
    "            dA1 = dA1 * self.D1  # Only backprop through active neurons\n",
    "            dA1 = dA1 / (1 - self.dropout_rate)  # Scale gradients\n",
    "            \n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # ReLU derivative\n",
    "        \n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update with gradient clipping\n",
    "        for grad in [dW1, dW2]:\n",
    "            np.clip(grad, -1.0, 1.0, out=grad)\n",
    "        \n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean(-y_true * np.log(y_pred + 1e-8) - (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+10}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING MINIMAL NETWORK ===\n",
      "Epoch 10: Loss = 0.8895\n",
      "Epoch 20: Loss = 0.8072\n",
      "Epoch 30: Loss = 0.8563\n",
      "Epoch 40: Loss = 0.9113\n",
      "Epoch 50: Loss = 0.9662\n",
      "Test Accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "#Training the network \n",
    "X_train_normalized = (X_train - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "X_test_normalized = (X_test - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "\n",
    "input_size = X_train_normalized.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleNeuralNetwork(input_size, hidden_size, output_size, 0.2)\n",
    "\n",
    "#Training with dropout applied now\n",
    "losses = model.train(X_train_normalized, y_train, epochs=50, learning_rate=0.01)\n",
    "\n",
    "#Predict without dropout\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "accuracy = np.mean(y_pred.flatten() == y_test)\n",
    "print(f\"Test Accuracy With Dropout Applied to model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db5b67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Metrics:\n",
      "Accuracy:  87.00\n",
      "Precision: 91.49\n",
      "Recall:    82.69\n",
      "F1-Score:  86.87\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, F1\n",
    "tp = np.sum((y_test == 1) & (y_pred.flatten() == 1))\n",
    "fp = np.sum((y_test == 0) & (y_pred.flatten() == 1))\n",
    "fn = np.sum((y_test == 1) & (y_pred.flatten() == 0))\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Final Test Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy*100:.2f}\")\n",
    "print(f\"Precision: {precision*100:.2f}\")\n",
    "print(f\"Recall:    {recall*100:.2f}\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292643d",
   "metadata": {},
   "source": [
    "Model's performance \n",
    "The model's accuracy \n",
    "The precision means that the model can \n",
    "The recall means that this feedforward neural network is finding out almost all the spam compared to the non-spam emails.\n",
    "\n",
    "The F1-score of the model, which indicates the harmonic mean between model's precision and recall, means that the neural network model.can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985ce28",
   "metadata": {},
   "source": [
    "This is where the wiki-text dataset sent through a neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vocab for the wiki-text dataset\n",
    "#Assign a unique index to each word in the vocabulary.\n",
    "def build_dataset_vocav(wikitext_dataset):\n",
    "    #Keep a set of unique words in the wiki-text dataset\n",
    "    unique_words = set()\n",
    "    for sentence in wikitext_dataset:\n",
    "        for word in sentence:\n",
    "            unique_words.add(word) #This way, a set doesn't keep duplicates\n",
    "    \n",
    "    #Index the words in the dataset\n",
    "    word_to_index = {}\n",
    "    for i,word in enumerate(unique_words):\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "    #create one-hot encoding vectors for the neural network\n",
    "    one_hot_vectors = []\n",
    "     \n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self, batch_size = 64, learning_rate = 0.01, epochs = 10):\n",
    "        self.batch = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epochs\n",
    "        \n",
    "    def softmax():\n",
    "        return 0\n",
    "    \n",
    "    #Model training\n",
    "    def train_model(self, X_train, y_train,):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d54099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the perplexity of the neural network   \n",
    "def perplexity():\n",
    "    model_perplexity = 0\n",
    "    return model_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a78c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
