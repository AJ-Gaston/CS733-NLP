{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4066c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6514db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "#ham_spam csv\n",
    "\n",
    "ham_spam_pd = pd.read_csv('ham-spam.csv')\n",
    "    \n",
    "#read in wiki texts\n",
    "wiki_text = []\n",
    "path = './wikitext-2/'\n",
    "fileList = os.listdir(path)\n",
    "for i in fileList:\n",
    "    #Need a way to skip the blank lines\n",
    "    file = open(os.path.join('wikitext-2/'+ i), 'r')\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            wiki_text.append(line)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0217805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text)) #Type String\n",
    "print(type(ham_spam_pd)) #Type pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(len(wiki_text))\n",
    "for line in wiki_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Reusing code from assignment3\n",
    "def preprocessing(text_list: list[str]):\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Get rid of the stop words in the wiki_text\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "            \n",
    "            #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "    \n",
    "wiki_text_dataset: list[list[str]] = preprocessing(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text_dataset))\n",
    "for list in wiki_text_dataset:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the ham_spam dataframe\n",
    "import re\n",
    "def preprocess_dataframe(text):\n",
    "    regex = '[^A-Za-z0-9]+'\n",
    "    try:\n",
    "        if not isinstance(text,str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        #Tokenize the words, lower the words, and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if isinstance(word, str) and word.isalpha()]\n",
    "        tokens = [re.sub(regex,'', word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(\"Error processing {text}, Error: {e}\")\n",
    "\n",
    "ham_spam_pd[\"Text\"] = ham_spam_pd[\"Text\"].apply(preprocess_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df461e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e71a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply word2vec to the ham spam csv\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "#I need to apply this to the second column ONLY!!!\n",
    "#first column is just labels\n",
    "\n",
    "#I used the skip-gram model because the assignment didn't specify using CBOW or Skip-gram\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#function to get the vectors\n",
    "def get_word_vectors(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        #If the word can be found in the model, append it\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            #Otherwise, make it a 0\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    result = np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)\n",
    "    return result\n",
    "\n",
    "ham_spam_pd[\"average email embedding\"] = ham_spam_pd[\"Text\"].apply(lambda x: get_word_vectors(x, wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what the size of the input is\n",
    "embedding_lengths = ham_spam_pd[\"average email embedding\"].apply(len)\n",
    "print(f\"Total length of email word embeddings: {embedding_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc48202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the dataset where the independent variable is the average email embeddings and the dependent variable is the spam classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(ham_spam_pd[\"average email embedding\"], ham_spam_pd[\"IsSpam\"], test_size=0.2,random_state=42)\n",
    "\n",
    "#I'm getting into issues here\n",
    "#Make it into a numpy array before splitting to make it easier\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # He initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        # Simple forward pass - no dropout initially\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid\n",
    "        return self.A2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Gradient calculations\n",
    "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # ReLU derivative\n",
    "        \n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update with gradient clipping\n",
    "        for grad in [dW1, dW2]:\n",
    "            np.clip(grad, -1.0, 1.0, out=grad)\n",
    "        \n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean(-y_true * np.log(y_pred + 1e-8) - (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Gradient norm = 3.687358\n",
      "Weight change: 0.000387\n",
      "Epoch:1 | training_loss:1.424614480793975\n",
      "Epoch 1: Gradient norm = 7.181062\n",
      "Weight change: 0.000588\n",
      "Epoch:2 | training_loss:2.0530370217919645\n",
      "Epoch 2: Gradient norm = 10.123245\n",
      "Weight change: 0.000590\n",
      "Epoch:3 | training_loss:2.0741169496541785\n",
      "Epoch 3: Gradient norm = 6.772403\n",
      "Weight change: 0.002818\n",
      "Epoch:4 | training_loss:0.9329015911844468\n",
      "Epoch 4: Gradient norm = 2.091256\n",
      "Weight change: 0.001115\n",
      "Epoch:5 | training_loss:1.0040384277400725\n",
      "Epoch 5: Gradient norm = 1.082719\n",
      "Weight change: 0.000909\n",
      "Epoch:6 | training_loss:0.999381395054661\n",
      "Epoch 6: Gradient norm = 0.724196\n",
      "Weight change: 0.001109\n",
      "Epoch:7 | training_loss:1.057331860836463\n",
      "Epoch 7: Gradient norm = 0.609772\n",
      "Weight change: 0.001028\n",
      "Epoch:8 | training_loss:1.0935649063398913\n",
      "Epoch 8: Gradient norm = 0.563113\n",
      "Weight change: 0.001042\n",
      "Epoch:9 | training_loss:1.1396612403052986\n",
      "Epoch 9: Gradient norm = 0.532981\n",
      "Weight change: 0.001011\n",
      "Epoch:10 | training_loss:1.1805379624091388\n",
      "Epoch 10: Gradient norm = 0.501917\n",
      "Weight change: 0.000998\n",
      "Epoch:11 | training_loss:1.2233344468667144\n",
      "Epoch 11: Gradient norm = 0.460299\n",
      "Weight change: 0.000967\n",
      "Epoch:12 | training_loss:1.2617539812289063\n",
      "Epoch 12: Gradient norm = 0.433850\n",
      "Weight change: 0.000945\n",
      "Epoch:13 | training_loss:1.2989004732707052\n",
      "Epoch 13: Gradient norm = 0.404780\n",
      "Weight change: 0.000920\n",
      "Epoch:14 | training_loss:1.3339111292615529\n",
      "Epoch 14: Gradient norm = 0.383382\n",
      "Weight change: 0.000896\n",
      "Epoch:15 | training_loss:1.3667830601558764\n",
      "Epoch 15: Gradient norm = 0.364560\n",
      "Weight change: 0.000876\n",
      "Epoch:16 | training_loss:1.3986185802467723\n",
      "Epoch 16: Gradient norm = 0.347959\n",
      "Weight change: 0.000854\n",
      "Epoch:17 | training_loss:1.4289474522975576\n",
      "Epoch 17: Gradient norm = 0.334540\n",
      "Weight change: 0.000835\n",
      "Epoch:18 | training_loss:1.4580008350192362\n",
      "Epoch 18: Gradient norm = 0.323411\n",
      "Weight change: 0.000818\n",
      "Epoch:19 | training_loss:1.4864441778197441\n",
      "Epoch 19: Gradient norm = 0.311188\n",
      "Weight change: 0.000801\n",
      "Epoch:20 | training_loss:1.5141076385592962\n",
      "Epoch 20: Gradient norm = 0.301623\n",
      "Weight change: 0.000784\n",
      "Epoch:21 | training_loss:1.5411241831290377\n",
      "Epoch 21: Gradient norm = 0.291046\n",
      "Weight change: 0.000768\n",
      "Epoch:22 | training_loss:1.567308458968097\n",
      "Epoch 22: Gradient norm = 0.282050\n",
      "Weight change: 0.000753\n",
      "Epoch:23 | training_loss:1.5925029256872512\n",
      "Epoch 23: Gradient norm = 0.273272\n",
      "Weight change: 0.000739\n",
      "Epoch:24 | training_loss:1.6173746518728602\n",
      "Epoch 24: Gradient norm = 0.264946\n",
      "Weight change: 0.000724\n",
      "Epoch:25 | training_loss:1.641018662409048\n",
      "Epoch 25: Gradient norm = 0.257041\n",
      "Weight change: 0.000712\n",
      "Epoch:26 | training_loss:1.6645757677327004\n",
      "Epoch 26: Gradient norm = 0.249308\n",
      "Weight change: 0.000698\n",
      "Epoch:27 | training_loss:1.6871595711514107\n",
      "Epoch 27: Gradient norm = 0.242691\n",
      "Weight change: 0.000686\n",
      "Epoch:28 | training_loss:1.709106721805083\n",
      "Epoch 28: Gradient norm = 0.237595\n",
      "Weight change: 0.000675\n",
      "Epoch:29 | training_loss:1.7309172297151192\n",
      "Epoch 29: Gradient norm = 0.231299\n",
      "Weight change: 0.000664\n",
      "Epoch:30 | training_loss:1.752201891057715\n",
      "Epoch 30: Gradient norm = 0.225316\n",
      "Weight change: 0.000653\n",
      "Epoch:31 | training_loss:1.7728554394214837\n",
      "Epoch 31: Gradient norm = 0.219589\n",
      "Weight change: 0.000643\n",
      "Epoch:32 | training_loss:1.7929507195099068\n",
      "Epoch 32: Gradient norm = 0.214947\n",
      "Weight change: 0.000633\n",
      "Epoch:33 | training_loss:1.8126705259657359\n",
      "Epoch 33: Gradient norm = 0.209904\n",
      "Weight change: 0.000624\n",
      "Epoch:34 | training_loss:1.8319176495653435\n",
      "Epoch 34: Gradient norm = 0.205410\n",
      "Weight change: 0.000615\n",
      "Epoch:35 | training_loss:1.8507760020881128\n",
      "Epoch 35: Gradient norm = 0.201237\n",
      "Weight change: 0.000606\n",
      "Epoch:36 | training_loss:1.8692480233029507\n",
      "Epoch 36: Gradient norm = 0.197371\n",
      "Weight change: 0.000598\n",
      "Epoch:37 | training_loss:1.8872504008807864\n",
      "Epoch 37: Gradient norm = 0.193321\n",
      "Weight change: 0.000590\n",
      "Epoch:38 | training_loss:1.9047998033047844\n",
      "Epoch 38: Gradient norm = 0.189455\n",
      "Weight change: 0.000582\n",
      "Epoch:39 | training_loss:1.9220459885788357\n",
      "Epoch 39: Gradient norm = 0.186056\n",
      "Weight change: 0.000575\n",
      "Epoch:40 | training_loss:1.93904760733072\n",
      "Epoch 40: Gradient norm = 0.182621\n",
      "Weight change: 0.000568\n",
      "Epoch:41 | training_loss:1.9559505856067851\n",
      "Epoch 41: Gradient norm = 0.179314\n",
      "Weight change: 0.000561\n",
      "Epoch:42 | training_loss:1.9722016232412556\n",
      "Epoch 42: Gradient norm = 0.176398\n",
      "Weight change: 0.000555\n",
      "Epoch:43 | training_loss:1.9883517165050346\n",
      "Epoch 43: Gradient norm = 0.173415\n",
      "Weight change: 0.000548\n",
      "Epoch:44 | training_loss:2.004105199888688\n",
      "Epoch 44: Gradient norm = 0.170413\n",
      "Weight change: 0.000542\n",
      "Epoch:45 | training_loss:2.019738034098975\n",
      "Epoch 45: Gradient norm = 0.167858\n",
      "Weight change: 0.000536\n",
      "Epoch:46 | training_loss:2.0349864137825815\n",
      "Epoch 46: Gradient norm = 0.165384\n",
      "Weight change: 0.000531\n",
      "Epoch:47 | training_loss:2.0501239622848844\n",
      "Epoch 47: Gradient norm = 0.162945\n",
      "Weight change: 0.000525\n",
      "Epoch:48 | training_loss:2.0649893332669853\n",
      "Epoch 48: Gradient norm = 0.160587\n",
      "Weight change: 0.000520\n",
      "Epoch:49 | training_loss:2.079700892952855\n",
      "Epoch 49: Gradient norm = 0.158098\n",
      "Weight change: 0.000515\n",
      "Epoch:50 | training_loss:2.0942007851145834\n"
     ]
    }
   ],
   "source": [
    "#Training the network \n",
    "X_train_normalized = (X_train - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "X_test_normalized = (X_test - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "\n",
    "# 1. Create simple network\n",
    "input_size = X_train_normalized.shape[1]\n",
    "hidden_size = 64  # Start small\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# 2. Train with conservative parameters\n",
    "print(\"=== TRAINING MINIMAL NETWORK ===\")\n",
    "losses = model.train(X_train_normalized, y_train, epochs=50, learning_rate=0.01)\n",
    "\n",
    "# 3. Evaluate\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "accuracy = np.mean(y_pred.flatten() == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7db5b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_fp_tn_fn(y_true, y_prediction):\n",
    "    tp = np.sum((y_true == 1) & (y_prediction == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_prediction == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_prediction == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_prediction == 0))\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def accuracy(tp, fp, tn, fn):\n",
    "    \n",
    "    return ((tp + tn))/(tp + fp + tn + fn)\n",
    "\n",
    "def precision(tp, fp):\n",
    "    return tp /(tp + fp)\n",
    "    \n",
    "def recall(tp, fn):\n",
    "    return tp/ (tp + fn)\n",
    "\n",
    "def f1_score(p, r):\n",
    "    return (2 * p * r)/(p + r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e63f4714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL'S ACCURACY: 48.22\n",
      "MODEL'S PRECISION: 52.00\n",
      "MODEL'S RECALL: 5.50\n",
      "MODEL'S F1 SCORE: 9.95\n"
     ]
    }
   ],
   "source": [
    "true_positive, false_positive, true_negative, false_negative = compute_tp_fp_tn_fn(y_true, fnn_predict)\n",
    "model_accuracy = accuracy(true_positive, false_positive, true_negative, false_negative)\n",
    "model_precision = precision(true_positive, false_positive)\n",
    "model_recall = recall(true_positive, false_negative)\n",
    "model_f1_score = f1_score(model_precision, model_recall)\n",
    "\n",
    "print(f\"MODEL'S ACCURACY: {model_accuracy * 100:.2f}\")\n",
    "print(f\"MODEL'S PRECISION: {model_precision * 100:.2f}\")\n",
    "print(f\"MODEL'S RECALL: {model_recall * 100:.2f}\")\n",
    "print(f\"MODEL'S F1 SCORE: {model_f1_score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985ce28",
   "metadata": {},
   "source": [
    "This is where the wiki-text dataset sent through a neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vocab for the wiki-text dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self, batch_size = 64, learning_rate = 0.01, epochs = 10):\n",
    "        self.batch = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epochs\n",
    "        \n",
    "    def softmax():\n",
    "        return 0\n",
    "    #Model training\n",
    "    def train_model():\n",
    "        return 0\n",
    "    #Finds the perplexity of the neural network   \n",
    "    def perplexity():\n",
    "        model_perplexity = 0\n",
    "        return model_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d54099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a78c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
