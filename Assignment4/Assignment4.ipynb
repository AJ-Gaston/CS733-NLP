{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4066c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6514db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "#ham_spam csv\n",
    "\n",
    "ham_spam_pd = pd.read_csv('ham-spam.csv')\n",
    "    \n",
    "#read in wiki texts\n",
    "wiki_text = []\n",
    "path = './wikitext-2/'\n",
    "fileList = os.listdir(path)\n",
    "for i in fileList:\n",
    "    #Need a way to skip the blank lines\n",
    "    file = open(os.path.join('wikitext-2/'+ i), 'r')\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            wiki_text.append(line)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0217805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text)) #Type String\n",
    "print(type(ham_spam_pd)) #Type pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(len(wiki_text))\n",
    "for line in wiki_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Reusing code from assignment3\n",
    "def preprocessing(text_list: list[str]):\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Get rid of the stop words in the wiki_text\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "            \n",
    "            #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "    \n",
    "wiki_text_dataset: list[list[str]] = preprocessing(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text_dataset))\n",
    "for list in wiki_text_dataset:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the ham_spam dataframe\n",
    "import re\n",
    "def preprocess_dataframe(text):\n",
    "    regex = '[^A-Za-z0-9]+'\n",
    "    try:\n",
    "        if not isinstance(text,str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        #Tokenize the words, lower the words, and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if isinstance(word, str) and word.isalpha()]\n",
    "        tokens = [re.sub(regex,'', word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(\"Error processing {text}, Error: {e}\")\n",
    "\n",
    "ham_spam_pd[\"Text\"] = ham_spam_pd[\"Text\"].apply(preprocess_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df461e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsSpam</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[key, issues, going, forwarda, year, end, revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[congrats, contratulations, execution, central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[key, issues, going, forwardall, control, set,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[epmi, files, protest, entergy, transcoattache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[california, power, please, contact, kristin, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsSpam                                               Text\n",
       "0       0  [key, issues, going, forwarda, year, end, revi...\n",
       "1       0  [congrats, contratulations, execution, central...\n",
       "2       0  [key, issues, going, forwardall, control, set,...\n",
       "3       0  [epmi, files, protest, entergy, transcoattache...\n",
       "4       0  [california, power, please, contact, kristin, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e71a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply word2vec to the ham spam csv\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "#I need to apply this to the second column ONLY!!!\n",
    "#first column is just labels\n",
    "\n",
    "#I used the skip-gram model because the assignment didn't specify using CBOW or Skip-gram\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#function to get the vectors\n",
    "def get_word_vectors(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        #If the word can be found in the model, append it\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            #Otherwise, make it a 0\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "            \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)\n",
    "\n",
    "ham_spam_pd[\"average email embedding\"] = ham_spam_pd[\"Text\"].apply(lambda x: get_word_vectors(x, wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsSpam</th>\n",
       "      <th>Text</th>\n",
       "      <th>average_email_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[key, issues, going, forwarda, year, end, revi...</td>\n",
       "      <td>[-0.06715451346503364, 0.08746767044067383, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[congrats, contratulations, execution, central...</td>\n",
       "      <td>[0.024125382706925675, 0.01771401070259713, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[key, issues, going, forwardall, control, set,...</td>\n",
       "      <td>[-0.0274718948032545, 0.047499897169030234, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[epmi, files, protest, entergy, transcoattache...</td>\n",
       "      <td>[-0.013229049955095563, 0.02034743172781808, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[california, power, please, contact, kristin, ...</td>\n",
       "      <td>[0.018123513574629137, 0.04188768947220877, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsSpam                                               Text  \\\n",
       "0       0  [key, issues, going, forwarda, year, end, revi...   \n",
       "1       0  [congrats, contratulations, execution, central...   \n",
       "2       0  [key, issues, going, forwardall, control, set,...   \n",
       "3       0  [epmi, files, protest, entergy, transcoattache...   \n",
       "4       0  [california, power, please, contact, kristin, ...   \n",
       "\n",
       "                             average_email_embedding  \n",
       "0  [-0.06715451346503364, 0.08746767044067383, -0...  \n",
       "1  [0.024125382706925675, 0.01771401070259713, -0...  \n",
       "2  [-0.0274718948032545, 0.047499897169030234, -0...  \n",
       "3  [-0.013229049955095563, 0.02034743172781808, 0...  \n",
       "4  [0.018123513574629137, 0.04188768947220877, 0....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(ham_spam_pd[\"IsSpam\"], ham_spam_pd[\"average email embedding\"], test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class feedforward_neural_network:\n",
    "    def __init__(self, hidden_size = 256, dropout = 0.02, learning_rate = 0.001, epochs = 30):\n",
    "        #initial variables\n",
    "        self.hidden_layer_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        #Training Variables(set to change during the training model)\n",
    "        \n",
    "    def sigmoid(self, x:np.array):\n",
    "        return 1/(1+ np.exp(x))\n",
    "        \n",
    "    def feedforward(self, input_vector):\n",
    "        losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    \n",
    "    #This is to test the accuracy, precision, and f1 score of the model\n",
    "    def model_accuracy_score():\n",
    "        accuracy = 0\n",
    "        return accuracy\n",
    "    \n",
    "    def model_precision_score():\n",
    "        precision = 0\n",
    "        return precision\n",
    "    \n",
    "    def model_recall_score():\n",
    "        recall = 0\n",
    "        return recall\n",
    "    \n",
    "    def model_f1_score():\n",
    "        f1 = 0\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the network \n",
    "ff_nn = feedforward_neural_network()\n",
    "nn_results = ff_nn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985ce28",
   "metadata": {},
   "source": [
    "This is where the wiki-text dataset sent through a neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vocab for the wiki-text dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self, batch_size = 64, learning_rate = 0.01, epochs = 10):\n",
    "        self.batch = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epochs\n",
    "    \n",
    "    #Model training\n",
    "    def train_model():\n",
    "        return 0\n",
    "    #Finds the perplexity of the neural network   \n",
    "    def perplexity():\n",
    "        model_perplexity = 0\n",
    "        return model_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d54099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a78c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0773f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
