{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6514db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the datasets\n",
    "#ham_spam csv\n",
    "\n",
    "ham_spam_pd = pd.read_csv('ham-spam.csv')\n",
    "    \n",
    "#read in wiki texts\n",
    "wiki_text = []\n",
    "path = './wikitext-2/'\n",
    "fileList = os.listdir(path)\n",
    "for i in fileList:\n",
    "    #Need a way to skip the blank lines\n",
    "    file = open(os.path.join('wikitext-2/'+ i), 'r')\n",
    "    for line in file:\n",
    "        if not line.isspace():\n",
    "            wiki_text.append(line)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0217805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text)) #Type String\n",
    "print(type(ham_spam_pd)) #Type pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(len(wiki_text))\n",
    "for line in wiki_text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Reusing code from assignment3 (Altering it because there are numbers with words and it doesn't fully change numbers to words)\n",
    "def preprocessing(text_list: list[str]):\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text) #Get rid of punctuation completely\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Get rid of the stop words in the wiki_text\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            #If we find any digits folowed by words\n",
    "            if re.search(r'\\d+', word_lower):\n",
    "                #replace the digits with words\n",
    "                word_lower = re.sub(r'\\d+', \n",
    "                                  lambda x: p.number_to_words(x.group()), \n",
    "                                  word_lower)\n",
    "                converted_tokens = word_tokenize(word_lower)\n",
    "                for token in converted_tokens:\n",
    "                    # 'and' often appears in number conversions so I need to look for that\n",
    "                    if token not in stop_words and token not in ['and']:\n",
    "                        filtered.append(token)\n",
    "            else:\n",
    "                if word_lower not in stop_words:\n",
    "                    filtered.append(word_lower)\n",
    "            \n",
    "            #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "    \n",
    "wiki_text_dataset: list[list[str]] = preprocessing(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22115b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "print(type(wiki_text_dataset))\n",
    "for list in wiki_text_dataset:\n",
    "    print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the ham_spam dataframe\n",
    "import re\n",
    "def preprocess_dataframe(text):\n",
    "    regex = '[^A-Za-z0-9]+'\n",
    "    try:\n",
    "        if not isinstance(text,str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        #Tokenize the words, lower the words, and remove stopwords\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if isinstance(word, str) and word.isalpha()]\n",
    "        tokens = [re.sub(regex,'', word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(\"Error processing {text}, Error: {e}\")\n",
    "\n",
    "ham_spam_pd[\"Text\"] = ham_spam_pd[\"Text\"].apply(preprocess_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df461e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "ham_spam_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e71a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply word2vec to the ham spam csv\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "#I need to apply this to the second column ONLY!!!\n",
    "#first column is just labels\n",
    "\n",
    "#I used the skip-gram model because the assignment didn't specify using CBOW or Skip-gram\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#function to get the vectors\n",
    "def get_word_vectors(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        #If the word can be found in the model, append it\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "        else:\n",
    "            #Otherwise, make it a 0\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    result = np.mean(vectors, axis=0) if vectors else np.zeros(model.wv.vector_size)\n",
    "    return result\n",
    "\n",
    "ham_spam_pd[\"average email embedding\"] = ham_spam_pd[\"Text\"].apply(lambda x: get_word_vectors(x, wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK Check to see what the size of the input is\n",
    "embedding_lengths = ham_spam_pd[\"average email embedding\"].apply(len)\n",
    "print(f\"Total length of email word embeddings: {embedding_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the dataset where the independent variable is the average email embeddings and the dependent variable is the spam classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(ham_spam_pd[\"average email embedding\"], ham_spam_pd[\"IsSpam\"], test_size=0.2,random_state=42)\n",
    "\n",
    "#I'm getting into issues here\n",
    "#Make it into a numpy array before splitting to make it easier\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        #Initialization of the weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        # Simple forward pass - no dropout initially\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU\n",
    "        \n",
    "        # DROPOUT HERE - Only to hidden layer (A1)\n",
    "        if training and self.dropout_rate > 0:\n",
    "            self.D1 = (np.random.rand(*self.A1.shape) > self.dropout_rate).astype(float)\n",
    "            self.A1 = self.A1 * self.D1  # Apply dropout mask\n",
    "            self.A1 = self.A1 / (1 - self.dropout_rate)  # Inverted dropout scaling\n",
    "        else:\n",
    "            self.D1 = np.ones_like(self.A1)  # No dropout during inference\n",
    "            \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))  # Sigmoid\n",
    "        return self.A2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Gradient calculations\n",
    "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        # APPLYING DROPOUT DURING BACKPROP\n",
    "        if self.dropout_rate > 0:\n",
    "            dA1 = dA1 * self.D1  # Only backprop through active neurons\n",
    "            dA1 = dA1 / (1 - self.dropout_rate)  # Scale gradients\n",
    "            \n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # ReLU derivative\n",
    "        \n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update with gradient clipping\n",
    "        for grad in [dW1, dW2]:\n",
    "            np.clip(grad, -1.0, 1.0, out=grad)\n",
    "        \n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean(-y_true * np.log(y_pred + 1e-8) - (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+10}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the network \n",
    "X_train_normalized = (X_train - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "X_test_normalized = (X_test - np.mean(X_train)) / (np.std(X_train) + 1e-8)\n",
    "\n",
    "input_size = X_train_normalized.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleNeuralNetwork(input_size, hidden_size, output_size, 0.2)\n",
    "\n",
    "#Training with dropout applied now\n",
    "losses = model.train(X_train_normalized, y_train, epochs=50, learning_rate=0.01)\n",
    "\n",
    "#Predict without dropout\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "accuracy = np.mean(y_pred.flatten() == y_test)\n",
    "print(f\"Test Accuracy With Dropout Applied to model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F1\n",
    "tp = np.sum((y_test == 1) & (y_pred.flatten() == 1))\n",
    "fp = np.sum((y_test == 0) & (y_pred.flatten() == 1))\n",
    "fn = np.sum((y_test == 1) & (y_pred.flatten() == 0))\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Final Test Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy*100:.2f}\")\n",
    "print(f\"Precision: {precision*100:.2f}\")\n",
    "print(f\"Recall:    {recall*100:.2f}\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292643d",
   "metadata": {},
   "source": [
    "Model's performance \n",
    "The model's accuracy \n",
    "The precision means that the model can \n",
    "The recall means that this feedforward neural network is finding out almost all the spam compared to the non-spam emails.\n",
    "\n",
    "The F1-score of the model, which indicates the harmonic mean between model's precision and recall, means that the neural network model.can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985ce28",
   "metadata": {},
   "source": [
    "This is where the wiki-text dataset sent through a neural network for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vocab for the wiki-text dataset\n",
    "#Assign a unique index to each word in the vocabulary.\n",
    "def build_dataset_vocab(wikitext_dataset):\n",
    "    #Keep a set of unique words in the wiki-text dataset\n",
    "\n",
    "    unique_words = set()\n",
    "    for sentence in wikitext_dataset:\n",
    "        for word in sentence:\n",
    "            unique_words.add(word) #This way, a set doesn't keep duplicates\n",
    "    \n",
    "    #Index the words in the dataset (both ways word-index, index-word)\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for i,word in enumerate(sorted(unique_words)):\n",
    "        word_to_index[word] = i\n",
    "        index_to_word[i] = word\n",
    "    \n",
    "    return word_to_index,index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SANITY CHECK\n",
    "word_index, index_word = build_dataset_vocab(wiki_text_dataset)\n",
    "vocab = len(word_index)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need a way to prepare the dataset where I have the 3 previous words and the target word\n",
    "def prepare_dataset_for_nn(wiki_text_dataset, word_to_index):\n",
    "    sequences = []\n",
    "    targets = [] #store as indices instead of one-hot\n",
    "    \n",
    "    #Go through eahc list in the dataset\n",
    "    for sentence in wiki_text_dataset:\n",
    "        #If it's greater than 4, then get the 3 context words with the target\n",
    "        if len(sentence) >= 4:\n",
    "            for i in range(len(sentence) - 3):\n",
    "                context = sentence[i:i+3]   #Get the words are i, i+1, i+2\n",
    "                target = sentence[i+3]      #get the target word at i+3\n",
    "                \n",
    "                #Convert the words into numerical indices with the word index created above\n",
    "                context_indices = [word_to_index.get(word,0) for word in context] #Use get() for unknown words\n",
    "                target_indices = word_to_index.get(target, 0)\n",
    "                \n",
    "                sequences.append(context_indices)\n",
    "                targets.append(target_indices)\n",
    "                \n",
    "    #Convert these into numpy arrays for the neural network\n",
    "    X = np.array(sequences) #Shape should be (n_samples, 3)\n",
    "    y = np.array(targets) #Should be (n_samples) instead of (n_samples,vocab)\n",
    "    \n",
    "    #make the one-hot encoded targets(word indices to binary vector) --> works, but the kernel dies because it's too large!\n",
    "    #y = np.zeros((len(targets), len(word_to_index)))\n",
    "    #y[np.arange(len(targets)), targets] = 1 #Shape should be (n_samples, vocab size)\n",
    "                \n",
    "            \n",
    "    return X,y\n",
    "\n",
    "X,y_indices = prepare_dataset_for_nn(wiki_text_dataset,word_index)\n",
    "print(f\"X shape: {X.shape}\")         \n",
    "print(f\"y shape: {y_indices.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ff9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train_indices,y_test_indices = train_test_split(X,y_indices,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "class word_predictor_neural_network:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size= 64, learning_rate = 0.01, epochs = 10):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        #EMBEDING of the neural network\n",
    "        self.W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01 #Make it the size of the vocabulary and the embedding dimension\n",
    "        \n",
    "        #HIDDEN LAYER of the neural network\n",
    "        #Make a hidden layer where the input is 3 * the embedding dimentsion after concatenation\n",
    "        self.W1 = np.random.randn(3 * embedding_dim, hidden_dim) * 0.01\n",
    "        #biases of the hidden layer\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        \n",
    "        #OUTPUT LAYER of the neural network\n",
    "        #The output weights are the hidden dimension and the size of the vocab\n",
    "        self.W2 = np.random.rand(hidden_dim, vocab_size) * 0.01\n",
    "        #Biases of the output weight where it's the size of the dataset's vocab\n",
    "        self.b2 = np.zeros((1,vocab_size))\n",
    "    \n",
    "    def one_hot_encoding(self, X_indices):\n",
    "        #Convert batch of word indices into one-hot vectors\n",
    "        batch_size = X_indices.shape[0]\n",
    "        X_one_hot = np.zeros((batch_size, 3, self.vocab_size))\n",
    "        #Go through the batches\n",
    "        for i in range(batch_size):\n",
    "            for j in range(3):\n",
    "                word_idx = X_indices[i,j]\n",
    "                X_one_hot[i, j, word_idx] = 1\n",
    "        return X_one_hot\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x,axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, y, probs):\n",
    "        #Compute the loss function(cross-entrpy)\n",
    "        #Add 1e-8 for numerical stability with np.log()\n",
    "        return -np.mean(np.sum(y * np.log(probs + 1e-8), axis=1))\n",
    "    \n",
    "    def backpropagation(self, X_indices, y, probs):\n",
    "        batch_size = X_indices.shape[0]\n",
    "        \n",
    "        #Calculate the gradients for the output layer\n",
    "        dz2 = probs - y #dL/dz2\n",
    "        dW2 = np.dot(self.a1.T, dz2)/batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        #Backpropagation for the hidden layer\n",
    "        da1 = np.dot(dz2, self.W2.T) #Dot product of dz and the output weights(transposed)\n",
    "        dz1 = da1 * (self.z1 > 0) #This is necessary for the ReLU activation\n",
    "        dW1 = np.dot(self.concatenated.T, dz1)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)/batch_size\n",
    "        \n",
    "        #Backpropagation through the concatenated and embedded layer\n",
    "        d_concatenated = np.dot(dz1, self.W1.T) #dL/d(concatenated)\n",
    "        d_embeddings = d_concatenated.reshape(batch_size, 3,self.embedding_dim)\n",
    "        \n",
    "        #Backprop through the embedding layer to update W_embd\n",
    "        X_one_hot = self.one_hot_encoding(X_indices)\n",
    "        #Zero initially\n",
    "        dW_embed =  np.zeros_like(self.W_embed)\n",
    "        \n",
    "        #Gradient for the embedding matrix of the one-hot vectors and embedding gradient\n",
    "        for i in range(batch_size):\n",
    "            for j in range(3):\n",
    "                dW_embed = dW_embed + np.outer(X_one_hot[i,j], d_embeddings[i,j])\n",
    "                \n",
    "        dW_embed = dW_embed/batch_size\n",
    "        \n",
    "        #Update the weights and the biases(parameters)\n",
    "        self.W_embed = self.W_embed - self.learning_rate * dW_embed\n",
    "        self.W1 = self.W1 - self.learning_rate * dW1\n",
    "        self.b1 = self.b1 - self.learning_rate * db1\n",
    "        self.W2 = self.W2 - self.learning_rate * dW2\n",
    "        self.b2 = self.b2 - self.learning_rate * db2\n",
    "        return \n",
    "    \n",
    "    def forward(self, X_indices):\n",
    "        batch_size = X_indices.shape[0]\n",
    "        \n",
    "        #Get the one-hot encoding for the forward feeding\n",
    "        X_one_hot = self.one_hot_encoding(X_indices)\n",
    "        \n",
    "        #Embedding Layer - multiplying the one-hot vector with the embedding dimension\n",
    "        #Each one-hot vector (1,vocab_size) * (vocab_size,embedding_dim) = (1,embedding_dim)\n",
    "        #make the embeddings which is the batch\n",
    "        self.embeddings = np.zeros((batch_size, 3, self.embedding_dim))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(3):\n",
    "                # Matrix multiplication: one-hot vector * embedding matrix\n",
    "                self.embeddings[i, j] = np.dot(X_one_hot[i, j], self.W_embed)\n",
    "                \n",
    "        #Concatenate the 3 embeddings into a single vector\n",
    "        self.concatenated = self.embeddings.reshape(batch_size, -1) #Shape should be [batch_size, 3 * embedding_dim]\n",
    "        \n",
    "        #Hidden layer with ReLU\n",
    "        #dot product of the hidden weights and the concatenated vectors + the bias of the hidden layer\n",
    "        self.z1 = np.dot(self.concatenated, self.W1) + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)  # ReLU activation\n",
    "        \n",
    "        # Output Layer with softmax\n",
    "        #dot product of the activation and the output weights with the bias of output\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        #Use the softmax function above to get the best probability\n",
    "        probs = self.softmax(self.z2)\n",
    "        return probs\n",
    "    \n",
    "    #Model training\n",
    "    def train_model(self, X, y_indices):\n",
    "        losses = []\n",
    "        #Get n_samples, X's 1\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        #Now training with target indices instead of one-hot vectors\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            num_batches = 0\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                end_idx = min(i + self.batch_size, n_samples)\n",
    "                X_batch = X[i:end_idx]\n",
    "                y_batch_indices = y_indices[i:end_idx]\n",
    "                \n",
    "                # Convert ONLY this batch to one-hot (much smaller memory footprint)\n",
    "                y_batch_onehot = np.zeros((len(X_batch), self.vocab_size))\n",
    "                y_batch_onehot[np.arange(len(X_batch)), y_batch_indices] = 1\n",
    "                \n",
    "                #Forward pass\n",
    "                probs = self.forward(X_batch)\n",
    "                batch_loss = self.compute_loss(y_batch_onehot, probs)\n",
    "                \n",
    "                # Calculate accuracy with the batches\n",
    "                pred_indices = np.argmax(probs, axis=1)\n",
    "                batch_accuracy = np.mean(pred_indices == y_batch_indices)\n",
    "                \n",
    "                epoch_loss += batch_loss\n",
    "                epoch_accuracy += batch_accuracy\n",
    "                num_batches += 1\n",
    "            \n",
    "                #backpropagation\n",
    "                self.backpropagation(X_batch, y_batch_onehot, probs)\n",
    "                \n",
    "            #Compute th eaverage loss by the epoch / number of batches\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            #Compute the average accuracy with the epoch accuracy / batches\n",
    "            avg_accuracy = epoch_accuracy/ num_batches\n",
    "            #append the average loss to the list of losses\n",
    "            losses.append(avg_loss)\n",
    "            print(f\"EPOCH: {epoch +1} | Training Loss = {avg_loss:.4f} | Average Accuracy = {avg_accuracy:.4f} \")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def prediction(self, X_test_indices, index_to_word, top_k=1):\n",
    "        if len(X_test.shape) == 1:\n",
    "            X_test_indices = X_test_indices.reshape(1,-1)\n",
    "            \n",
    "        all_probs = self.forward(X_test_indices)\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i in range(len(X_test_indices)):\n",
    "            probs = all_probs[i]\n",
    "            top_k_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "            predictions = [(index_to_word[idx], probs[idx]) for idx in top_k_indices]\n",
    "            all_predictions.append(predictions)\n",
    "        #If the input was single context\n",
    "        if len(all_predictions) == 1:\n",
    "            return all_predictions[0] #Return the first eleemnt of all_predictions\n",
    "        else:\n",
    "            return all_predictions #Return all the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the perplexity of the neural network   \n",
    "def calculate_cross_entropy_loss(y_true, y_pred_probs):\n",
    "    \"\"\"Calculate the cross entropy loss for the y_true against the y_prediction probabilities\"\"\"\n",
    "    #For each sample, -sum(y_true * log(y_pred)) \n",
    "    #add 1e-8 for numerical stability(small values)\n",
    "    losses = -np.sum(y_true * np.log(y_pred_probs+1e-8), axis=1)\n",
    "    #Get teh average loss out of all the losses calculated \n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_perplexity(word_pred_model, X_test, y_test_indices, vocab_size):\n",
    "    \"\"\" Calculate the perplexity of the neural network model\"\"\"\n",
    "    #Get the probabilities from the model's forward function using the target indices\n",
    "    batch_size = 1000\n",
    "    n_samples = X_test.shape[0]\n",
    "    total_loss = 0\n",
    "    \n",
    "    #Process this in batches to avoid memory issues\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        X_batch = X_test[i:end_idx]\n",
    "        y_batch_indices = y_test_indices[i:end_idx]\n",
    "        \n",
    "        #Convert the batches into one-hot\n",
    "        y_batch_onehot = np.zeros((len(X_batch), vocab_size))\n",
    "        y_batch_onehot[np.arange(len(X_batch)), y_batch_indices] = 1\n",
    "        \n",
    "        #Forward pass\n",
    "        probs = word_pred_model.forward(X_batch)\n",
    "        batch_loss = calculate_cross_entropy_loss(y_batch_onehot, probs)\n",
    "        total_loss += batch_loss * (end_idx - i)\n",
    "    \n",
    "    #Compute the average loss by the total loss/ n-samples\n",
    "    avg_loss = total_loss / n_samples\n",
    "    model_perplexity = np.exp(avg_loss)\n",
    "    return model_perplexity, avg_loss\n",
    "\n",
    "def calculate_accuracy(model, X, y_indices):\n",
    "    \"\"\"\n",
    "    Calculate accuracy using the target indices\n",
    "    \"\"\"\n",
    "    # Get predictions for all test examples\n",
    "    probs = model.forward(X)\n",
    "    pred_indices = np.argmax(probs, axis=1)\n",
    "    accuracy = np.mean(pred_indices == y_indices)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the words through the model\n",
    "wp_model = word_predictor_neural_network(vocab_size=vocab,embedding_dim=50, hidden_dim=128)\n",
    "losses = wp_model.train_model(X_train, y_train_indices)\n",
    "\n",
    "# Calculate perplexity and accuracy\n",
    "test_perplexity, test_loss = calculate_perplexity(wp_model, X_test, y_test_indices, vocab_size=vocab)\n",
    "test_accuracy = calculate_accuracy(wp_model, X_test, y_test_indices)\n",
    "\n",
    "print(\"=== NEURAL NETWORK WORD PREDICTION EVALUATION ===\")\n",
    "print(f\"Test Cross-Entropy Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030bdbb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
