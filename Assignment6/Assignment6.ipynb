{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0642c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class Index                                              Title  \\\n",
      "0            3                  Fears for T N pension after talks   \n",
      "1            4  The Race is On: Second Private Team Sets Launc...   \n",
      "2            4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
      "3            4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
      "4            4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
      "\n",
      "                                         Description  \n",
      "0  Unions representing workers at Turner   Newall...  \n",
      "1  SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
      "2  AP - A company founded by a chemistry research...  \n",
      "3  AP - It's barely dawn when Mike Fitzpatrick st...  \n",
      "4  AP - Southern California's smog-fighting agenc...  \n",
      "Index(['Class Index', 'Title', 'Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Reading in the AG News Dataset\n",
    "import glob\n",
    "import pandas as pd\n",
    "ag_news = glob.glob('./archive-2/*.csv')\n",
    "#Make an empty list\n",
    "df_list = []\n",
    "#Convert the files into pandas dataframe\n",
    "for file in ag_news:\n",
    "    df = pd.read_csv(file)\n",
    "    #Append the dataframe to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "#Use pandas concat function to make all the dataframes into one\n",
    "ag_news_pd = pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "#Sanity Check\n",
    "print(ag_news_pd.head())\n",
    "print(ag_news_pd.columns)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02b8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the two columns (Title and Description)together\n",
    "ag_news_pd['Text'] = ag_news_pd['Title'].astype(str) + \" \" + ag_news_pd['Description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eacb035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#This is a function to tokenize the dataset\n",
    "def word_tokenizer(text):\n",
    "    #use nltk's word tokenize function\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = ['[CLS]'] + tokens  # Add CLS token at position 0\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pad_truncate(text,max_sentence_length=128, pad_token=\"<pad>\"):\n",
    "    #If the sentence is over 128\n",
    "    if len(text) > max_sentence_length:\n",
    "        text = text[:max_sentence_length]\n",
    "    #If the sentence is less then or equal to 128, pad it it with the pad_token\n",
    "    return text + [pad_token] * (max_sentence_length - len(text))\n",
    "\n",
    "#This is where the attention masking occurs\n",
    "def create_attention_mask(sentence, pad_token=\"<pad>\"):\n",
    "    #Casual or masked attention(from the textbook)\n",
    "    mask = []\n",
    "    for token in sentence:\n",
    "        if token == pad_token:\n",
    "            mask.append(0)\n",
    "        else:\n",
    "            mask.append(1)\n",
    "    return mask\n",
    "\n",
    "#Apply it to every line in the dataset\n",
    "ag_news_pd['Text'] = ag_news_pd['Text'].apply(word_tokenizer)\n",
    "\n",
    "#Apply the pad_truncation\n",
    "ag_news_pd['Text'] = ag_news_pd['Text'].apply(pad_truncate)\n",
    "\n",
    "#after applying the word_tokenizing and padding/truncating\n",
    "#Apply the attention masks for the words\n",
    "ag_news_pd['Attention Mask'] = ag_news_pd['Text'].apply(create_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb6f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary from ALL data\n",
    "all_tokens = []\n",
    "for token_list in ag_news_pd['Text']:\n",
    "    # Filter out <pad> tokens for vocabulary building\n",
    "    real_tokens = [t for t in token_list if t != '<pad>']\n",
    "    all_tokens.extend(real_tokens)\n",
    "\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Define special tokens with their CORRECT IDs from the start\n",
    "vocab = {\n",
    "    '<pad>': 0,    # MUST be 0 for padding\n",
    "    '<unk>': 1,    # MUST be 1 for unknown tokens  \n",
    "    '[CLS]': 2     # MUST be 2 for classification token\n",
    "}\n",
    "\n",
    "# Remove special tokens from counter if they exist\n",
    "# (Sometimes they appear as regular tokens too)\n",
    "for special_token in ['<pad>', '<unk>', '[CLS]']:\n",
    "    if special_token in counter:\n",
    "        del counter[special_token]\n",
    "\n",
    "# Sort by frequency (most common first) for better learning\n",
    "sorted_tokens = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "# Add regular tokens starting from ID 3\n",
    "current_id = 3\n",
    "for token, count in sorted_tokens:\n",
    "    vocab[token] = current_id\n",
    "    current_id += 1\n",
    "\n",
    "vocab_size = current_id  # current_id is already next available ID\n",
    "\n",
    "#Function to convert tokens to ids for encoder\n",
    "def convert_to_ids(token_list):\n",
    "    ids = []\n",
    "    for token in token_list:\n",
    "        if token == '[CLS]':\n",
    "            ids.append(2)  # Use the CORRECT ID\n",
    "        elif token in vocab:\n",
    "            ids.append(vocab[token])\n",
    "        else:\n",
    "            ids.append(1)  # <unk>\n",
    "    return ids\n",
    "\n",
    "# Apply to dataframe\n",
    "ag_news_pd['Input IDs'] = ag_news_pd['Text'].apply(convert_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ce7596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Text</th>\n",
       "      <th>Attention Mask</th>\n",
       "      <th>Input IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>[[CLS], Fears, for, T, N, pension, after, talk...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 4464, 13, 954, 7822, 1924, 43, 204, 5408, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>[[CLS], The, Race, is, On, :, Second, Private,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 22, 3285, 28, 515, 29, 2704, 5010, 1061, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>[[CLS], Ky., Company, Wins, Grant, to, Study, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 13990, 1222, 1072, 7894, 6, 1523, 60362, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3                  Fears for T N pension after talks   \n",
       "1            4  The Race is On: Second Private Team Sets Launc...   \n",
       "2            4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Unions representing workers at Turner   Newall...   \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...   \n",
       "2  AP - A company founded by a chemistry research...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  [[CLS], Fears, for, T, N, pension, after, talk...   \n",
       "1  [[CLS], The, Race, is, On, :, Second, Private,...   \n",
       "2  [[CLS], Ky., Company, Wins, Grant, to, Study, ...   \n",
       "\n",
       "                                      Attention Mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           Input IDs  \n",
       "0  [2, 4464, 13, 954, 7822, 1924, 43, 204, 5408, ...  \n",
       "1  [2, 22, 3285, 28, 515, 29, 2704, 5010, 1061, 3...  \n",
       "2  [2, 13990, 1222, 1072, 7894, 6, 1523, 60362, 1...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A way to see what the dataframe looks like now\n",
    "ag_news_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f44d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe and convert them into tensors for PyTorch\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "def dataframe_to_tensors(X_df, y_series):\n",
    "    \"\"\"\n",
    "    This converts a pandas dataframe/series (of lists) to a pytorch tensor \n",
    "    Args:\n",
    "        X_df (Pandas dataframe): The dataframe of attention mask and input ids\n",
    "        y_series (pandas Series): The categories of each news\n",
    "    \"\"\"\n",
    "    input_ids = torch.tensor(X_df['Input IDs'].tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(X_df['Attention Mask'].tolist(), dtype=torch.long)\n",
    "    labels = torch.tensor(y_series.tolist(), dtype=torch.long).sub(1) #I need this because PyTorch wants 0-index, not 1-index\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "#But I need to convert the training and test sets to tensors for PyTorch library\n",
    "#Split the dataframe into a train/test set (80-20)\n",
    "X = ag_news_pd[[\"Attention Mask\", \"Input IDs\"]]\n",
    "y = ag_news_pd[\"Class Index\"]\n",
    "X_train_df, X_test_df, y_train_series,y_test_series= train_test_split(X,y,test_size=0.2,random_state=42, stratify=y)\n",
    "\n",
    "#Convert to Torch tensor\n",
    "train_input_ids, train_attention_mask, train_labels = dataframe_to_tensors(X_train_df, y_train_series)\n",
    "test_input_ids, test_attention_mask, test_labels = dataframe_to_tensors(X_test_df, y_test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "# Making multiple classes instead of one class for better debugging\n",
    "\n",
    "# MultiHead Attention Class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        #Linear projections\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 4. Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(1), float('-inf'))\n",
    "        \n",
    "        # 5. Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout_layer(attn_weights)\n",
    "        \n",
    "        # 6. Apply attention to values\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # 7. Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # 8. Output projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        return output\n",
    "\n",
    "# FeedForward Class (deals with the feed forward)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=256, d_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# ENCODER LAYER (FIXED) (issues with the layer normalization)\n",
    "class FixedEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=4, d_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output = self.attention(x_norm, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        x_norm = self.norm2(x)\n",
    "        ff_output = self.feed_forward(x_norm)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#POSITIONAL ENCODING class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=256, max_len=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# MAIN TRANSFORMER ENCODER (Uses the other classes)\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes=4, d_model=256, \n",
    "                 num_heads=4, d_ff=512, num_layers=3, \n",
    "                 max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            FixedEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Embeddings\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Initialize ALL linear layers properly\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if 'classifier' in name and '3' in name:  # Last layer\n",
    "                    # ISSUE HERE: initialize last layer to near-zero\n",
    "                    nn.init.normal_(module.weight, mean=0.0, std=0.001)\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                else:\n",
    "                    # Other linear layers\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embedding(input_ids)  # [batch, seq_len, d_model]\n",
    "        x = x * math.sqrt(self.d_model)  # Scale embeddings\n",
    "        \n",
    "        # Positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Prepare mask (True positions to mask)\n",
    "        if attention_mask is not None:\n",
    "            mask = (attention_mask == 0)  # True for padding tokens\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Get CLS token (position 0)\n",
    "        cls_token = x[:, 0, :]  # [batch, d_model]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Use Tensor Dataset and DataLoader for pytorch\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 34,423,684\n",
      "Training samples: 102,080\n",
      "Test samples: 25,520\n",
      "\n",
      "Starting training...\n",
      "  Epoch 1, Batch 500: loss=0.1449, batch_acc=93.8%\n",
      "  Epoch 1, Batch 1000: loss=0.4266, batch_acc=81.2%\n",
      "  Epoch 1, Batch 1500: loss=0.6616, batch_acc=78.1%\n",
      "  Epoch 1, Batch 2000: loss=0.2362, batch_acc=93.8%\n",
      "  Epoch 1, Batch 2500: loss=0.1581, batch_acc=93.8%\n",
      "  Epoch 1, Batch 3000: loss=0.2239, batch_acc=93.8%\n",
      "\n",
      "Epoch 1/5:\n",
      "  Train - Loss: 0.3451, Acc: 88.03%\n",
      "  Val   - Loss: 0.2857, Acc: 91.27%\n",
      "   New best model saved (Acc: 91.27%)\n",
      "  Epoch 2, Batch 500: loss=0.0308, batch_acc=100.0%\n",
      "  Epoch 2, Batch 1000: loss=0.6186, batch_acc=87.5%\n",
      "  Epoch 2, Batch 1500: loss=0.7995, batch_acc=84.4%\n",
      "  Epoch 2, Batch 2000: loss=0.0823, batch_acc=96.9%\n",
      "  Epoch 2, Batch 2500: loss=0.1263, batch_acc=93.8%\n",
      "  Epoch 2, Batch 3000: loss=0.3156, batch_acc=87.5%\n",
      "\n",
      "Epoch 2/5:\n",
      "  Train - Loss: 0.1769, Acc: 94.57%\n",
      "  Val   - Loss: 0.2393, Acc: 92.30%\n",
      "   New best model saved (Acc: 92.30%)\n",
      "  Epoch 3, Batch 500: loss=0.0003, batch_acc=100.0%\n",
      "  Epoch 3, Batch 1000: loss=0.0248, batch_acc=100.0%\n"
     ]
    }
   ],
   "source": [
    "#TRAINING THE TRANSFORMER\n",
    "model = SimpleTransformerEncoder(\n",
    "    vocab_size=len(vocab),\n",
    "    num_classes=4,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    d_ff=512,\n",
    "    num_layers=3,\n",
    "    max_len=128,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"Training samples: {len(train_input_ids):,}\")\n",
    "print(f\"Test samples: {len(test_input_ids):,}\")\n",
    "\n",
    "# Adam optimizer with good settings\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0005,  # Good for transformers\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "num_epochs = 5\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_correct = 0\n",
    "    epoch_train_total = 0\n",
    "    \n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (between 1.0 and 0.1)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        epoch_train_correct += (preds == labels).sum().item()\n",
    "        epoch_train_total += labels.size(0)\n",
    "        \n",
    "        # Progress every 500 batches\n",
    "        if batch_idx % 500 == 0 and batch_idx > 0:\n",
    "            batch_acc = 100. * (preds == labels).sum().item() / labels.size(0)\n",
    "            print(f\"  Epoch {epoch+1}, Batch {batch_idx}: \"\n",
    "                  f\"loss={loss.item():.4f}, batch_acc={batch_acc:.1f}%\")\n",
    "    \n",
    "    train_epoch_loss = epoch_train_loss / len(train_loader)\n",
    "    train_epoch_acc = 100. * epoch_train_correct / epoch_train_total\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accs.append(train_epoch_acc)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_correct = 0\n",
    "    epoch_val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            epoch_val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            epoch_val_correct += (preds == labels).sum().item()\n",
    "            epoch_val_total += labels.size(0)\n",
    "    \n",
    "    val_epoch_loss = epoch_val_loss / len(test_loader)\n",
    "    val_epoch_acc = 100. * epoch_val_correct / epoch_val_total\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accs.append(val_epoch_acc)\n",
    "    \n",
    "    #EPOCH SUMMARY\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train - Loss: {train_epoch_loss:.4f}, Acc: {train_epoch_acc:.2f}%\")\n",
    "    print(f\"  Val   - Loss: {val_epoch_loss:.4f}, Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "    #Update the best accuracy (was stopping at one)\n",
    "    if val_epoch_acc > best_val_acc:\n",
    "        best_val_acc = val_epoch_acc\n",
    "        patience_counter = 0\n",
    "        print(f\"   New best model saved (Acc: {val_epoch_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement for {patience_counter}/{patience} epochs\")\n",
    "    \n",
    "    # Early stopping (WITH PATIENCE NOW)\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "    #If accuracy plateaus high enough, can stop (This always stop at one epoch tho)\n",
    "    #if val_epoch_acc > 90.0:\n",
    "        #print(f\"\\nReached >90% accuracy!\")\n",
    "        #break\n",
    "\n",
    "#Looks to see what was the best validation for ALL epochs\n",
    "print(f\"\\nCompleted {epoch+1} epochs\")\n",
    "print(f\"Best accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ddb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model with testloader\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.19%\n",
      "Precision: 92.23%\n",
      "Recall:    92.19%\n",
      "F1-Score:  92.21%\n",
      "\n",
      "23,527/25,520 correct\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "#Convert Tensors back to numpy arrays\n",
    "import numpy as np\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Accuracy\n",
    "correct = (all_preds == all_labels).sum()\n",
    "total = len(all_labels)\n",
    "accuracy = 100. * correct / total\n",
    "\n",
    "# Calculate precision, recall, f1 (macro)\n",
    "def calculate_metrics(labels, preds):\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    \n",
    "    for i in range(4):\n",
    "        tp = ((labels == i) & (preds == i)).sum()\n",
    "        fp = ((labels != i) & (preds == i)).sum()\n",
    "        fn = ((labels == i) & (preds != i)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        precision_sum += precision\n",
    "        recall_sum += recall\n",
    "    \n",
    "    precision_avg = precision_sum / 4\n",
    "    recall_avg = recall_sum / 4\n",
    "    f1_avg = 2 * precision_avg * recall_avg / (precision_avg + recall_avg) if (precision_avg + recall_avg) > 0 else 0\n",
    "    \n",
    "    return precision_avg, recall_avg, f1_avg\n",
    "\n",
    "precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall:    {recall*100:.2f}%\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{correct:,}/{total:,} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3deb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a PyPlot figure showing training and validation loss/accuracy, Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plotting for training and validation loss\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "#Make a subplot\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "epochs_trained = range(1, len(train_losses) + 1)\n",
    "\n",
    "#Add the epochs trained, the training and validation loss with markers and label\n",
    "ax1.plot(epochs_trained, train_losses, 'b-', marker='o', linewidth=2, label='Training Loss')\n",
    "ax1.plot(epochs_trained, val_losses, 'r-', marker='s', linewidth=2, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(epochs_trained)\n",
    "\n",
    "# Add best epoch marker\n",
    "if len(val_losses) > 0:\n",
    "    best_epoch = val_losses.index(min(val_losses))\n",
    "    ax1.plot(best_epoch + 1, val_losses[best_epoch], 'g*', markersize=15, \n",
    "             label=f'Best Epoch {best_epoch+1}')\n",
    "    ax1.legend()\n",
    "\n",
    "#Repeat the same process with training and validation accuracy\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "ax2.plot(epochs_trained, train_accs, 'b-', marker='o', linewidth=2, label='Training Accuracy')\n",
    "ax2.plot(epochs_trained, val_accs, 'r-', marker='s', linewidth=2, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training & Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.set_xticks(epochs_trained)\n",
    "\n",
    "# Add best epoch marker for accuracy\n",
    "if len(val_accs) > 0:\n",
    "    best_acc_epoch = val_accs.index(max(val_accs))\n",
    "    ax2.plot(best_acc_epoch + 1, val_accs[best_acc_epoch], 'g*', markersize=15,\n",
    "             label=f'Best: {val_accs[best_acc_epoch]:.1f}%')\n",
    "    ax2.legend()\n",
    "    \n",
    "#Make a scatter plot to show loss vs accuracy\n",
    "# 3. Loss vs Accuracy (scatter)\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "scatter = ax3.scatter(train_losses, train_accs, c=epochs_trained, cmap='viridis', \n",
    "                      s=100, alpha=0.7, edgecolors='black')\n",
    "ax3.set_xlabel('Training Loss')\n",
    "ax3.set_ylabel('Training Accuracy (%)')\n",
    "ax3.set_title('Loss vs Accuracy (Training)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "#Confusion Matrix\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot as heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax4)\n",
    "ax4.set_xlabel('Predicted')\n",
    "ax4.set_ylabel('True')\n",
    "ax4.set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335778d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 Setting Up a Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
