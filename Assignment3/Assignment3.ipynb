{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6367b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "import csv\n",
    "with open('transcripts.csv', encoding='latin-1') as TED_transcript:\n",
    "    transcript_csv = csv.reader(TED_transcript)\n",
    "    next(transcript_csv)\n",
    "    transcripts = list(transcript_csv)\n",
    "    \n",
    "#Converting the csv file to a list of strings (get rid of the url) and eliminating the second column\n",
    "transcript_data: list[str] = [t[0] for t in transcripts]\n",
    "#Sanity check\n",
    "#print(type(transcript_data)) prints out list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bdd843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data and Corpus Creation\n",
    "import re\n",
    "import inflect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text_list(text_list):\n",
    "    \"\"\"\n",
    "    Preprocesses text in a list of string\n",
    "    Args:\n",
    "        text_list(list[str]): a list of string\n",
    "\n",
    "    Returns:\n",
    "        processed_text(list[list(str)])): a list of string preprocessed for concatenation into a single corpus\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        #use re.sub to get rid pf punctuations and replace hem with whitespace\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        #tokenize the words\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #get rid fo stop words using NLTK's stop word list and convert numbers('10') into words ('ten')\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "        \n",
    "        #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "\n",
    "#creates a list of list of tokenized words\n",
    "preprocessed_corpus: list[list[str]] = preprocess_text_list(transcript_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY CHECK!!!!\n",
      "Type of corpus: <class 'list'>\n",
      "Type of first element: <class 'list'>\n",
      "Number of sentences: 2467\n",
      "First 2 sentences:\n",
      "  Sentence 0: ['good', 'morning', 'laughter', 'great', 'blown', 'away', 'whole', 'thing']...\n",
      "  Sentence 1: ['thank', 'much', 'chris', 'truly', 'great', 'honor', 'opportunity', 'come']...\n"
     ]
    }
   ],
   "source": [
    "#SANITY CHECK THIS IS OPTIONAL \n",
    "print(\"SANITY CHECK!!!!\")\n",
    "print(f\"Type of corpus: {type(preprocessed_corpus)}\")\n",
    "print(f\"Type of first element: {type(preprocessed_corpus[0])}\")\n",
    "print(f\"Number of sentences: {len(preprocessed_corpus)}\")\n",
    "print(f\"First 2 sentences:\")\n",
    "for i, sentence in enumerate(preprocessed_corpus[:2]):\n",
    "    print(f\"  Sentence {i}: {sentence[:8]}...\")  # First 8 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec (using Logistic regression) - Turn it into a class since there seems to be a lot going on from the textbook\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#This for the randomization\n",
    "np.random.seed(42)\n",
    "\n",
    "class word2vec:\n",
    "    #initialize the embedding dimension, window size, negative samples, learning rate, minimium count of word, and number of epochs\n",
    "    def __init__(self, embedding_dim = 100, window_size = 5, negative_samples = 10, learning_rate = 0.025, min_count = 1, epoch = 10):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epoch\n",
    "        \n",
    "        #Training variables (originally None (set during the training corpus))\n",
    "        self.vocab = None \n",
    "        self.word_to_index = None \n",
    "        self.index_to_word = None\n",
    "        self.W_input = None\n",
    "        self.W_output = None\n",
    "        self.noise_words = None\n",
    "        self.noise_prob = None\n",
    "        \n",
    "    def build_vocabulary(self, corpus:list[list[str]]):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary of the corpus (index words and count the frequency of the words)\n",
    "        Args:\n",
    "            corpus (list[list[str]]): a list of list of strings containing tokenized text of ted talk transcripts\n",
    "        \"\"\"\n",
    "        word_count = Counter()\n",
    "        #Count every word that appears in a sentence in the corpus of transcripts\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        #Get the vocab of the corpus (even though min_count is 1, still need to handle empty words)\n",
    "        self.vocab = [word for word, count in word_count.items() if count >= self.min_count]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        #Create a mapping of words (index to a word, word to an index)\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "        self.word_freq = {word: word_count[word] for word in self.vocab}\n",
    "        \n",
    "    def compute_noise_distribution(self):\n",
    "        \"\"\"Compute the noise distribution for the words (negative sampling)\"\"\"\n",
    "        #Get the sum of the word frequencies\n",
    "        total = sum(self.word_freq.values())\n",
    "        \n",
    "        #Create a dictionary of noise distribution to each word\n",
    "        noise_distribution = {}\n",
    "        \n",
    "        #This is from 6.8.2 Learn skip-gram embeddings (page 124-125)\n",
    "        for word in self.vocab:\n",
    "            prob = (self.word_freq[word]/total) ** 0.75\n",
    "            noise_distribution[word] = prob\n",
    "            \n",
    "        #Normalize the distribution\n",
    "        total_prob = sum(noise_distribution.values()) #Total probability\n",
    "        self.noise_words = list(noise_distribution.keys()) #A list of words from the nosie distribution dictionary\n",
    "        self.noise_prob = [prob/total_prob for prob in noise_distribution.values()] #The probability for each word/total probability\n",
    "    \n",
    "    def init_embeddings(self):\n",
    "        \"\"\"Initializing the embedding matrices (input and output)\n",
    "        \"\"\"\n",
    "        limit = 0.5/self.embedding_dim #Small values to avoid saturation\n",
    "        self.W_input = np.random.uniform(-limit, limit, (self.vocab_size, self.embedding_dim))\n",
    "        self.W_output = np.random.uniform(-limit, limit, (self.vocab_size, self.embedding_dim))\n",
    "    \n",
    "    def sigmoid(self, x:np.array):\n",
    "        \"\"\"\n",
    "        The sigmoid function of logistic regression (neceassary for word2vec)\n",
    "        Args:\n",
    "            x (ndarray): numpy array of dot product of the target word and the context word\n",
    "\n",
    "        Returns:\n",
    "            the sigmoid function of x \n",
    "        \"\"\"\n",
    "        #Might have large and negative values(ADD LATER IF THAT OCCURS!!)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def negative_sample(self):\n",
    "        \"\"\"sample negative word\"\"\"\n",
    "        return np.random.choice(self.noise_words, p=self.noise_prob)\n",
    "    \n",
    "    def get_context_words(self, sentence:list[str], center_pos):\n",
    "        #get the context words from the corpus\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentence (list[str]) : a list containing the target words and context words\n",
    "        Returns:\n",
    "            context (list): a list containing the context words that surround the target word in a sentence\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        #Get the 5 words before the context word\n",
    "        start = max(0, center_pos - self.window_size)\n",
    "        #Get the 5 words after the context word\n",
    "        end = min(len(sentence), center_pos + self.window_size + 1)\n",
    "        \n",
    "        for pos in range(start,end):\n",
    "            #If the position is not in the center position and the word is in the vocabulary (within the index))\n",
    "            if pos != center_pos and sentence[pos] in self.word_to_index:\n",
    "                #Add the word to the context list\n",
    "                context.append(sentence[pos])\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def update_embeddings(self, center_word:str, context_word:str, label:int):\n",
    "        \"\"\"\n",
    "        Update the embeddings for one pair of words\n",
    "        Args:\n",
    "            center_word (string): the center/target word in the sentence\n",
    "            context_word (string):the word surrounding the center/target word\n",
    "            label (int): the groundtruth of the word pairings\n",
    "        \n",
    "        Return\n",
    "            error ** 2\n",
    "        \"\"\"\n",
    "        #Get the index of the center word and the context word\n",
    "        center_index = self.word_to_index[center_word]\n",
    "        context_index = self.word_to_index[context_word]\n",
    "        \n",
    "        #create the vectors for the dot product\n",
    "        v_center = self.W_input[center_index] #vector for the center/target word\n",
    "        u_context = self.W_output[context_index] #vector for the context word\n",
    "        \n",
    "        dot_product = np.dot(v_center, u_context)\n",
    "        prediction = self.sigmoid(dot_product)\n",
    "        error = label - prediction\n",
    "        \n",
    "        #Update the embedding matrices\n",
    "        self.W_input[center_index] = self.W_input[center_index] + self.learning_rate * error * u_context\n",
    "        self.W_output[context_index] = self.W_output[context_index] + self.learning_rate * error * v_center\n",
    "        \n",
    "        return error ** 2\n",
    "    \n",
    "    def train(self, corpus:list[list[str]]):\n",
    "        \"\"\"\n",
    "        This function trains the model using the corpus created in the previous cell\n",
    "        Args:\n",
    "            corpus (list[list[str]]): The entire tokenized ted talk transcripts\n",
    "        \"\"\"\n",
    "        #Build the vocabulary from the corpus\n",
    "        self.build_vocabulary(corpus)\n",
    "        \n",
    "        #Compute the noise distribution for the words\n",
    "        self.compute_noise_distribution()\n",
    "        \n",
    "        #initialize the embedding matrices\n",
    "        self.init_embeddings()\n",
    "        \n",
    "        #All the losses during the training\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            #total loss during the epoch training\n",
    "            total_loss = 0\n",
    "            pairs = 0\n",
    "            \n",
    "            #Look at eahc sentence in the corpus\n",
    "            for sentence in corpus:\n",
    "                #Look for the center word and center position in the sentence\n",
    "                for center_pos, center_word in enumerate(sentence):\n",
    "                    #If that word isn not in the vocab\n",
    "                    if center_pos not in self.word_to_index:\n",
    "                        continue #Skip to the next iteration\n",
    "                \n",
    "                    context_words = self.get_context_words(sentence, center_pos) #get the context words in the sentecne\n",
    "\n",
    "                    #Positive samples (context words)\n",
    "                    for context_word in context_words:\n",
    "                        loss = self.update_embeddings(center_word, context_word, 1) #Make it 1 bc they're a pair\n",
    "                        total_loss += loss\n",
    "                        pairs += 1\n",
    "                        \n",
    "                    #Negative samples (random words in the lexicon)\n",
    "                    for _ in range(self.negative_samples):\n",
    "                        #Grab a negative word from the negative sample function\n",
    "                        negative_word = self.negative_sample()\n",
    "                        \n",
    "                        #Make sure the negative word chosen at random is not the context word\n",
    "                        if negative_word != context_word:\n",
    "                            loss = self.update_embeddings(center_word, negative_word, 0) #make it 0 bc the two don't appear together\n",
    "                            total_loss += loss\n",
    "                            pairs += 1\n",
    "                            \n",
    "            avg_loss = total_loss/pairs if pairs > 0 else 0 #get the average loss (total loss divided by number of pairs(if pairs > 0))\n",
    "            losses.append(avg_loss) #append the average loss to the list of losses\n",
    "            \n",
    "            #Decay the learning rate\n",
    "            self.learning_rate *= 0.99\n",
    "        \n",
    "        #Return everything for word2vec analysis (Includ word index and index to word for sanity check)\n",
    "        return {\n",
    "            'word_vectors': self.W_input,\n",
    "            'vocabulary': self.vocab,\n",
    "            'word_to_index': self.word_to_idx,\n",
    "            'index_to_word': self.idx_to_word,\n",
    "            'training_losses': losses\n",
    "        }\n",
    "        \n",
    "    # Added this to get the vector of a certain word (Didn't have this originally)\n",
    "    def get_vector(self, word):\n",
    "        \"\"\"\n",
    "        A helper function to get the vector of a word for cosine similarity\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            return self.W_input[self.word_to_index[word]]\n",
    "        return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "#Co-occurence matrix creation\n",
    "def create_cooccurence_matrix(corpus: list[list[str]], window_size = 5):\n",
    "    \"\"\"\n",
    "    Creates a co-occurrence matrix from the corpus with a default window size of 5\n",
    "    Args:\n",
    "        sentence (list[str]): a list of strings \n",
    "        window_size (int, optional): The size of the context window around each word. Defaults to 5.\n",
    "        \n",
    "    Returns\n",
    "        cooccurrence_matrix (np.array): a  2D co-occurrence matrix of all the words in the corpus\n",
    "        vocab (lis[str]): a list of all the words in the corpus\n",
    "        word_to_index (dict): a dictionary mapping every word to matrix index\n",
    "    \"\"\"\n",
    "    #Create a default dictionary of word countds\n",
    "    word_counts = defaultdict(int)\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            word_counts[word] += 1\n",
    "    \n",
    "    #Make a sorted list of words from the word_counts\n",
    "    vocab = sorted(word_counts.keys())\n",
    "    #Map every word to an index in the vocab list\n",
    "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "    #Make a voacb size for the co-occurrence matrix\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    #Initialize an empty co-occurrence matrix (make it the the size of the vocab (32 int to be safe))\n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    #Fill in the co-occurence matrix with the words in the corpus\n",
    "    for sentence in corpus:\n",
    "        #Look for the center word in the sentenxe\n",
    "        for center_pos, center_word in enumerate(sentence):\n",
    "            #If it's not there, then skip and go to the next one\n",
    "            if center_word not in word_to_index:\n",
    "                continue\n",
    "            center_index = word_to_index[center_word]\n",
    "            \n",
    "            #Start with the max of 0 or the center position - the window size defined above\n",
    "            start = max(0, center_pos - window_size)\n",
    "            #End with the minimum of the sentence length or the center position + window size + 1\n",
    "            end = min(len(sentence), center_pos + window_size + 1)\n",
    "            \n",
    "            for context_pos in range(start,end):\n",
    "                #Check to see if the context word is the center/target word\n",
    "                if context_pos == center_pos:\n",
    "                    continue\n",
    "                \n",
    "                #Check to see if the context word is in the word_to_index\n",
    "                context_word = sentence[context_pos]\n",
    "                if context_word not in word_to_index:\n",
    "                    continue #Skip if it's not in the word_to_index\n",
    "                \n",
    "                context_index = word_to_index[context_word]\n",
    "                \n",
    "                #Symmetric co-occurrence (both directions need to be +1)\n",
    "                cooccurrence_matrix[center_index, context_index] += 1\n",
    "                cooccurrence_matrix[context_index, center_index] += 1\n",
    "                \n",
    "    return cooccurrence_matrix, vocab, word_to_index\n",
    "\n",
    "def pmi(cooccurence_matrix: np.array, smoothing = 0.1):\n",
    "    \"\"\"\n",
    "    This computes the pointwise mutual information of the corpus\n",
    "    \n",
    "    Arguments:\n",
    "        cooccurrence_matrix(np.array): a  2D co-occurrence matrix of all the words in the corpus\n",
    "        smoothing(float, optional): Smoothing factor to avoid the log(0)\n",
    "    Returns:\n",
    "        pmi_matrix: a 2D numpy array holding all the pointwise mutual information of the corpus words\n",
    "    \"\"\"\n",
    "    vocab_size = cooccurence_matrix.shape[0]\n",
    "    \n",
    "    #smooth the matrix to avoid zero division (make it into a float to add the smoothing)\n",
    "    smoothed_matrix = cooccurence_matrix.astype(np.float64) + smoothing\n",
    "    \n",
    "    #compute the total co-occurrences\n",
    "    total_cooccurrences = np.sum(smoothed_matrix)\n",
    "    \n",
    "    #Get the joint probability P(word_i, word_j)\n",
    "    Prob_joint = smoothed_matrix/total_cooccurrences\n",
    "    \n",
    "    #Get the marginal probabilities (P(word_i) and P(word_j))\n",
    "    P_row = np.sum(joint_prob,axis=1) #P(word_i)\n",
    "    P_col = np.sum(joint_prob,axis=0) #P(word_j)\n",
    "    \n",
    "    #Initialize the pmi matrix with vocab_size\n",
    "    pmi_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    #Go through the vocab size\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if cooccurence_matrix[i,j] > 0: #Look for word that actually co-occur\n",
    "                #Get the joint probability of word i and j\n",
    "                joint_prob = Prob_joint[i,j]\n",
    "                #Get the product of the marginal probabilities\n",
    "                marginal_product = P_row[i] * P_col[j]\n",
    "                \n",
    "                #If the marginal product > 0, add the log(joint porbability/marginal_product) to the pmi_matrix\n",
    "                #Otherwise, the pmi_value is 0 for word i and j\n",
    "                if marginal_product > 0:\n",
    "                    pmi_value = math.log(joint_prob/marginal_product)\n",
    "                    pmi_matrix[i,j] = pmi_value\n",
    "                    \n",
    "    return pmi_matrix\n",
    "def ppmi(pmi_matrix):\n",
    "    \"\"\"\n",
    "    This compute the positive pointwise mutual information of the corpus (taking information from pmi)\n",
    "    \n",
    "    Arguments:\n",
    "        pmi_matrix(np.array): a 2D numpy array containing the pmi_values for words that co-occur in the co-occurrence matrix\n",
    "        \n",
    "        Returns:\n",
    "        ppmi_matrix(np.array): a 2D numpy array with negative PMI values set to 0\n",
    "    \"\"\"\n",
    "    ppmi_matrix = np.maximum(0,pmi_matrix)\n",
    "    \n",
    "    return ppmi_matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def term_frequency(sentence: list[str]):\n",
    "    \"\"\"\n",
    "    Returns the term frequency in a document\n",
    "    Args:\n",
    "        sentence (list[str]): the document where the word frequency is counted\n",
    "\n",
    "    Returns:\n",
    "        tf_dict (dictionary): a dictionary that has the word as the key and the times it appears in the document/sentence  as a value\n",
    "    \"\"\"\n",
    "    total_terms = len(sentence)\n",
    "    term_counts = Counter(sentence)\n",
    "    \n",
    "    tf_dict = {}\n",
    "    for word, count in term_counts.items():\n",
    "        tf_dict[word] = count/total_terms\n",
    "        \n",
    "    return tf_dict\n",
    "\n",
    "def inverse_doc_freq(doc_freq, corpus: list[list[str]]):\n",
    "    \"\"\"Returns the inverse document frequency\n",
    "    \n",
    "    Args:\n",
    "        doc_freq (dictionary): a dictionary that contains the word and the number of documents the word appears in\n",
    "        corpus (list[list[str]]): this is the entire corpus containing all the transcripts\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    idf_scores (dictionary): a dictionary of word to its idf score\n",
    "    \"\"\"\n",
    "    idf_scores = {} #Creating a dictionary\n",
    "    total_docs = len(corpus) #This is because we want to have the total number of documents\n",
    "    for word, freq in doc_freq.items():\n",
    "        idf = math.log(total_docs/ (freq +1))\n",
    "        idf_scores[word] = idf\n",
    "        \n",
    "    return idf_scores\n",
    "\n",
    "def compute_document_frequency(corpus:list[list[str]]):\n",
    "    \"\"\"\n",
    "    A helper function to count how many documents contain each word\n",
    "    \n",
    "    Args:\n",
    "        corpus (list[list[str]]): the entire corpus/transcripts\n",
    "\n",
    "    Returns:\n",
    "        doc_freq(Counter): a counter that has every word in each document\n",
    "    \"\"\"\n",
    "    doc_freq = Counter()\n",
    "    \n",
    "    #iterate through every document in the corpus\n",
    "    for document in corpus:\n",
    "        \n",
    "        #We don't want to count the same word over and over again, so make it a set since it doesn't contain duplicates\n",
    "        unique_word = set(document)\n",
    "        #Adds the number of counts in the document frequency counter\n",
    "        doc_freq.update(unique_word)\n",
    "        \n",
    "    return doc_freq\n",
    "\n",
    "def tfidf(corpus:list[list[str]]):\n",
    "    \"\"\" \n",
    "    This computes the entire tf-idf of the corpus\n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "    A TF-IDF matrix representing each word's importance to the entire corpus\n",
    "    \"\"\"\n",
    "    #Make a vocab set\n",
    "    vocab = set()\n",
    "    \n",
    "    for document in corpus:\n",
    "        vocab.update(document)\n",
    "    vocab = sorted(vocab)\n",
    "    \n",
    "    #Get the document frequency fomr the corpus\n",
    "    doc_freq = compute_document_frequency(corpus)\n",
    "    \n",
    "    #Get the idf scores using doc_freq and corpus\n",
    "    idf_scores = inverse_doc_freq(doc_freq, corpus)\n",
    "    \n",
    "    #creating a matrix to hold the tf-idf of eveyr word in the corpus\n",
    "    tfidf_matrix = []\n",
    "    \n",
    "    for document in corpus:\n",
    "        \n",
    "        #Get the term frequency score of each document\n",
    "        tf_scores = term_frequency(document)\n",
    "        \n",
    "        #Compute the tfidf for every word in the corpus\n",
    "        doc_tfidf = {}\n",
    "        for word in vocab:\n",
    "            if word in tf_scores:\n",
    "                \n",
    "                #TF-IDF = TF(word) * IDF(word)\n",
    "                doc_tfidf[word] = tf_scores[word] * idf_scores[word]\n",
    "                \n",
    "            else:\n",
    "                #If the word doesn't appear, make it 0\n",
    "                doc_tfidf[word] = 0\n",
    "    \n",
    "        tfidf_matrix.append(doc_tfidf)\n",
    "    \n",
    "    return tfidf_matrix, vocab, idf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually train the models\n",
    "\n",
    "#Word2vec\n",
    "word2vec_model = word2vec(embedding_dim=100,window_size=5,negative_samples=10, learning_rate=0.025, min_count=1, epoch=30)\n",
    "word2vec_results = word2vec_model.train(preprocessed_corpus)\n",
    "\n",
    "#PPMI\n",
    "\n",
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select word pairs\n",
    "word_pair1 = set((\"technology\", \"innovation\"), (\"learning\", \"fun\"))\n",
    "word_pair2 = set((\"future\", \"artificial\"),(\"intelligence\", \"machine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6276b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Analysis\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e5d7",
   "metadata": {},
   "source": [
    "Analysis from Heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
