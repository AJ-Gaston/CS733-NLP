{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6367b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "import csv\n",
    "with open('transcripts.csv', encoding='latin-1') as TED_transcript:\n",
    "    transcript_csv = csv.reader(TED_transcript)\n",
    "    transcripts = list(transcript_csv)\n",
    "    \n",
    "#Converting the csv file to a list of strings (get rid of the url) and eliminating the second column\n",
    "transcript_data = [t[0] for t in transcripts]\n",
    "#Sanity check\n",
    "#print(type(transcript_data)) prints out list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd843d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LazyCorpusLoader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m----> 7\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_text_list\u001b[39m(text_list):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Preprocesses text in a list of string\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        processed_text(list[list(str)])): a list of string preprocessed for concatenation into a single corpus\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LazyCorpusLoader' object is not callable"
     ]
    }
   ],
   "source": [
    "#Preprocessing the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text_list(text_list):\n",
    "    \"\"\"\n",
    "    Preprocesses text in a list of string\n",
    "    Args:\n",
    "        text_list(list[str]): a list of string\n",
    "\n",
    "    Returns:\n",
    "        processed_text(list[list(str)])): a list of string preprocessed for concatenation into a single corpus\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        #use re.sub to get rid pf punctuations \n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "        #tokenize the words\n",
    "        tokens = word_tokenize(text)\n",
    "        #get rid fo stop words using NLTK's stop word list\n",
    "        filtered = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "preprocessed_text = preprocess_text_list(transcript_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6276b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Analysis\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e5d7",
   "metadata": {},
   "source": [
    "Analysis from Heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
