{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6367b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "import csv\n",
    "with open('transcripts.csv', encoding='latin-1') as TED_transcript:\n",
    "    transcript_csv = csv.reader(TED_transcript)\n",
    "    next(transcript_csv)\n",
    "    transcripts = list(transcript_csv)\n",
    "    \n",
    "#Converting the csv file to a list of strings (get rid of the url) and eliminating the second column\n",
    "transcript_data: list[str] = [t[0] for t in transcripts]\n",
    "#Sanity check\n",
    "#print(type(transcript_data)) prints out list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data and Corpus Creation\n",
    "import re\n",
    "import inflect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "p = inflect.engine()\n",
    "stop_words: set[str] = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text_list(text_list):\n",
    "    \"\"\"\n",
    "    Preprocesses text in a list of string\n",
    "    Args:\n",
    "        text_list(list[str]): a list of string\n",
    "\n",
    "    Returns:\n",
    "        processed_text(list[list(str)])): a list of string preprocessed for concatenation into a single corpus\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    for text in text_list:\n",
    "        #use re.sub to get rid pf punctuations and replace hem with whitespace\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        #tokenize the words\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #get rid fo stop words using NLTK's stop word list and convert numbers('10') into words ('ten')\n",
    "        filtered  = []\n",
    "        for word in tokens:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower.isdigit():\n",
    "                word_lower = p.number_to_words(word_lower)\n",
    "            if word_lower not in stop_words:\n",
    "                filtered.append(word_lower)\n",
    "        \n",
    "        #append the filtered words to the list\n",
    "        processed_text.append(filtered)\n",
    "    return processed_text\n",
    "\n",
    "#creates a list of list of tokenized words\n",
    "preprocessed_list_of_list: list[list[str]] = preprocess_text_list(transcript_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec (using Logistic regression) - Turn it into a class since there seems to be a lot going on from the textbook\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "class word2vec:\n",
    "    \n",
    "    #initialize the embedding dimension, window size, negative samples, learning rate, minimium count of word, and number of epochs\n",
    "    def __init__(self, embedding_dim = 100, window_size = 5, negative_samples = 10, learning_rate = 0.025, min_count = 1, epoch = 10):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epoch\n",
    "        \n",
    "        #Training variables (originally None (set during the training corpus))\n",
    "        self.vocab = None \n",
    "        self.word_to_index = None \n",
    "        self.index_to_word = None\n",
    "        self.W_input = None\n",
    "        self.W_output = None\n",
    "        self.noise_words = None\n",
    "        self.noise_prob = None\n",
    "        \n",
    "    def build_vocabulary(self, corpus):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary of the corpus (index words and count the frequency of the words)\n",
    "        Args:\n",
    "            corpus (list[list[str]]): a list of list of strings containing tokenized text of ted talk transcripts\n",
    "        \"\"\"\n",
    "        word_count = Counter()\n",
    "        #Count every word that appears in a sentence in the corpus of transcripts\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        #Get the vocab of the corpus (even though min_count is 1, still need to handle empty words)\n",
    "        self.vocab = [word for word, count in word_count.items() if count >= self.min_count]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        #Create a mapping of words (index to a word, word to an index)\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "        self.word_freq = {word: word_count[word] for word in self.vocab}\n",
    "        \n",
    "    def compute_noise_distribution():\n",
    "        \"\"\"Compute the noise distribution for the words\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def init_embeddings(self):\n",
    "        \"\"\"Initializing the embedding matrices (input and output)\n",
    "        \"\"\"\n",
    "        limit = 0.5/self.embedding_dim #Small values to avoid saturation\n",
    "        self.W_input = np.random.uniform(-limit, limit, (self.vocab_size, self.embedding_dim))\n",
    "        self.W_output = np.random.uniform()\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        This function trains the model using the corpus created in the previous cell\n",
    "        Args:\n",
    "            corpus (list[list[str]]): The entire tokenized ted talk transcripts\n",
    "        \"\"\"\n",
    "        self.build_vocabulary(corpus)\n",
    "        self.compute_noise_distribution()\n",
    "        return \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ac2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select word pairs\n",
    "word_pair1 = set((\"technology\", \"innovation\"), (\"learning\", \"fun\"))\n",
    "word_pair2 = set((\"future\", \"artificial\"),(\"intelligence\", \"machine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d76b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6276b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Analysis\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e5d7",
   "metadata": {},
   "source": [
    "Analysis from Heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
