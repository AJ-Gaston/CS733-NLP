{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code (It was more than 1))\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "\n",
    "possible_unknown1 = [\"The\", \"hidden\", \"markov\", \"model\", \"is\", \"working\", \"well\", \".\"]\n",
    "possible_unknown2 = [\"Computer\", \"science\", \"is\", \"cool\", \"but\", \"very\", \"hard\", \".\"]\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeks2geeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    pos_tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos' : pos_tag,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'prev_pos': '' if i == 0 else sentence[i-1][1],  # Previous word's POS tag\n",
    "        'next_pos': '' if i == len(sentence)-1 else sentence[i+1][1],  # Next word's POS tag\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        #print(f\"Sentence[i][0]: {sentence[i][0]}\") \n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        #print(f\"Sentence[i][1] is: {sentence[i][1]}\")\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce92284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the x_train is : 41686\n",
      "The length of the y_train is : 41686\n",
      "The length of the X_test is : 10422\n",
      "The length of the y_test is : 10422\n"
     ]
    }
   ],
   "source": [
    "#check the size of the training and test sets\n",
    "print(f\"The length of the x_train is : {len(X_train)}\")\n",
    "print(f\"The length of the y_train is : {len(y_train)}\")\n",
    "print(f\"The length of the X_test is : {len(X_test)}\")\n",
    "print(f\"The length of the y_test is : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#Need to modify to handle more than one tag\n",
    "class LinearChainConditionalRandomField():\n",
    "    '''\n",
    "    My implementation of linear-chain Conditional Random Field\n",
    "    '''\n",
    "    def __init__(self, learning_rate=0.01, max_iter = 50, l2_penalty = 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.weights = defaultdict(float) #Use the defaultdict to make a dictionary fo floats for the weights\n",
    "        self.all_labels = set() #A set for all the possible POS tags\n",
    "\n",
    "    def log_sum_exp(self, log_values):\n",
    "        \"\"\"\n",
    "            Compute the log sum of exponential values\n",
    "        Args:\n",
    "            log_values (float): the log values\n",
    "        \"\"\"\n",
    "        if not log_values:\n",
    "            return -float('inf')\n",
    "        max_val = max(log_values)\n",
    "        if max_val == -float('inf'):\n",
    "            return -float('inf')\n",
    "        return max_val + math.log(sum(math.exp(x - max_val) for x in log_values))\n",
    "    \n",
    "    def convert_features(self, features_dict, label) -> dict:\n",
    "        \"\"\"\n",
    "        Converts existing features to feature vector with label\n",
    "        Args:\n",
    "            feature_dict(dict): a dictionary of the features\n",
    "            label(): the label of the \n",
    "        Returns:\n",
    "            dict: a feature vector containing the feature's name, value, and label\n",
    "        \"\"\"\n",
    "        feature_vector = {}\n",
    "\n",
    "        #Only use the most important features to avoid explosion\n",
    "        #I decided to make it the word, previous word, next word and the tags for those words\n",
    "        important_features = ['word', 'prev_word', 'next_word', 'prev_pos', 'next_pos']\n",
    "        \n",
    "        #See if the most important features exist\n",
    "        for feature_name in important_features:\n",
    "            if feature_name in features_dict:\n",
    "                value = features_dict[feature_name]\n",
    "                if value and isinstance(value, str):\n",
    "                    feature_vector[f\"{feature_name}_{value}_{label}\"] = 1\n",
    "        \n",
    "        # Add a few other features\n",
    "        if features_dict.get('is_capitalized'):\n",
    "            feature_vector[f\"cap_{label}\"] = 1\n",
    "        if features_dict.get('is_numeric'):\n",
    "            feature_vector[f\"num_{label}\"] = 1\n",
    "        if features_dict.get('has_hyphen'):\n",
    "            feature_vector[f\"hyphen_{label}\"] = 1\n",
    "            \n",
    "        return feature_vector\n",
    "    def compute_transition_features(self, prev_label, current_label):\n",
    "        \"\"\"\n",
    "        Compute the transition features of the previous tag\n",
    "        and the current tag\n",
    "        Args:\n",
    "            prev_label (_type_): _description_\n",
    "            current_label (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            dictionary: a dictionary containing the transition from previous label to current label\n",
    "        \"\"\"\n",
    "        return {f\"trans_{prev_label}_{current_label}\": 1}\n",
    "    \n",
    "    def compute_score(self, sequence_features, labels):\n",
    "        \"\"\"\n",
    "        Compute score using the precomputed features\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            labels ():\n",
    "        Returns:\n",
    "            float: the score of the emission and transition features\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # First word\n",
    "        first_features = self.convert_features(sequence_features[0], labels[0])\n",
    "        for feat, value in first_features.items():\n",
    "            score += self.weights[feat] * value\n",
    "            \n",
    "        # Remaining words with transitions\n",
    "        for i in range(1, len(sequence_features)):\n",
    "            # Emission features\n",
    "            emission_features = self.convert_features(sequence_features[i], labels[i])\n",
    "            for feat, value in emission_features.items():\n",
    "                score += self.weights[feat] * value\n",
    "                \n",
    "            # Transition features\n",
    "            transition_features = self.compute_transition_features(labels[i-1], labels[i])\n",
    "            for feat, value in transition_features.items():\n",
    "                score += self.weights[feat] * value\n",
    "                \n",
    "        return score\n",
    "    \n",
    "    def forward_algorithm_log(self, sequence_features, possible_labels):\n",
    "        \"\"\"\n",
    "        Forward algorithm for computing partition function\n",
    "        This is for the training part of the model\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            list[defaultdict]: contains alpha[observations][labels]\n",
    "            float: the partition function or sum of all the alpha values\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        alpha_log = [defaultdict(float) for _ in range(T)]\n",
    "        \n",
    "        # Initialize first position\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            score = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            alpha_log[0][label] = score  # Store log-scores directly\n",
    "        \n",
    "        # Recursion in log-space\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                log_scores = []\n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission + transition score\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    total_score = alpha_log[t-1][prev_label] + emission_score + transition_score\n",
    "                    log_scores.append(total_score)\n",
    "                \n",
    "                alpha_log[t][current_label] = self.log_sum_exp(log_scores)\n",
    "        \n",
    "        # Partition function is log_sum_exp of final alphas\n",
    "        log_Z = self.log_sum_exp(list(alpha_log[T-1].values()))\n",
    "        return alpha_log, log_Z\n",
    "    \n",
    "    def backward_algorithm_log(self, sequence_features, possible_labels) -> list:\n",
    "        \"\"\"\n",
    "        Backward algorithm\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            list[defaultdict]: a list of the observation and labels\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        beta_log = [defaultdict(float) for _ in range(T)]\n",
    "        \n",
    "        # Initialize last position\n",
    "        for label in possible_labels[T-1]:\n",
    "            beta_log[T-1][label] = 0.0  # log(1) = 0\n",
    "        \n",
    "        # Recursion backwards\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for current_label in possible_labels[t]:\n",
    "                log_scores = []\n",
    "                for next_label in possible_labels[t+1]:\n",
    "                    emission_features = self.convert_features(sequence_features[t+1], next_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    transition_features = self.compute_transition_features(current_label, next_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    total_score = beta_log[t+1][next_label] + emission_score + transition_score\n",
    "                    log_scores.append(total_score)\n",
    "                \n",
    "                beta_log[t][current_label] = self.log_sum_exp(log_scores)\n",
    "                \n",
    "        return beta_log\n",
    "    \n",
    "    def compute_marginals_log(self, sequence_features, possible_labels):\n",
    "        \"\"\"\n",
    "        Compute marginal probabilities using forward-backward\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            list[defaultdict] : the node marginals, probability of each label at each position P(y_t | X)\n",
    "            list[defaultdict] : the edge marginals, probability of each label transition P(y_{t-1}, y_t | X)\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        alpha_log, log_Z = self.forward_algorithm_log(sequence_features, possible_labels)\n",
    "        beta_log = self.backward_algorithm_log(sequence_features, possible_labels)\n",
    "        \n",
    "        # Node marginals in log-space, then convert to probabilities\n",
    "        node_marginals = [defaultdict(float) for _ in range(T)]\n",
    "        for t in range(T):\n",
    "            for label in possible_labels[t]:\n",
    "                log_prob = alpha_log[t][label] + beta_log[t][label] - log_Z\n",
    "                node_marginals[t][label] = math.exp(log_prob) if math.isfinite(log_prob) else 0.0\n",
    "        \n",
    "        # Edge marginals\n",
    "        edge_marginals = [defaultdict(lambda: defaultdict(float)) for _ in range(T-1)]\n",
    "        for t in range(T-1):\n",
    "            for prev_label in possible_labels[t]:\n",
    "                for current_label in possible_labels[t+1]:\n",
    "                    emission_features = self.convert_features(sequence_features[t+1], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    log_prob = (alpha_log[t][prev_label] + emission_score + \n",
    "                            transition_score + beta_log[t+1][current_label] - log_Z)\n",
    "                    edge_marginals[t][prev_label][current_label] = math.exp(log_prob) if math.isfinite(log_prob) else 0.0\n",
    "        \n",
    "        return node_marginals, edge_marginals\n",
    "    \n",
    "    def viterbi_decode(self, sequence_features, possible_labels):\n",
    "        \"\"\"Viterbi algorithm for finding the most likely label sequence\"\"\"\n",
    "        T = len(sequence_features)\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            delta[0][label] = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission features\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    # Transition features\n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        best_path = [None] * T\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Find best final label\n",
    "        for label in possible_labels[T-1]:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        # Backtrack through the sequence\n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path, best_score\n",
    "    \n",
    "    def fit(self, X_train:list, y_train:list):\n",
    "        \"\"\"Train the CRF model\n",
    "        Args:\n",
    "            X_train (list): the words in the training set\n",
    "            y_train (list): the part-of-speech tag(s) for each word\n",
    "        \"\"\"\n",
    "        #Train with numerical stability fixes\n",
    "        # Collect all possible labels\n",
    "        for sentence_labels in y_train:\n",
    "            for label_set in sentence_labels:\n",
    "                self.all_labels.update(label_set)\n",
    "        \n",
    "        print(f\"Training with {len(self.all_labels)} unique labels\")\n",
    "        \n",
    "        # Convert y_train from sets to lists for training\n",
    "        y_train_single = []\n",
    "        for sentence_labels in y_train:\n",
    "            sentence_single = [next(iter(tag_set)) for tag_set in sentence_labels]\n",
    "            y_train_single.append(sentence_single)\n",
    "        \n",
    "        \n",
    "        print(f\"Using {len(X_train)} sequences for stable training\")\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            total_loss = 0.0\n",
    "            grad = defaultdict(float)\n",
    "            grad_norm = 0.0\n",
    "            \n",
    "            for sequence_features, true_labels in zip(X_train, y_train):\n",
    "                possible_labels = [self.all_labels for _ in range(len(sequence_features))]\n",
    "                \n",
    "                try:\n",
    "                    # Use log-space algorithms for stability\n",
    "                    node_marginals, edge_marginals = self.compute_marginals_log(sequence_features, possible_labels)\n",
    "                    \n",
    "                    # Compute empirical features from true labels\n",
    "                    empirical_features = defaultdict(float)\n",
    "                    \n",
    "                    # First position\n",
    "                    first_features = self.convert_features(sequence_features[0], true_labels[0])\n",
    "                    for feat, value in first_features.items():\n",
    "                        empirical_features[feat] += value\n",
    "                    \n",
    "                    # Remaining positions with transitions\n",
    "                    for i in range(1, len(sequence_features)):\n",
    "                        emission_features = self.convert_features(sequence_features[i], true_labels[i])\n",
    "                        for feat, value in emission_features.items():\n",
    "                            empirical_features[feat] += value\n",
    "                        \n",
    "                        transition_features = self.compute_transition_features(true_labels[i-1], true_labels[i])\n",
    "                        for feat, value in transition_features.items():\n",
    "                            empirical_features[feat] += value\n",
    "                    \n",
    "                    # Compute expected features from marginals\n",
    "                    expected_features = defaultdict(float)\n",
    "                    \n",
    "                    # Node features (emission)\n",
    "                    for t in range(len(sequence_features)):\n",
    "                        for label, prob in node_marginals[t].items():\n",
    "                            features = self.convert_features(sequence_features[t], label)\n",
    "                            for feat, value in features.items():\n",
    "                                expected_features[feat] += prob * value\n",
    "                    \n",
    "                    # Edge features (transition)\n",
    "                    for t in range(len(sequence_features)-1):\n",
    "                        for prev_label in possible_labels[t]:\n",
    "                            for current_label in possible_labels[t+1]:\n",
    "                                prob = edge_marginals[t][prev_label][current_label]\n",
    "                                trans_features = self.compute_transition_features(prev_label, current_label)\n",
    "                                for feat, value in trans_features.items():\n",
    "                                    expected_features[feat] += prob * value\n",
    "                    \n",
    "                    # Update gradient with clipping\n",
    "                    for feat in set(empirical_features.keys()) | set(expected_features.keys()):\n",
    "                        update = (empirical_features[feat] - expected_features[feat])\n",
    "                        grad[feat] += update\n",
    "                        grad_norm += update * update\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    true_score = self.compute_score(sequence_features, true_labels)\n",
    "                    alpha_log, log_Z = self.forward_algorithm_log(sequence_features, possible_labels)\n",
    "                    \n",
    "                    if math.isfinite(log_Z):\n",
    "                        log_likelihood = true_score - log_Z\n",
    "                        total_loss -= log_likelihood\n",
    "                    else:\n",
    "                        total_loss += 100.0  # Penalty for numerical issues\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in sequence: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Gradient clipping\n",
    "            #Necessary to handle vanishing/exploding gradient\n",
    "            grad_norm = math.sqrt(grad_norm)\n",
    "            if grad_norm > 1.0:\n",
    "                clip_factor = 1.0 / grad_norm\n",
    "                for feat in grad:\n",
    "                    grad[feat] *= clip_factor\n",
    "            \n",
    "            # Update weights with regularization and clipping\n",
    "            for feat in grad:\n",
    "                # L2 regularization\n",
    "                grad[feat] -= self.l2_penalty * self.weights[feat]\n",
    "                self.weights[feat] += self.learning_rate * grad[feat]\n",
    "                \n",
    "                # Weight clipping to prevent explosion\n",
    "                if abs(self.weights[feat]) > 10.0:\n",
    "                    self.weights[feat] = math.copysign(10.0, self.weights[feat])\n",
    "            \n",
    "            avg_loss = total_loss / len(X_train) if X_train else 0.0\n",
    "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping if loss becomes NaN\n",
    "            # Prevent epxloding/vanishing gradient\n",
    "            if math.isnan(avg_loss):\n",
    "                print(\"Loss became NaN - stopping early\")\n",
    "                break\n",
    "            \n",
    "            # Reduce learning rate\n",
    "            self.learning_rate *= 0.95\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Simple prediction\n",
    "        Args:\n",
    "            X_test(list): a list of words\n",
    "        Returns:\n",
    "            list: a list of the possible tags for each word\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for sequence_features in X_test:\n",
    "            possible_labels = [self.all_labels for _ in range(len(sequence_features))]\n",
    "            best_path, _ = self.viterbi_decode(sequence_features, possible_labels)\n",
    "            predictions.append(best_path)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate accuracy on test set\n",
    "        Args:\n",
    "            X_test (list): test set of words\n",
    "            y_test (list)\n",
    "        Return:\n",
    "            float: the accuracy of the CRF model\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for pred_seq, true_seq in zip(predictions, y_test): #For the predicted sequence and the true sequence\n",
    "            for pred_label, true_set in zip(pred_seq, true_seq): #For the predicted label and the true sequence\n",
    "                if pred_label in true_set:  # Check if prediction is in possible tags\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 48 unique labels\n",
      "Using 41686 sequences for stable training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m crf \u001b[38;5;241m=\u001b[39m LinearChainConditionalRandomField(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m crf\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[13], line 352\u001b[0m, in \u001b[0;36mLinearChainConditionalRandomField.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    351\u001b[0m true_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_score(sequence_features, true_labels)\n\u001b[0;32m--> 352\u001b[0m alpha_log, log_Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_algorithm_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misfinite(log_Z):\n\u001b[1;32m    355\u001b[0m     log_likelihood \u001b[38;5;241m=\u001b[39m true_score \u001b[38;5;241m-\u001b[39m log_Z\n",
      "Cell \u001b[0;32mIn[13], line 134\u001b[0m, in \u001b[0;36mLinearChainConditionalRandomField.forward_algorithm_log\u001b[0;34m(self, sequence_features, possible_labels)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prev_label \u001b[38;5;129;01min\u001b[39;00m possible_labels[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Emission + transition score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     emission_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_features(sequence_features[t], current_label)\n\u001b[0;32m--> 134\u001b[0m     emission_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43memission_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     transition_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_transition_features(prev_label, current_label)\n\u001b[1;32m    137\u001b[0m     transition_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat] \u001b[38;5;241m*\u001b[39m value \u001b[38;5;28;01mfor\u001b[39;00m feat, value \u001b[38;5;129;01min\u001b[39;00m transition_features\u001b[38;5;241m.\u001b[39mitems())\n",
      "Cell \u001b[0;32mIn[13], line 134\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prev_label \u001b[38;5;129;01min\u001b[39;00m possible_labels[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Emission + transition score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     emission_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_features(sequence_features[t], current_label)\n\u001b[0;32m--> 134\u001b[0m     emission_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat] \u001b[38;5;241m*\u001b[39m value \u001b[38;5;28;01mfor\u001b[39;00m feat, value \u001b[38;5;129;01min\u001b[39;00m emission_features\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    136\u001b[0m     transition_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_transition_features(prev_label, current_label)\n\u001b[1;32m    137\u001b[0m     transition_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat] \u001b[38;5;241m*\u001b[39m value \u001b[38;5;28;01mfor\u001b[39;00m feat, value \u001b[38;5;129;01min\u001b[39;00m transition_features\u001b[38;5;241m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "crf = LinearChainConditionalRandomField(learning_rate=0.05, max_iter=15)\n",
    "crf.fit(X_train, y_train)\n",
    "crf.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to read in the GMB_Dataset\n",
    "file_path = './GMB_dataset.txt'\n",
    "with open(file_path, 'r') as gmb_data:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Modified to capture the two previous words and the next two words \n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    pos_tag = sentence[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos': pos_tag,\n",
    "        'is_first': i == 0,\n",
    "        'is_last': i == len(sentence) - 1,\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'prev_pos': '' if i == 0 else sentence[i-1][1],\n",
    "        'next_pos': '' if i == len(sentence)-1 else sentence[i+1][1],\n",
    "        'prev_prev_pos': '' if i <= 1 else sentence[i-2][1],  # Two words back POS\n",
    "        'next_next_pos': '' if i >= len(sentence)-2 else sentence[i+2][1],  # Two words ahead POS\n",
    "        'pos_bigram': f\"{'' if i == 0 else sentence[i-1][1]}_{pos_tag}\",  # POS bigram with previous word\n",
    "        'has_hyphen': '-' in word,\n",
    "        'is_numeric': word.isdigit(),\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF for Named Entity Recognition (NER)\n",
    "\n",
    "class ConditionalRandomFieldNer:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "    def predict(self, X_train):\n",
    "        predictions = []\n",
    "        return predictions\n",
    "    def model_evaluation(self, X_test, y_test):\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        accuracy = 0.0\n",
    "        return precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences for the CRFs\n",
    "CRF_test_sentence1 = [\"She\", \"likes\", \"to\", \"read\",\"books\"]\n",
    "CRF_test_sentence2 = []\n",
    "CRF_test_sentence3 = []\n",
    "CRF_test_sentence4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
