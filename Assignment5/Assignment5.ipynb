{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code (It was more than 1))\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "\n",
    "possible_unknown1 = [\"The\", \"hidden\", \"markov\", \"model\", \"is\", \"working\", \"well\", \".\"]\n",
    "possible_unknown2 = [\"Computer\", \"science\", \"is\", \"cool\", \"but\", \"very\", \"hard\", \".\"]\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeksforgeeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    pos_tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos' : pos_tag,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'prev_pos': '' if i == 0 else sentence[i-1][1],  # Previous word's POS tag\n",
    "        'next_pos': '' if i == len(sentence)-1 else sentence[i+1][1],  # Next word's POS tag\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        #print(f\"Sentence[i][0]: {sentence[i][0]}\") \n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        #print(f\"Sentence[i][1] is: {sentence[i][1]}\")\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce92284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the x_train is : 41686\n",
      "The length of the y_train is : 41686\n",
      "The length of the X_test is : 10422\n",
      "The length of the y_test is : 10422\n"
     ]
    }
   ],
   "source": [
    "#check the size of the training and test sets\n",
    "print(f\"The length of the x_train is : {len(X_train)}\")\n",
    "print(f\"The length of the y_train is : {len(y_train)}\")\n",
    "print(f\"The length of the X_test is : {len(X_test)}\")\n",
    "print(f\"The length of the y_test is : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#Need to modify to handle more than one tag\n",
    "class LinearChainConditionalRandomField():\n",
    "    '''\n",
    "    My implementation of linear-chain Conditional Random Field\n",
    "    '''\n",
    "    def __init__(self, learning_rate=0.05, max_iter = 20, l2_penalty = 0.1, batch_size = 2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.batch_size = batch_size #Need this o handle a dataset of over 10,000 \n",
    "        self.weights = defaultdict(float) #Use the defaultdict to make a dictionary fo floats for the weights\n",
    "        self.all_labels = set() #A set for all the possible POS tags\n",
    "\n",
    "    def log_sum_exp(self, log_values):\n",
    "        \"\"\"\n",
    "            Compute the log sum of exponential values\n",
    "        Args:\n",
    "            log_values (float): the log values\n",
    "        \"\"\"\n",
    "        if not log_values:\n",
    "            return -float('inf')\n",
    "        max_val = max(log_values)\n",
    "        if max_val == -float('inf'):\n",
    "            return -float('inf')\n",
    "        return max_val + math.log(sum(math.exp(x - max_val) for x in log_values))\n",
    "    \n",
    "    def convert_features(self, features_dict, label) -> dict:\n",
    "        \"\"\"\n",
    "        Converts existing features to feature vector with label\n",
    "        Args:\n",
    "            feature_dict(dict): a dictionary of the features\n",
    "            label(): the label of the \n",
    "        Returns:\n",
    "            dict: a feature vector containing the feature's name, value, and label\n",
    "        \"\"\"\n",
    "        feature_vector = {}\n",
    "\n",
    "        #Only use the most important features to avoid explosion\n",
    "        #I decided to make it the word, previous word, next word and the tags for those words\n",
    "        important_features = ['word', 'prev_word', 'next_word', 'prev_pos', 'next_pos']\n",
    "        \n",
    "        #See if the most important features exist\n",
    "        for feature_name in important_features:\n",
    "            if feature_name in features_dict:\n",
    "                value = features_dict[feature_name]\n",
    "                if value and isinstance(value, str):\n",
    "                    feature_vector[f\"{feature_name}_{value}_{label}\"] = 1\n",
    "        \n",
    "        # Add a few other features (Now that I can get it work)\n",
    "        if features_dict.get('is_capitalized'):\n",
    "            feature_vector[f\"cap_{label}\"] = 1\n",
    "        if features_dict.get('is_numeric'):\n",
    "            feature_vector[f\"num_{label}\"] = 1\n",
    "        if features_dict.get('has_hyphen'):\n",
    "            feature_vector[f\"hyphen_{label}\"] = 1\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    def compute_transition_features(self, prev_label, current_label):\n",
    "        \"\"\"\n",
    "        Compute the transition features of the previous tag\n",
    "        and the current tag\n",
    "        Args:\n",
    "            prev_label (_type_): _description_\n",
    "            current_label (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            dictionary: a dictionary containing the transition from previous label to current label\n",
    "        \"\"\"\n",
    "        return {f\"trans_{prev_label}_{current_label}\": 1}\n",
    "    \n",
    "    def compute_score(self, sequence_features, labels):\n",
    "        \"\"\"\n",
    "        Compute score using the precomputed features\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            labels ():\n",
    "        Returns:\n",
    "            float: the score of the emission and transition features\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # First word\n",
    "        first_features = self.convert_features(sequence_features[0], labels[0])\n",
    "        for feat, value in first_features.items():\n",
    "            score += self.weights[feat] * value\n",
    "            \n",
    "        # Remaining words with transitions\n",
    "        for i in range(1, len(sequence_features)):\n",
    "            # Emission features\n",
    "            emission_features = self.convert_features(sequence_features[i], labels[i])\n",
    "            for feat, value in emission_features.items():\n",
    "                score += self.weights[feat] * value\n",
    "                \n",
    "            # Transition features\n",
    "            transition_features = self.compute_transition_features(labels[i-1], labels[i])\n",
    "            for feat, value in transition_features.items():\n",
    "                score += self.weights[feat] * value\n",
    "                \n",
    "        return score\n",
    "    \n",
    "    def forward_algorithm_log(self, sequence_features, possible_labels):\n",
    "        \"\"\"\n",
    "        Forward algorithm for computing partition function\n",
    "        This is for the training part of the model\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            list[defaultdict]: contains alpha[observations][labels]\n",
    "            float: the partition function or sum of all the alpha values\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        alpha_log = [defaultdict(float) for _ in range(T)]\n",
    "        \n",
    "        # Initialize first position\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            score = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            alpha_log[0][label] = score  # Store log-scores directly\n",
    "        \n",
    "        # Recursion in log-space\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                log_scores = []\n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission + transition scores for the labels\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    total_score = alpha_log[t-1][prev_label] + emission_score + transition_score\n",
    "                    log_scores.append(total_score)\n",
    "                \n",
    "                alpha_log[t][current_label] = self.log_sum_exp(log_scores)\n",
    "        \n",
    "        # Partition function is log_sum_exp of final alphas\n",
    "        log_Z = self.log_sum_exp(list(alpha_log[T-1].values()))\n",
    "        return alpha_log, log_Z\n",
    "    \n",
    "    def backward_algorithm_log(self, sequence_features, possible_labels) -> list:\n",
    "        \"\"\"\n",
    "        Backward algorithm\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            list[defaultdict]: a list of the observation and labels\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        beta_log = [defaultdict(float) for _ in range(T)]\n",
    "        \n",
    "        # Initialize last position\n",
    "        for label in possible_labels[T-1]:\n",
    "            beta_log[T-1][label] = 0.0  # log(1) = 0\n",
    "        \n",
    "        # Recursion backwards\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for current_label in possible_labels[t]:\n",
    "                log_scores = []\n",
    "                for next_label in possible_labels[t+1]:\n",
    "                    emission_features = self.convert_features(sequence_features[t+1], next_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    transition_features = self.compute_transition_features(current_label, next_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    total_score = beta_log[t+1][next_label] + emission_score + transition_score\n",
    "                    log_scores.append(total_score)\n",
    "                \n",
    "                beta_log[t][current_label] = self.log_sum_exp(log_scores)\n",
    "                \n",
    "        return beta_log\n",
    "    \n",
    "    def compute_marginals_log(self, sequence_features, possible_labels):\n",
    "        \"\"\"\n",
    "        Compute marginal probabilities for one sequence\n",
    "        Args:\n",
    "            sequence_features ():\n",
    "            possible_labels ():\n",
    "        Returns:\n",
    "            float: the log of the partition\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        alpha_log, log_Z = self.forward_algorithm_log(sequence_features, possible_labels)\n",
    "        \n",
    "        # For mini-batch, only need the loss, not full marginals\n",
    "        #This is to save computation time since it takes over an hour for one iteration\n",
    "        #when looking at every edge and node marginal\n",
    " \n",
    "        return log_Z\n",
    "    def compute_batch_gradient(self, X_batch, y_batch):\n",
    "        \"\"\"Compute gradient for a mini-batch\"\"\"\n",
    "        grad = defaultdict(float)\n",
    "        batch_loss = 0.0\n",
    "        \n",
    "        for seq_features, true_labels in zip(X_batch, y_batch):\n",
    "            possible_labels = [self.all_labels for _ in range(len(seq_features))]\n",
    "            \n",
    "            # Compute true score\n",
    "            true_score = self.compute_score(seq_features, true_labels)\n",
    "            \n",
    "            # Compute log Z (partition function)\n",
    "            log_Z = self.compute_marginals_log(seq_features, possible_labels)\n",
    "            \n",
    "            if math.isfinite(log_Z):\n",
    "                # Negative log likelihood\n",
    "                log_likelihood = true_score - log_Z\n",
    "                batch_loss -= log_likelihood\n",
    "                \n",
    "                # Approximate gradient: boost true features\n",
    "                prev_label = None\n",
    "                for i, (features, label) in enumerate(zip(seq_features, true_labels)):\n",
    "                    # Emission features\n",
    "                    emission_feats = self.convert_features(features, label)\n",
    "                    for feat in emission_feats:\n",
    "                        grad[feat] += self.learning_rate\n",
    "                    \n",
    "                    # Transition features\n",
    "                    if prev_label is not None:\n",
    "                        trans_feat = f\"trans_{prev_label}_{label}\"\n",
    "                        grad[trans_feat] += self.learning_rate * 0.5\n",
    "                    \n",
    "                    prev_label = label\n",
    "            else:\n",
    "                batch_loss += 100.0  # Penalty for numerical issues\n",
    "        \n",
    "        return grad, batch_loss\n",
    "    \n",
    "    def viterbi_decode(self, sequence_features, possible_labels):\n",
    "        \"\"\"Viterbi algorithm for finding the most likely label sequence\"\"\"\n",
    "        T = len(sequence_features)\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize the first word/tag in the sequence\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            delta[0][label] = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission features\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    # Transition features\n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        #Initialize to nothing (for best path an array of size T with None)\n",
    "        #for the best score, make it a float with -infinity\n",
    "        best_path = [None] * T\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Find best final label\n",
    "        for label in possible_labels[T-1]:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        # Backtrack through the sequence\n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path, best_score\n",
    "    \n",
    "    def fit(self, X_train:list, y_train:list):\n",
    "        \"\"\"Train the CRF model\n",
    "        Now with mini-batches since the dataset is too large\n",
    "        Args:\n",
    "            X_train (list): the words in the training set\n",
    "            y_train (list): the part-of-speech tag(s) for each word\n",
    "        \"\"\"\n",
    "        # Collect all labels\n",
    "        for sentence_labels in y_train:\n",
    "            for label_set in sentence_labels:\n",
    "                self.all_labels.update(label_set)\n",
    "        \n",
    "        # Convert to single labels for training\n",
    "        y_train_single = []\n",
    "        for sentence_labels in y_train:\n",
    "            sentence_single = [next(iter(tag_set)) for tag_set in sentence_labels]\n",
    "            y_train_single.append(sentence_single)\n",
    "        \n",
    "        print(f\"Training on {len(X_train)} sequences with {len(self.all_labels)} labels\")\n",
    "        print(f\"Batch size: {self.batch_size}, Total batches: {len(X_train) // self.batch_size + 1}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iter):\n",
    "            start_time = time.time()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Shuffle data each iteration\n",
    "            indices = list(range(len(X_train)))\n",
    "            random.shuffle(indices)\n",
    "            X_shuffled = [X_train[i] for i in indices]\n",
    "            y_shuffled = [y_train_single[i] for i in indices]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for batch_start in range(0, len(X_shuffled), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(X_shuffled))\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                y_batch = y_shuffled[batch_start:batch_end]\n",
    "                \n",
    "                \n",
    "                # Compute gradient for this batch\n",
    "                grad, batch_loss = self.compute_batch_gradient(X_batch, y_batch)\n",
    "                total_loss += batch_loss\n",
    "                \n",
    "                # Update weights with regularization\n",
    "                for feat in grad:\n",
    "                    # L2 regularization\n",
    "                    if self.weights[feat] != 0:\n",
    "                        grad[feat] -= self.l2_penalty * self.weights[feat]\n",
    "                    \n",
    "                    self.weights[feat] += grad[feat]\n",
    "                    \n",
    "                    # Weight clipping\n",
    "                    if abs(self.weights[feat]) > 10.0:\n",
    "                        self.weights[feat] = math.copysign(10.0, self.weights[feat])\n",
    "            \n",
    "            avg_loss = total_loss / len(X_train)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            #Need this to see if there's numerical instability\n",
    "            if math.isnan(avg_loss):\n",
    "                print(\"Loss became NaN - stopping early\")\n",
    "                break\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.learning_rate *= 0.95\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Simple prediction\n",
    "        Args:\n",
    "            X_test(list): a list of words\n",
    "        Returns:\n",
    "            list: a list of the possible tags for each word\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for sequence_features in X_test:\n",
    "            possible_labels = [self.all_labels for _ in range(len(sequence_features))]\n",
    "            best_path, _ = self.viterbi_decode(sequence_features, possible_labels)\n",
    "            predictions.append(best_path)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate accuracy on test set\n",
    "        Args:\n",
    "            X_test (list): test set of words\n",
    "            y_test (list)\n",
    "        Return:\n",
    "            float: the accuracy of the CRF model\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for pred_seq, true_seq in zip(predictions, y_test): #For the predicted sequence and the true sequence\n",
    "            for pred_label, true_set in zip(pred_seq, true_seq): #For the predicted label and the true sequence\n",
    "                if pred_label in true_set:  # Check if prediction is in possible tags\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DATASET TRAINING ===\n",
      "Training on 41686 sequences with 48 labels\n",
      "Batch size: 2000, Total batches: 21\n",
      "Iteration 0, Loss: 47.9152, Time: 3576.1s\n",
      "Iteration 1, Loss: 25.2611, Time: 3617.9s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize with optimized parameters\n",
    "crf = LinearChainConditionalRandomField(\n",
    "    learning_rate=0.03,\n",
    "    max_iter=10,\n",
    "    batch_size=2000,  # Process 2000 sequences at once\n",
    "    l2_penalty=0.01\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "print(\"=== FULL DATASET TRAINING ===\")\n",
    "crf.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n=== FINAL EVALUATION ===\")\n",
    "test_accuracy = crf.evaluate(X_test[:5000], y_test[:5000])\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the GMB dataset\n",
    "import pandas as pd\n",
    "#Read it in without headers with latin1 encoding\n",
    "gmb_pd = pd.read_csv(\"./GMB_dataset.txt\", sep=\"\\t\", header=None,encoding=\"latin1\")\n",
    "#Take the first row as the heading with the names in the file\n",
    "#Re-index the dataset appropriately\n",
    "gmb_pd.columns = gmb_pd.iloc[0]\n",
    "gmb_pd = gmb_pd[1:]\n",
    "gmb_pd.columns = ['Index','Sentence #','Word','POS','Tag']\n",
    "gmb_pd = gmb_pd.reset_index(drop=True)\n",
    "gmb_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Modified for Named Entity Recognition (Looks at the previous word and the next word in the sentence)\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run my read-in data through the word feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and testing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X is the words in the sentences and y is the tags\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF for Named Entity Recognition (NER)\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "class ConditionalRandomFieldNer:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=50, batch_size = 2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.batch_size = batch_size\n",
    "    def viterbi_decode():\n",
    "        \"\"\"Viterbi algorithm for finding the most likely label sequence\"\"\"\n",
    "        T = len(sequence_features)\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize the first word/tag in the sequence\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            delta[0][label] = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission features\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    # Transition features\n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        #Initialize to nothing (for best path an array of size T with None)\n",
    "        #for the best score, make it a float with -infinity\n",
    "        best_path = [None] * T\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Find best final label\n",
    "        for label in possible_labels[T-1]:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        # Backtrack through the sequence\n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path, best_score \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #Learning rate decay\n",
    "        self.learning_rate *= 0.95\n",
    "    def predict(self, X_train):\n",
    "        predictions = []\n",
    "        return predictions\n",
    "    def model_evaluation(self, X_test, y_test):\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        accuracy = 0.0\n",
    "        f1_score = (2 *precision * recall) / (precision + recall)\n",
    "        return precision, recall, accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now train the model with the GMB dataset\n",
    "crf = ConditionalRandomFieldNer()\n",
    "#Return the model evaluation\n",
    "model_precision, model_recall, model_accuracy, model_f1_score = crf.model_evaluation()\n",
    "\n",
    "#Print out the model's evaluation\n",
    "print(f\"Model Precision: {model_precision * 100:.2f}\"\n",
    "      f\"Model Recall: {model_recall * 100:.2f}\"\n",
    "      f\"Model Accuracy: {model_accuracy * 100:.2f}\"\n",
    "      f\"Model's F1_score: {model_f1_score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
