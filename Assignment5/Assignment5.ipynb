{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code (It was more than 1))\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n",
      "Input sentence: ['Computer', 'science', 'is', 'cool', 'but', 'very', 'hard', '.']\n",
      "HMM prediction of second long sentence:  [('Computer', 'FW'), ('science', 'FW'), ('is', 'FW'), ('cool', 'FW'), ('but', 'FW'), ('very', 'FW'), ('hard', 'FW'), ('.', 'FW')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n",
    "\n",
    "possible_unknown1 = [\"Computer\", \"science\", \"is\", \"cool\", \"but\", \"very\", \"hard\", \".\"]\n",
    "predicted_possible_unknown1 = hmm.predict(possible_unknown1)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_possible_unknown1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798aebc7",
   "metadata": {},
   "source": [
    "I created these sentences with tags via this website:\n",
    "https://parts-of-speech.info/\n",
    "And this is what the website predicted for each sentence\n",
    "For sentence 1: [DT, NN, VBD] - Matches the HMM Model's prediction\n",
    "\n",
    "For sentence 2: [NNP, MD, VB, DT, NN, RB] - Matches the HMM's prediction\n",
    "\n",
    "For sentence 3: [PRP, VBP, WRB, VB, DT, NN] - Matches the HMM's prediction\n",
    "\n",
    "For sentence 4: [PRP, VBP, RB, JJ] - Matches the HMM's prediction except for the punctuation\n",
    "\n",
    "For long sentence 1: [DT, NN, NN, VBD, IN, DT, NN, VBZ, VBN, VBN, NN, PRP, VBP, NN, MD, VB, VBN] - Matches the HMM's prediction except for the punctuations\n",
    "\n",
    "For long sentence 2: [NN, DT, NN, VBD, IN, DT, JJ, NN, MD, VB, IN, DT, NN, WP, VBZ, VBN, TO, DT, NN] - The model predicted that Today was an RB and it had TO as a tag unlike the website\n",
    "\n",
    "For possible unknown sentence 1: Here everything is tagegd as the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeksforgeeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    pos_tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos' : pos_tag,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'prev_pos': '' if i == 0 else sentence[i-1][1],  # Previous word's POS tag\n",
    "        'next_pos': '' if i == len(sentence)-1 else sentence[i+1][1],  # Next word's POS tag\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        #print(f\"Sentence[i][0]: {sentence[i][0]}\") \n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        #print(f\"Sentence[i][1] is: {sentence[i][1]}\")\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce92284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the x_train is : 41686\n",
      "The length of the y_train is : 41686\n",
      "The length of the X_test is : 10422\n",
      "The length of the y_test is : 10422\n"
     ]
    }
   ],
   "source": [
    "#check the size of the training and test sets\n",
    "print(f\"The length of the x_train is : {len(X_train)}\")\n",
    "print(f\"The length of the y_train is : {len(y_train)}\")\n",
    "print(f\"The length of the X_test is : {len(X_test)}\")\n",
    "print(f\"The length of the y_test is : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class LinearChainConditionalRandomField:\n",
    "    def __init__(self, learning_rate=0.1, max_iter=15, batch_size=500, l2_penalty=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.weights = defaultdict(float)\n",
    "        self.all_labels = set()\n",
    "        self.label_list = []  # Cache for faster iteration\n",
    "    \n",
    "    def convert_features(self, features_dict, label):\n",
    "        \"\"\"Converts features\n",
    "            Args:\n",
    "                features_dict (dict): a sictionary of all the features I pass in from training/test set\n",
    "                label (str): a tag associated with the word\n",
    "            Returns:\n",
    "                dictionary: a dictionary of important features\n",
    "        \"\"\"\n",
    "        feature_vector = {}\n",
    "        \n",
    "        # ONLY these 3 features - remove the rest\n",
    "        word = features_dict['word']\n",
    "        feature_vector[f\"w_{word}_{label}\"] = 1\n",
    "        \n",
    "        if features_dict['prev_word']:\n",
    "            feature_vector[f\"p_{features_dict['prev_word']}_{label}\"] = 1\n",
    "            \n",
    "        if features_dict.get('is_capitalized'):\n",
    "            feature_vector[f\"c_{label}\"] = 1\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    def compute_score(self, sequence_features, labels):\n",
    "        \"\"\"Optimized scoring - direct dict lookups\n",
    "            Args:\n",
    "                sequence_features(list): the features of the words in a sequence/sentence\n",
    "                labels (list): All the tags for a sequence/sentence\n",
    "            Returns:\n",
    "                float: the score of the weights for word and its labels, the previous word and its labels as well its features\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        prev_label = None\n",
    "        \n",
    "        for features, label in zip(sequence_features, labels):\n",
    "            # Direct dict lookups - no function calls\n",
    "            word = features['word']\n",
    "            score += self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "            \n",
    "            if features['prev_word']:\n",
    "                score += self.weights.get(f\"p_{features['prev_word']}_{label}\", 0)\n",
    "                \n",
    "            if features.get('is_capitalized'):\n",
    "                score += self.weights.get(f\"c_{label}\", 0)\n",
    "            \n",
    "            if prev_label is not None:\n",
    "                score += self.weights.get(f\"t_{prev_label}_{label}\", 0)\n",
    "            \n",
    "            prev_label = label\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def approximate_forward_pass(self, sequence_features):\n",
    "        \"\"\"\n",
    "        APPROXIMATE forward pass\n",
    "        Uses unary potentials only, ignores transitions for gradient\n",
    "        Args:\n",
    "            sequence_features(list): the features of words in a sentence\n",
    "        Returns:\n",
    "            float: the total partitiion function (as a log)\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        if T == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        labels = self.label_list\n",
    "        total_log_Z = 0.0\n",
    "        \n",
    "        # Sum over positions (assumes independence - fast approximation)\n",
    "        for t in range(T):\n",
    "            scores = []\n",
    "            for label in labels:\n",
    "                word = sequence_features[t]['word']\n",
    "                score = self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "                if sequence_features[t].get('is_capitalized'):\n",
    "                    score += self.weights.get(f\"c_{label}\", 0)\n",
    "                scores.append(score)\n",
    "            \n",
    "            if scores:\n",
    "                max_score = max(scores)\n",
    "                total_log_Z += max_score + math.log(sum(math.exp(s - max_score) for s in scores))\n",
    "        \n",
    "        return total_log_Z\n",
    "    \n",
    "    def compute_batch_gradient_fast(self, X_batch, y_batch):\n",
    "        \"\"\"ULTRA-FAST gradient computation\n",
    "            Args:\n",
    "            Returns:\n",
    "                dict: a dictionary of all the batch gradients\n",
    "                float: the loss from each batch\n",
    "        \"\"\"\n",
    "        grad = defaultdict(float)\n",
    "        batch_loss = 0.0\n",
    "        \n",
    "        for seq_features, true_labels in zip(X_batch, y_batch):\n",
    "            # Compute true score (fast)\n",
    "            true_score = self.compute_score(seq_features, true_labels)\n",
    "            \n",
    "            # Compute approximate log_Z (very fast)\n",
    "            log_Z = self.approximate_forward_pass(seq_features)\n",
    "            \n",
    "            if math.isfinite(log_Z):\n",
    "                batch_loss -= (true_score - log_Z)\n",
    "                \n",
    "                # Boost true features (perceptron-style)\n",
    "                prev_label = None\n",
    "                for features, label in zip(seq_features, true_labels):\n",
    "                    # Emission\n",
    "                    word = features['word']\n",
    "                    grad[f\"w_{word}_{label}\"] += self.learning_rate\n",
    "                    \n",
    "                    if features['prev_word']:\n",
    "                        grad[f\"p_{features['prev_word']}_{label}\"] += self.learning_rate * 0.3\n",
    "                        \n",
    "                    if features.get('is_capitalized'):\n",
    "                        grad[f\"c_{label}\"] += self.learning_rate * 0.1\n",
    "                    \n",
    "                    # Transition\n",
    "                    if prev_label is not None:\n",
    "                        grad[f\"t_{prev_label}_{label}\"] += self.learning_rate * 0.5\n",
    "                    \n",
    "                    prev_label = label\n",
    "        \n",
    "        return grad, batch_loss\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Training function-optimized\n",
    "            Args:\n",
    "                X_train(list): a list of all the words in the training data\n",
    "                y_train(list): a list of the tag(s) for eahc word in the training data\n",
    "        \"\"\"\n",
    "        # Convert labels\n",
    "        y_train_single = []\n",
    "        for sentence_labels in y_train:\n",
    "            sentence_single = [next(iter(tag_set)) for tag_set in sentence_labels]\n",
    "            y_train_single.append(sentence_single)\n",
    "            self.all_labels.update(sentence_single)\n",
    "        \n",
    "        self.label_list = list(self.all_labels)  # Cache for speed\n",
    "        \n",
    "        print(f\"FAST training on {len(X_train)} sequences\")\n",
    "        print(\"Using approximate gradients\")\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            start_time = time.time()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Shuffle\n",
    "            indices = list(range(len(X_train)))\n",
    "            random.shuffle(indices)\n",
    "            X_shuffled = [X_train[i] for i in indices]\n",
    "            y_shuffled = [y_train_single[i] for i in indices]\n",
    "            \n",
    "            # Process batches\n",
    "            for batch_start in range(0, len(X_shuffled), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(X_shuffled))\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                y_batch = y_shuffled[batch_start:batch_end]\n",
    "                \n",
    "                # FAST gradient computation\n",
    "                grad, batch_loss = self.compute_batch_gradient_fast(X_batch, y_batch)\n",
    "                total_loss += batch_loss\n",
    "                \n",
    "                # Update weights\n",
    "                for feat, update in grad.items():\n",
    "                    # Simple update - skip complex regularization during training\n",
    "                    self.weights[feat] += update\n",
    "                    \n",
    "                    # Optional: lightweight clipping\n",
    "                    if abs(self.weights[feat]) > 20.0:\n",
    "                        self.weights[feat] = math.copysign(20.0, self.weights[feat])\n",
    "            \n",
    "            avg_loss = total_loss / len(X_train)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Iter {iteration}, Loss: {avg_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.learning_rate *= 0.9\n",
    "    \n",
    "    def viterbi_decode(self, sequence_features):\n",
    "        \"\"\"Optimized Viterbi\n",
    "            Args:\n",
    "                sequence_features(list): the features of every word in a sequence\n",
    "            Returns:\n",
    "                list: a list of the best tags for words all the way from the end of a sentence to the beginning\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        if T == 0:\n",
    "            return []\n",
    "        \n",
    "        labels = self.label_list\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize\n",
    "        for label in labels:\n",
    "            features = sequence_features[0]\n",
    "            word = features['word']\n",
    "            delta[0][label] = self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "            if features.get('is_capitalized'):\n",
    "                delta[0][label] += self.weights.get(f\"c_{label}\", 0)\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            features = sequence_features[t]\n",
    "            word = features['word']\n",
    "            \n",
    "            for current_label in labels:\n",
    "                best_score = -1e10\n",
    "                best_prev_label = None\n",
    "                \n",
    "                # Precompute emission once per (t, current_label)\n",
    "                emission_score = self.weights.get(f\"w_{word}_{current_label}\", 0)\n",
    "                if features.get('is_capitalized'):\n",
    "                    emission_score += self.weights.get(f\"c_{current_label}\", 0)\n",
    "                \n",
    "                for prev_label in labels:\n",
    "                    transition_score = self.weights.get(f\"t_{prev_label}_{current_label}\", 0)\n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        best_path = [None] * T\n",
    "        best_score = -1e10\n",
    "        \n",
    "        for label in labels:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Fast evaluation\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (seq_features, true_seq) in enumerate(zip(X_test, y_test)):\n",
    "            if i >= 200:  # Limit evaluation for speed\n",
    "                break\n",
    "                \n",
    "            pred_labels = self.viterbi_decode(seq_features)\n",
    "            for pred_label, true_set in zip(pred_labels, true_seq):\n",
    "                if pred_label in true_set:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DATASET TRAINING ===\n",
      "FAST training on 41686 sequences\n",
      "Using approximate gradients\n",
      "Iter 0, Loss: -395.7195, Time: 14.7s\n",
      "Iter 1, Loss: -540.5592, Time: 15.2s\n",
      "Iter 2, Loss: -573.1979, Time: 15.6s\n",
      "Iter 3, Loss: -590.2288, Time: 14.6s\n",
      "Iter 4, Loss: -600.7107, Time: 13.8s\n",
      "Iter 5, Loss: -608.0966, Time: 14.1s\n",
      "Iter 6, Loss: -613.8068, Time: 15.1s\n",
      "Iter 7, Loss: -618.3808, Time: 14.9s\n",
      "Iter 8, Loss: -622.1357, Time: 14.4s\n",
      "Iter 9, Loss: -625.2462, Time: 13.6s\n",
      "\n",
      "=== FINAL EVALUATION ===\n",
      "Test Accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize with optimized parameters\n",
    "crf = LinearChainConditionalRandomField(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=10,\n",
    "    batch_size=2000,  # Process 2000 sequences at once\n",
    "    l2_penalty=0.01\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "print(\"=== FULL DATASET TRAINING ===\")\n",
    "crf.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n=== FINAL EVALUATION ===\")\n",
    "test_accuracy = crf.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index Sentence #           Word  POS Tag\n",
       "0    0.0        1.0      Thousands  NNS   O\n",
       "1    1.0        1.0             of   IN   O\n",
       "2    2.0        1.0  demonstrators  NNS   O\n",
       "3    3.0        1.0           have  VBP   O\n",
       "4    4.0        1.0        marched  VBN   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the GMB dataset\n",
    "import pandas as pd\n",
    "#Read it in without headers with latin1 encoding\n",
    "gmb_pd = pd.read_csv(\"./GMB_dataset.txt\", sep=\"\\t\", header=None,encoding=\"latin1\")\n",
    "#Take the first row as the heading with the names in the file\n",
    "#Re-index the dataset appropriately\n",
    "gmb_pd.columns = gmb_pd.iloc[0]\n",
    "gmb_pd = gmb_pd[1:]\n",
    "gmb_pd.columns = ['Index','Sentence #','Word','POS','Tag']\n",
    "gmb_pd = gmb_pd.reset_index(drop=True)\n",
    "gmb_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cb647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sentences from the dataset (from kaggle)\n",
    "#A class to retrieve the sentences from the dataset\n",
    "class getsentence(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1.0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        #using pandas groupby to get every sentence(with the word, pos, and tag)\n",
    "        self.grouped = self.data.groupby(\"Sentence #\")[['Word', 'POS', 'Tag']].apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d5fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n",
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = getsentence(gmb_pd)\n",
    "sentences = getter.sentences\n",
    "print(len(sentences))\n",
    "#This is how a sentence will look like. \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f77ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 995 gazetteer words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gazetteers #This is good for locations (albeit it's more US focused)\n",
    "# Load gazetteer words into a set for O(1) lookups\n",
    "gazetteer_words = set()\n",
    "# The gazetteers corpus contains files for different countries\n",
    "for fileid in gazetteers.fileids():\n",
    "    words = gazetteers.words(fileid)\n",
    "    gazetteer_words.update(word.lower() for word in words)\n",
    "\n",
    "print(f\"Loaded {len(gazetteer_words)} gazetteer words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Modified for Named Entity Recognition (Looks at the previous word and the next word in the sentence)\n",
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    Converts a word into a list of features indices\n",
    "\n",
    "    Args:\n",
    "        sent (list): a list of words with their features\n",
    "        i (int): the number to get a word or tag in a sentence\n",
    "\n",
    "    Returns:\n",
    "        list: a list of features that belong to a word\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Basic word features - now as indices instead of dictionaries\n",
    "    features.append(f\"word.lower():{word_lower}\")\n",
    "    features.append(f\"word.istitle():{word.istitle()}\")\n",
    "    features.append(f\"word.isupper():{word.isupper()}\")\n",
    "    features.append(f\"postag:{postag}\")\n",
    "    features.append(f\"postag[:2]:{postag[:2]}\")\n",
    "    \n",
    "    in_gazetteer = word_lower in gazetteer_words\n",
    "    features.append(f\"in_gazetteer:{in_gazetteer}\")\n",
    "    \n",
    "    # Gazetteer + word shape combinations\n",
    "    if in_gazetteer:\n",
    "        features.append(\"gazetteer_istitle:True\" if word.istitle() else \"gazetteer_istitle:False\")\n",
    "        features.append(\"gazetteer_isupper:True\" if word.isupper() else \"gazetteer_isupper:False\")\n",
    "        features.append(\"gazetteer_all_lower:True\" if word.islower() else \"gazetteer_all_lower:False\")\n",
    "        \n",
    "        # Gazetteer + position in sentence\n",
    "        if i == 0:\n",
    "            features.append(\"gazetteer_sentence_start:True\")\n",
    "        if i == len(sent)-1:\n",
    "            features.append(\"gazetteer_sentence_end:True\")\n",
    "            \n",
    "    # Context features gazetteer\n",
    "    if i > 0:\n",
    "        prev_word = sent[i-1][0]\n",
    "        prev_lower = prev_word.lower()\n",
    "        prev_postag = sent[i-1][1]\n",
    "        \n",
    "        features.append(f\"-1:word.lower():{prev_lower}\")\n",
    "        features.append(f\"-1:postag:{prev_postag}\")\n",
    "        \n",
    "        # Previous word gazetteer features\n",
    "        prev_in_gazetteer = prev_lower in gazetteer_words\n",
    "        features.append(f\"-1:in_gazetteer:{prev_in_gazetteer}\")\n",
    "        \n",
    "        # Combined features: current AND previous in gazetteer\n",
    "        if in_gazetteer and prev_in_gazetteer:\n",
    "            features.append(\"both_current_prev_gazetteer:True\")\n",
    "    \n",
    "    #If it's the first word, add the BOS tag        \n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "    \n",
    "    #Look for the next word    \n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_lower = next_word.lower()\n",
    "        next_postag = sent[i+1][1]\n",
    "        \n",
    "        features.append(f\"+1:word.lower():{next_lower}\")\n",
    "        features.append(f\"+1:postag:{next_postag}\")\n",
    "        \n",
    "        # Next word gazetteer features\n",
    "        next_in_gazetteer = next_lower in gazetteer_words\n",
    "        features.append(f\"+1:in_gazetteer:{next_in_gazetteer}\")\n",
    "        \n",
    "        # Combined features: current AND next in gazetteer\n",
    "        if in_gazetteer and next_in_gazetteer:\n",
    "            features.append(\"both_current_next_gazetteer:True\")\n",
    "    \n",
    "    #If it's the end, add EOD feature\n",
    "    else:\n",
    "        features.append(\"EOS\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e951ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2399 sentences\n",
      "Test: 600 sentences\n"
     ]
    }
   ],
   "source": [
    "#Split dataset into training and testing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#I need to split the sentences for the CRF\n",
    "# Split into train and test sets (80/20)\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "print(f\"Training: {len(train_sentences)} sentences\")\n",
    "\n",
    "print(f\"Test: {len(test_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF for Named Entity Recognition (NER)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "class ConditionalRandomFieldNer:\n",
    "    \"\"\"\n",
    "    My implementation of a linear chain conditional random field\n",
    "    for named entity recognition\n",
    "    Did not finish in time\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_weights = defaultdict(float)\n",
    "        self.labels = set() #A set of all the labels (no duplicates)\n",
    "        self.feature_map = {} #A map of every feature {feature_string -> feature_index}\n",
    "        self.label_map = {} #A map of every label {label string -> label_index}\n",
    "    \n",
    "    def build_feature_maps(self, training_data):\n",
    "        \"\"\"\n",
    "        Build feature maps with labels\n",
    "\n",
    "        Args:\n",
    "            training_data (_type_): _description_\n",
    "        \"\"\"\n",
    "        all_features  = set()\n",
    "        for sentence in training_data:\n",
    "            #Collect the features and labels\n",
    "            features_seq = sent2features(sentence)\n",
    "            for feature_list in features_seq:\n",
    "                for feature_str in feature_list:\n",
    "                    all_features.add(feature_str)\n",
    "                    \n",
    "        \n",
    "        # Create mappings for the later functions\n",
    "        self.feature_map = {feat: idx for idx, feat in enumerate(sorted(all_features))}\n",
    "        self.label_map = {label: idx for idx, label in enumerate(sorted(self.labels))}\n",
    "        \n",
    "        print(f\"Built feature map with {len(self.feature_map)} features\")\n",
    "        print(f\"Labels: {list(self.label_map.keys())}\")\n",
    "    def convert_features(self, feature_list, current_label):\n",
    "        \"\"\"\n",
    "        Convert feature list to feature vector for a specific label\n",
    "        Args:\n",
    "            feature_list (list):\n",
    "            current_label(str)\n",
    "        Returns:\n",
    "        \n",
    "        \"\"\"\n",
    "        # This creates emission features: feature + label combinations\n",
    "        feature_vector = defaultdict(float)\n",
    "        \n",
    "        for feature_str in feature_list:\n",
    "            # Emission feature: feature + current_label\n",
    "            emission_feature = f\"{feature_str}|||{current_label}\"\n",
    "            feature_vector[emission_feature] += 1.0\n",
    "            \n",
    "            # Also include the base feature (without label)\n",
    "            feature_vector[feature_str] += 1.0\n",
    "        \n",
    "        return feature_vector\n",
    "    \n",
    "    def compute_transition_features(self, prev_label, current_label):\n",
    "        \"\"\"Create transition features between labels\"\"\"\n",
    "        transition_feature = f\"TRANSITION:{prev_label}{current_label}\"\n",
    "        return {transition_feature: 1.0}\n",
    "    \n",
    "    def compute_sequence_score(self, features_sequence, labels_sequence):\n",
    "        \"\"\"Compute score for a given feature sequence and label sequence\"\"\"\n",
    "        total_score = 0.0\n",
    "        \n",
    "        for t in range(len(features_sequence)):\n",
    "            # Emission score\n",
    "            emission_features = self.convert_features(features_sequence[t], labels_sequence[t])\n",
    "            emission_score = sum(self.feature_weights[feat] * value \n",
    "                               for feat, value in emission_features.items())\n",
    "            total_score += emission_score\n",
    "            \n",
    "            # Transition score (except for first position)\n",
    "            if t > 0:\n",
    "                transition_features = self.compute_transition_features(\n",
    "                    labels_sequence[t-1], labels_sequence[t]\n",
    "                )\n",
    "                transition_score = sum(self.feature_weights[feat] * value \n",
    "                                     for feat, value in transition_features.items())\n",
    "                total_score += transition_score\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def viterbi_decode(self,features_sequence, possible_labels):\n",
    "        \"\"\"Viterbi algorithm for finding the most likely label sequence\"\"\"\n",
    "        T = len(features_sequence)\n",
    "        \n",
    "        # Initialize delta and psi\n",
    "        delta = [defaultdict(lambda: -float('inf')) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize first position\n",
    "        for label in possible_labels:\n",
    "            # Only emission features for first position (no transition)\n",
    "            emission_features = self.convert_features(features_sequence[0], label)\n",
    "            delta[0][label] = sum(self.feature_weights[feat] * value \n",
    "                                for feat, value in emission_features.items())\n",
    "            psi[0][label] = \"\"  # No previous label\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels:\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                for prev_label in possible_labels:\n",
    "                    # Emission features for current position\n",
    "                    emission_features = self.convert_features(features_sequence[t], current_label)\n",
    "                    emission_score = sum(self.feature_weights[feat] * value \n",
    "                                       for feat, value in emission_features.items())\n",
    "                    \n",
    "                    # Transition features\n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.feature_weights[feat] * value \n",
    "                                         for feat, value in transition_features.items())\n",
    "                    \n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        best_path = [None] * T\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Find best final label\n",
    "        for label in possible_labels:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        # Backtrack through the sequence\n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path, best_score\n",
    "    def process_sentence():\n",
    "        loss = 0.0\n",
    "        return loss, gradient\n",
    "    def fit(self, training_data, iterations=20, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        This handles the training of the CRF for NER\n",
    "        Args:\n",
    "            training_data (list): a list of a list of sentences\n",
    "            iterations (int, optional): the number of iterations for training. Defaults to 20.\n",
    "            learning_rate (float, optional): learning rate of the model. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        print(\"Building feature maps...\")\n",
    "        self.build_feature_maps(training_data)\n",
    "        \n",
    "        print(f\"Starting training with {len(training_data)} sentences...\")\n",
    "        for iteration in range(iterations):\n",
    "            total_loss = 0.0\n",
    "            for sentence in training_data:\n",
    "                loss = self.process_sentence(sentence)\n",
    "                total_loss += loss\n",
    "            \n",
    "        print(f\"Iteration {iteration}, Loss: {total_loss:.4f}\")\n",
    "        #Learning rate decay\n",
    "        self.learning_rate *= 0.95\n",
    "    def predict(self, sentence):\n",
    "        \"\"\"Predict labels for a sentence\"\"\"\n",
    "        features_sequence = sent2features(sentence)\n",
    "        possible_labels = list(self.label_map.keys())  # All possible labels\n",
    "        \n",
    "        best_path, best_score = self.viterbi_decode(features_sequence, possible_labels)\n",
    "        return best_path\n",
    "    \n",
    "    def model_evaluation(self, X_test, y_test):\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        all_true_labels = []\n",
    "        all_predicted_labels = []\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            true_labels = sent2labels(sentence)\n",
    "            predicted_labels = self.predict(sentence)\n",
    "            \n",
    "            all_true_labels.extend(true_labels)\n",
    "            all_predicted_labels.extend(predicted_labels)\n",
    "            \n",
    "            correct_predictions += sum(1 for t, p in zip(true_labels, predicted_labels) if t == p)\n",
    "            total_predictions += len(true_labels)\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        #Something to work on\n",
    "        precision = accuracy\n",
    "        recall = accuracy\n",
    "        f1_score = (2 *precision * recall) / (precision + recall)\n",
    "        return precision, recall, accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now train the model with the GMB dataset\n",
    "crf = ConditionalRandomFieldNer()\n",
    "crf.fit()\n",
    "#Return the model evaluation\n",
    "model_precision, model_recall, model_accuracy, model_f1_score = crf.model_evaluation()\n",
    "\n",
    "#Print out the model's evaluation\n",
    "print(f\"Model Precision: {model_precision * 100:.2f}\"\n",
    "      f\"Model Recall: {model_recall * 100:.2f}\"\n",
    "      f\"Model Accuracy: {model_accuracy * 100:.2f}\"\n",
    "      f\"Model's F1_score: {model_f1_score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
