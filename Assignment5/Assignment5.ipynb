{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code (It was more than 1))\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n",
      "Input sentence: ['Computer', 'science', 'is', 'cool', 'but', 'very', 'hard', '.']\n",
      "HMM prediction of second long sentence:  [('Computer', 'CD'), ('science', 'CD'), ('is', 'CD'), ('cool', 'CD'), ('but', 'CD'), ('very', 'CD'), ('hard', 'CD'), ('.', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n",
    "\n",
    "possible_unknown1 = [\"Computer\", \"science\", \"is\", \"cool\", \"but\", \"very\", \"hard\", \".\"]\n",
    "predicted_possible_unknown1 = hmm.predict(possible_unknown1)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_possible_unknown1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798aebc7",
   "metadata": {},
   "source": [
    "I created these sentences with tags via this website:\n",
    "https://parts-of-speech.info/\n",
    "And this is what the website predicted for each sentence\n",
    "For sentence 1: [DT, NN, VBD] - Matches the HMM Model's prediction\n",
    "\n",
    "For sentence 2: [NNP, MD, VB, DT, NN, RB] - Matches the HMM's prediction\n",
    "\n",
    "For sentence 3: [PRP, VBP, WRB, VB, DT, NN] - Matches the HMM's prediction\n",
    "\n",
    "For sentence 4: [PRP, VBP, RB, JJ] - Matches the HMM's prediction except for the punctuation\n",
    "\n",
    "For long sentence 1: [DT, NN, NN, VBD, IN, DT, NN, VBZ, VBN, VBN, NN, PRP, VBP, NN, MD, VB, VBN] - Matches the HMM's prediction except for the punctuations\n",
    "\n",
    "For long sentence 2: [NN, DT, NN, VBD, IN, DT, JJ, NN, MD, VB, IN, DT, NN, WP, VBZ, VBN, TO, DT, NN] - The model predicted that Today was an RB and it had TO as a tag unlike the website\n",
    "\n",
    "For possible unknown sentence 1: Here everything is tagegd as CD-cardinal. This might be because the model doesn't know these words in context, so it gave it CD as the most likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeksforgeeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    pos_tag = sentence[i][1]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'pos' : pos_tag,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'prev_pos': '' if i == 0 else sentence[i-1][1],  # Previous word's POS tag\n",
    "        'next_pos': '' if i == len(sentence)-1 else sentence[i+1][1],  # Next word's POS tag\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        #print(f\"Sentence[i][0]: {sentence[i][0]}\") \n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        #print(f\"Sentence[i][1] is: {sentence[i][1]}\")\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce92284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the x_train is : 41686\n",
      "The length of the y_train is : 41686\n",
      "The length of the X_test is : 10422\n",
      "The length of the y_test is : 10422\n"
     ]
    }
   ],
   "source": [
    "#check the size of the training and test sets\n",
    "print(f\"The length of the x_train is : {len(X_train)}\")\n",
    "print(f\"The length of the y_train is : {len(y_train)}\")\n",
    "print(f\"The length of the X_test is : {len(X_test)}\")\n",
    "print(f\"The length of the y_test is : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class LinearChainConditionalRandomField:\n",
    "    def __init__(self, learning_rate=0.1, max_iter=15, batch_size=500, l2_penalty=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_penalty = l2_penalty\n",
    "        self.weights = defaultdict(float)\n",
    "        self.all_labels = set()\n",
    "        self.label_list = []  # Cache for faster iteration\n",
    "    \n",
    "    def convert_features_fast(self, features_dict, label):\n",
    "        \"\"\"DRAMATICALLY simplified features - 5x faster\"\"\"\n",
    "        feature_vector = {}\n",
    "        \n",
    "        # ONLY these 3 features - remove the rest\n",
    "        word = features_dict['word']\n",
    "        feature_vector[f\"w_{word}_{label}\"] = 1\n",
    "        \n",
    "        if features_dict['prev_word']:\n",
    "            feature_vector[f\"p_{features_dict['prev_word']}_{label}\"] = 1\n",
    "            \n",
    "        if features_dict.get('is_capitalized'):\n",
    "            feature_vector[f\"c_{label}\"] = 1\n",
    "            \n",
    "        return feature_vector\n",
    "    \n",
    "    def compute_score_fast(self, sequence_features, labels):\n",
    "        \"\"\"Optimized scoring - direct dict lookups\"\"\"\n",
    "        score = 0.0\n",
    "        prev_label = None\n",
    "        \n",
    "        for features, label in zip(sequence_features, labels):\n",
    "            # Direct dict lookups - no function calls\n",
    "            word = features['word']\n",
    "            score += self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "            \n",
    "            if features['prev_word']:\n",
    "                score += self.weights.get(f\"p_{features['prev_word']}_{label}\", 0)\n",
    "                \n",
    "            if features.get('is_capitalized'):\n",
    "                score += self.weights.get(f\"c_{label}\", 0)\n",
    "            \n",
    "            if prev_label is not None:\n",
    "                score += self.weights.get(f\"t_{prev_label}_{label}\", 0)\n",
    "            \n",
    "            prev_label = label\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def approximate_forward_pass(self, sequence_features):\n",
    "        \"\"\"\n",
    "        APPROXIMATE forward pass - 100x faster than exact\n",
    "        Uses unary potentials only, ignores transitions for gradient\n",
    "        \"\"\"\n",
    "        T = len(sequence_features)\n",
    "        if T == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        labels = self.label_list\n",
    "        total_log_Z = 0.0\n",
    "        \n",
    "        # Sum over positions (assumes independence - fast approximation)\n",
    "        for t in range(T):\n",
    "            scores = []\n",
    "            for label in labels:\n",
    "                word = sequence_features[t]['word']\n",
    "                score = self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "                if sequence_features[t].get('is_capitalized'):\n",
    "                    score += self.weights.get(f\"c_{label}\", 0)\n",
    "                scores.append(score)\n",
    "            \n",
    "            if scores:\n",
    "                max_score = max(scores)\n",
    "                total_log_Z += max_score + math.log(sum(math.exp(s - max_score) for s in scores))\n",
    "        \n",
    "        return total_log_Z\n",
    "    \n",
    "    def compute_batch_gradient_fast(self, X_batch, y_batch):\n",
    "        \"\"\"ULTRA-FAST gradient computation\"\"\"\n",
    "        grad = defaultdict(float)\n",
    "        batch_loss = 0.0\n",
    "        \n",
    "        for seq_features, true_labels in zip(X_batch, y_batch):\n",
    "            # Compute true score (fast)\n",
    "            true_score = self.compute_score_fast(seq_features, true_labels)\n",
    "            \n",
    "            # Compute approximate log_Z (very fast)\n",
    "            log_Z = self.approximate_forward_pass(seq_features)\n",
    "            \n",
    "            if math.isfinite(log_Z):\n",
    "                batch_loss -= (true_score - log_Z)\n",
    "                \n",
    "                # Boost true features (perceptron-style)\n",
    "                prev_label = None\n",
    "                for features, label in zip(seq_features, true_labels):\n",
    "                    # Emission\n",
    "                    word = features['word']\n",
    "                    grad[f\"w_{word}_{label}\"] += self.learning_rate\n",
    "                    \n",
    "                    if features['prev_word']:\n",
    "                        grad[f\"p_{features['prev_word']}_{label}\"] += self.learning_rate * 0.3\n",
    "                        \n",
    "                    if features.get('is_capitalized'):\n",
    "                        grad[f\"c_{label}\"] += self.learning_rate * 0.1\n",
    "                    \n",
    "                    # Transition\n",
    "                    if prev_label is not None:\n",
    "                        grad[f\"t_{prev_label}_{label}\"] += self.learning_rate * 0.5\n",
    "                    \n",
    "                    prev_label = label\n",
    "        \n",
    "        return grad, batch_loss\n",
    "    \n",
    "    def fit_fast(self, X_train, y_train):\n",
    "        \"\"\"Training function-optimized\n",
    "            Args:\n",
    "            Returns'\n",
    "        \"\"\"\n",
    "        # Convert labels\n",
    "        y_train_single = []\n",
    "        for sentence_labels in y_train:\n",
    "            sentence_single = [next(iter(tag_set)) for tag_set in sentence_labels]\n",
    "            y_train_single.append(sentence_single)\n",
    "            self.all_labels.update(sentence_single)\n",
    "        \n",
    "        self.label_list = list(self.all_labels)  # Cache for speed\n",
    "        \n",
    "        print(f\"FAST training on {len(X_train)} sequences\")\n",
    "        print(\"Using approximate gradients\")\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            start_time = time.time()\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # Shuffle\n",
    "            indices = list(range(len(X_train)))\n",
    "            random.shuffle(indices)\n",
    "            X_shuffled = [X_train[i] for i in indices]\n",
    "            y_shuffled = [y_train_single[i] for i in indices]\n",
    "            \n",
    "            # Process batches\n",
    "            for batch_start in range(0, len(X_shuffled), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(X_shuffled))\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                y_batch = y_shuffled[batch_start:batch_end]\n",
    "                \n",
    "                # FAST gradient computation\n",
    "                grad, batch_loss = self.compute_batch_gradient_fast(X_batch, y_batch)\n",
    "                total_loss += batch_loss\n",
    "                \n",
    "                # Update weights\n",
    "                for feat, update in grad.items():\n",
    "                    # Simple update - skip complex regularization during training\n",
    "                    self.weights[feat] += update\n",
    "                    \n",
    "                    # Optional: lightweight clipping\n",
    "                    if abs(self.weights[feat]) > 20.0:\n",
    "                        self.weights[feat] = math.copysign(20.0, self.weights[feat])\n",
    "            \n",
    "            avg_loss = total_loss / len(X_train)\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Iter {iteration}, Loss: {avg_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "            \n",
    "            # Learning rate decay\n",
    "            self.learning_rate *= 0.9\n",
    "    \n",
    "    def viterbi_decode_fast(self, sequence_features):\n",
    "        \"\"\"Optimized Viterbi\"\"\"\n",
    "        T = len(sequence_features)\n",
    "        if T == 0:\n",
    "            return []\n",
    "        \n",
    "        labels = self.label_list\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize\n",
    "        for label in labels:\n",
    "            features = sequence_features[0]\n",
    "            word = features['word']\n",
    "            delta[0][label] = self.weights.get(f\"w_{word}_{label}\", 0)\n",
    "            if features.get('is_capitalized'):\n",
    "                delta[0][label] += self.weights.get(f\"c_{label}\", 0)\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            features = sequence_features[t]\n",
    "            word = features['word']\n",
    "            \n",
    "            for current_label in labels:\n",
    "                best_score = -1e10\n",
    "                best_prev_label = None\n",
    "                \n",
    "                # Precompute emission once per (t, current_label)\n",
    "                emission_score = self.weights.get(f\"w_{word}_{current_label}\", 0)\n",
    "                if features.get('is_capitalized'):\n",
    "                    emission_score += self.weights.get(f\"c_{current_label}\", 0)\n",
    "                \n",
    "                for prev_label in labels:\n",
    "                    transition_score = self.weights.get(f\"t_{prev_label}_{current_label}\", 0)\n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        best_path = [None] * T\n",
    "        best_score = -1e10\n",
    "        \n",
    "        for label in labels:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path\n",
    "    \n",
    "    def evaluate_fast(self, X_test, y_test):\n",
    "        \"\"\"Fast evaluation\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (seq_features, true_seq) in enumerate(zip(X_test, y_test)):\n",
    "            if i >= 200:  # Limit evaluation for speed\n",
    "                break\n",
    "                \n",
    "            pred_labels = self.viterbi_decode_fast(seq_features)\n",
    "            for pred_label, true_set in zip(pred_labels, true_seq):\n",
    "                if pred_label in true_set:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DATASET TRAINING ===\n",
      "FAST training on 41686 sequences\n",
      "Using approximate gradients\n",
      "Iter 0, Loss: -395.9693, Time: 15.1s\n",
      "Iter 1, Loss: -540.5333, Time: 14.5s\n",
      "Iter 2, Loss: -573.1614, Time: 14.3s\n",
      "Iter 3, Loss: -590.1856, Time: 14.3s\n",
      "Iter 4, Loss: -600.6617, Time: 14.5s\n",
      "Iter 5, Loss: -608.0557, Time: 14.2s\n",
      "Iter 6, Loss: -613.7582, Time: 14.4s\n",
      "Iter 7, Loss: -618.3364, Time: 14.7s\n",
      "Iter 8, Loss: -622.0952, Time: 14.8s\n",
      "Iter 9, Loss: -625.2018, Time: 14.6s\n",
      "\n",
      "=== FINAL EVALUATION ===\n",
      "Test Accuracy: 0.8514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize with optimized parameters\n",
    "crf = LinearChainConditionalRandomField(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=10,\n",
    "    batch_size=2000,  # Process 2000 sequences at once\n",
    "    l2_penalty=0.01\n",
    ")\n",
    "\n",
    "# Train on full dataset\n",
    "print(\"=== FULL DATASET TRAINING ===\")\n",
    "crf.fit_fast(\n",
    "    X_train, \n",
    "    y_train,\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n=== FINAL EVALUATION ===\")\n",
    "test_accuracy = crf.evaluate_fast(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index Sentence #           Word  POS Tag\n",
       "0    0.0        1.0      Thousands  NNS   O\n",
       "1    1.0        1.0             of   IN   O\n",
       "2    2.0        1.0  demonstrators  NNS   O\n",
       "3    3.0        1.0           have  VBP   O\n",
       "4    4.0        1.0        marched  VBN   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the GMB dataset\n",
    "import pandas as pd\n",
    "#Read it in without headers with latin1 encoding\n",
    "gmb_pd = pd.read_csv(\"./GMB_dataset.txt\", sep=\"\\t\", header=None,encoding=\"latin1\")\n",
    "#Take the first row as the heading with the names in the file\n",
    "#Re-index the dataset appropriately\n",
    "gmb_pd.columns = gmb_pd.iloc[0]\n",
    "gmb_pd = gmb_pd[1:]\n",
    "gmb_pd.columns = ['Index','Sentence #','Word','POS','Tag']\n",
    "gmb_pd = gmb_pd.reset_index(drop=True)\n",
    "gmb_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21cb647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sentences\n",
    "#A class to retrieve the sentences from the dataset\n",
    "class getsentence(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1.0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\")[['Word', 'POS', 'Tag']].apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1d5fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = getsentence(gmb_pd)\n",
    "sentences = getter.sentences\n",
    "#This is how a sentence will look like. \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Modified for Named Entity Recognition (Looks at the previous word and the next word in the sentence)\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run my read-in data through the word feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and testing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X is the words in the sentences and y is the tags\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF for Named Entity Recognition (NER)\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "class ConditionalRandomFieldNer:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=50, batch_size = 2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.batch_size = batch_size\n",
    "    def viterbi_decode():\n",
    "        \"\"\"Viterbi algorithm for finding the most likely label sequence\"\"\"\n",
    "        T = len(sequence_features)\n",
    "        delta = [defaultdict(float) for _ in range(T)]\n",
    "        psi = [defaultdict(str) for _ in range(T)]\n",
    "        \n",
    "        # Initialize the first word/tag in the sequence\n",
    "        for label in possible_labels[0]:\n",
    "            features = self.convert_features(sequence_features[0], label)\n",
    "            delta[0][label] = sum(self.weights[feat] * value for feat, value in features.items())\n",
    "            psi[0][label] = None\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for current_label in possible_labels[t]:\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                for prev_label in possible_labels[t-1]:\n",
    "                    # Emission features\n",
    "                    emission_features = self.convert_features(sequence_features[t], current_label)\n",
    "                    emission_score = sum(self.weights[feat] * value for feat, value in emission_features.items())\n",
    "                    \n",
    "                    # Transition features\n",
    "                    transition_features = self.compute_transition_features(prev_label, current_label)\n",
    "                    transition_score = sum(self.weights[feat] * value for feat, value in transition_features.items())\n",
    "                    \n",
    "                    score = delta[t-1][prev_label] + emission_score + transition_score\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_prev_label = prev_label\n",
    "                \n",
    "                delta[t][current_label] = best_score\n",
    "                psi[t][current_label] = best_prev_label\n",
    "        \n",
    "        # Backtrack\n",
    "        #Initialize to nothing (for best path an array of size T with None)\n",
    "        #for the best score, make it a float with -infinity\n",
    "        best_path = [None] * T\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        # Find best final label\n",
    "        for label in possible_labels[T-1]:\n",
    "            if delta[T-1][label] > best_score:\n",
    "                best_score = delta[T-1][label]\n",
    "                best_path[T-1] = label\n",
    "        \n",
    "        # Backtrack through the sequence\n",
    "        for t in range(T-2, -1, -1):\n",
    "            best_path[t] = psi[t+1][best_path[t+1]]\n",
    "        \n",
    "        return best_path, best_score \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #Learning rate decay\n",
    "        self.learning_rate *= 0.95\n",
    "    def predict(self, X_train):\n",
    "        predictions = []\n",
    "        return predictions\n",
    "    def model_evaluation(self, X_test, y_test):\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        accuracy = 0.0\n",
    "        f1_score = (2 *precision * recall) / (precision + recall)\n",
    "        return precision, recall, accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now train the model with the GMB dataset\n",
    "crf = ConditionalRandomFieldNer()\n",
    "#Return the model evaluation\n",
    "model_precision, model_recall, model_accuracy, model_f1_score = crf.model_evaluation()\n",
    "\n",
    "#Print out the model's evaluation\n",
    "print(f\"Model Precision: {model_precision * 100:.2f}\"\n",
    "      f\"Model Recall: {model_recall * 100:.2f}\"\n",
    "      f\"Model Accuracy: {model_accuracy * 100:.2f}\"\n",
    "      f\"Model's F1_score: {model_f1_score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
