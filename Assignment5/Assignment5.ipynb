{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code (It was more than 1))\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "\n",
    "possible_unknown = [\"The\", \"hidden\", \"markov\", \"model\", \"is\", \"working\", \"well\", \".\"]\n",
    "possible_unknown = [\"Computer\", \"science\", \"is\", \"cool\", \"but\", \"very\", \"hard\", \".\"]\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeks2geeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        #print(f\"Sentence[i][0]: {sentence[i][0]}\") \n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        #print(f\"Sentence[i][1] is: {sentence[i][1]}\")\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "#Need to modify to handle more than one tag\n",
    "class ConditionalRandomField():\n",
    "    '''\n",
    "    My implementation of linear-chain Conditional Random Field\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = defaultdict(float) #Use the defaultdict to make a dictionary fo floats for the weights\n",
    "        self.labels = set() #A set for all the possible POS tags\n",
    "        \n",
    "    def feature_extraction(self, feature_dict, current_tags, prev_tags=None):\n",
    "        '''Extract all the features for the states and transitions'''\n",
    "        features = []\n",
    "        \n",
    "        #Get the state features for the current tag\n",
    "        for current_tag in current_tags:\n",
    "            for key,value in feature_dict.items():\n",
    "                #Hanld if the value is a boolean item\n",
    "                if isinstance(value, bool):\n",
    "                    if value:\n",
    "                        features.append(f\"{key}=True{current_tag}\")\n",
    "                #Handle if the value is a string\n",
    "                elif isinstance(value, str):\n",
    "                    #Skip empty strings\n",
    "                    if value:\n",
    "                        features.append(f\"{key}={value}_{current_tag}\")\n",
    "                #If it's not a boolean value or a string, append it as what is\n",
    "                else:\n",
    "                    features.append(f\"{key}={value}_{current_tag}\")\n",
    "                    \n",
    "        \n",
    "        #get the transition features (previous tag -> current tag)\n",
    "        if prev_tags is not None:\n",
    "            for prev_tag in prev_tags:\n",
    "                for current_tag in current_tags:\n",
    "                    features.append(f\"trans_{prev_tag}_{current_tag}\")\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def compute_score(self, features:list):\n",
    "        return sum(self.weights[f] for f in features)\n",
    "    \n",
    "    def compute_log_likelihood(self, sentence_features:list, true_tags, Z:float):\n",
    "        \"\"\"Compute log-likelihood for a sentence\n",
    "        \"\"\"\n",
    "        log_likelihood = 0.0\n",
    "        \n",
    "        # Add state feature scores\n",
    "        for i, true_label in enumerate(true_tags):\n",
    "            features = self.feature_extraction(sentence_features[i], true_label)\n",
    "            log_likelihood += self.compute_score(features)\n",
    "        \n",
    "        # Add transition feature scores\n",
    "        for i in range(1, len(true_tags)):\n",
    "            prev_label = true_tags[i-1]\n",
    "            current_label = true_tags[i]\n",
    "            trans_feat = f\"trans_{prev_label}_{current_label}\"\n",
    "            log_likelihood += self.weights[trans_feat]\n",
    "        \n",
    "        # Subtract log partition function\n",
    "        log_likelihood -= math.log(Z) if Z > 0 else 0\n",
    "        \n",
    "        return log_likelihood\n",
    "        \n",
    "    def train(self,X_train:list, y_train:list, learning_rate:float=0.01,iterations:int=10):\n",
    "        \"\"\"\n",
    "        Train the linear-chain CRF using the X_train and the y_train\n",
    "        Args:\n",
    "            X_train (list): a list of features\n",
    "            y_train (list): a list of words that are associated with the feature\n",
    "            learning_rate (float, optional): _description_. Defaults to 0.01.\n",
    "            iterations (int, optional): _description_. Defaults to 10.\n",
    "        \"\"\"\n",
    "        #Collect all the labels in the y_train\n",
    "        for sentence_tags in y_train:\n",
    "            for tag_set in sentence_tags:\n",
    "                for tag in tag_set:\n",
    "                    self.labels.add(tag)\n",
    "                \n",
    "        #Debugging statement to see if tags were added        \n",
    "        print(f\"Training with {len(self.labels)} labels individual labels: {list(self.labels)}\")\n",
    "        \n",
    "        #Convert the y-train to a sequence of possible tags\n",
    "        \n",
    "        #had an error here because it couldn't iterate over an integer\n",
    "        for iteration in range(iterations):\n",
    "            total_loss = 0.0\n",
    "            #Make it so the model can see the sentence feature and the tag for that feature\n",
    "            for sentence_features, actual_tags in zip(X_train, y_train):\n",
    "                n = len(sentence_features)\n",
    "                \n",
    "                #Forward-backward algorithm using the features in X_train\n",
    "                alpha = self.forward_algorithm(sentence_features)\n",
    "                beta = self.backward_algorithm(sentence_features)\n",
    "                \n",
    "                #Compute the partition function\n",
    "                Z = sum(alpha[n-1].values())\n",
    "                \n",
    "                #Compute the expected and actual feature counts\n",
    "                expected_feature_counts = defaultdict(float)\n",
    "                actual_feature_counts = defaultdict(float)\n",
    "                \n",
    "                #State feature extraction\n",
    "                for i in range(n):\n",
    "                    for label in self.labels:\n",
    "                        prob = (alpha[i][label] * beta[i][label]) / Z\n",
    "                        features = self.feature_extraction(sentence_features[i], label)\n",
    "                        for feat in features:\n",
    "                            expected_feature_counts[feat] += prob             \n",
    "                #Transition feature extraction\n",
    "                for i in range(1,n):\n",
    "                    for prev_label in self.labels:\n",
    "                        for current_label in self.labels:\n",
    "                            prob = (alpha[i-1][prev_label] * math.exp(self.compute_score(self.feature_extraction(sentence_features[i], current_label)))\n",
    "                                    * math.exp(self.compute_score([f\"trans_{prev_label}_{current_label}\"])) * beta[i][current_label]) / Z\n",
    "                            expected_feature_counts[f\"trans_{prev_label}_{current_label}\"] += prob\n",
    "                            \n",
    "                #Actual counts from the features\n",
    "                for i, true_label in enumerate(actual_tags):\n",
    "                    features = self.feature_extraction(sentence_features[i], true_label)\n",
    "                    for feat in features:\n",
    "                        actual_feature_counts[feat] += 1.0\n",
    "                    if i > 0:\n",
    "                        prev_label = true_label[i-1]\n",
    "                        actual_feature_counts[f\"trans_{prev_label}_{current_label}\"] += 1.0\n",
    "                \n",
    "                #Update the weights\n",
    "                all_features = set(expected_feature_counts.keys()) | set(actual_feature_counts.keys())\n",
    "                for feat in all_features:\n",
    "                    #Gradient = actual feature - expected feature\n",
    "                    gradient = actual_feature_counts[feat] - expected_feature_counts[feat]\n",
    "                    \n",
    "                    #Update the weights of that feature with the learning rate and gradient\n",
    "                    self.weights[feat] += learning_rate * gradient\n",
    "                #Calculate the log likelihood for this sentence\n",
    "                log_likelihood = self.compute_log_likelihood(sentence_features, actual_tags, Z)\n",
    "                \n",
    "                total_loss -= log_likelihood\n",
    "            print(f\"Iteration {iteration + 1}, Loss: {total_loss}\")\n",
    "    \n",
    "    def forward_algorithm(self, sentence_features, possible_tag_sequence):\n",
    "        n = len(sentence_features)\n",
    "        alpha = [defaultdict(float) for _ in range(n)]\n",
    "        \n",
    "        #Initialize the first position in the sentence\n",
    "        for tag in possible_tag_sequence:\n",
    "            features = self.feature_extraction(sentence_features[0], {tag})\n",
    "            alpha[0][frozenset({tag})] = math.exp(self.compute_score(features))\n",
    "        \n",
    "        #Forward pass\n",
    "        #Go through every other word in the sentence (1-n)\n",
    "        for i in range(1,n):\n",
    "            #Look at the current tag\n",
    "            for current_tag in possible_tag_sequence[i]:\n",
    "                current_tag_set = frozenset({current_tag})\n",
    "                total = 0.0\n",
    "                #get the current features and the score of that state\n",
    "                current_features = self.feature_extraction(sentence_features[i], {current_tag})\n",
    "                state_score = math.exp(self.compute_score(current_features))\n",
    "                \n",
    "                #Look at the previous labels\n",
    "                #Get the transition features and the score\n",
    "                for prev_tag in possible_tag_sequence[i-1]:\n",
    "                    prev_tag_set = frozenset({prev_tag})\n",
    "                    transition_features = [f\"trans_{prev_tag}_{current_tag}\"]\n",
    "                    transition_score = math.exp(self.compute_score(transition_features))\n",
    "                    \n",
    "                    total += alpha[i-1][prev_tag_set] * state_score * transition_score\n",
    "                    \n",
    "                alpha[i][current_tag_set] = total\n",
    "                \n",
    "        return alpha\n",
    "    \n",
    "    def backward_algorithm(self, sentence_features, possible_tag_set):\n",
    "        n = len(sentence_features)\n",
    "        beta = [defaultdict(float) for _ in range(n)]\n",
    "        \n",
    "        #Initialize the last position in the sentence\n",
    "        for label in self.labels:\n",
    "            beta[n-1][label] = 1.0\n",
    "            \n",
    "        #Backward pass\n",
    "        #Go all the way back to the first word in the sentence\n",
    "        for i in range(n-2, -1, -1):\n",
    "            for current_label in self.labels:\n",
    "                total = 0.0\n",
    "                #For the next label in the sentence\n",
    "                for next_label in self.labels:\n",
    "                    next_features = self.feature_extraction(sentence_features[i+1], next_label)\n",
    "                    state_score = math.exp(self.compute_score(next_features))\n",
    "                    transition_features = [f\"trans_{current_label}_{next_label}\"]\n",
    "                    transition_score = math.exp(self.compute_score(transition_features))\n",
    "                    total += beta[i+1][next_label] * state_score * transition_score\n",
    "                \n",
    "                beta[i][current_label] = total\n",
    "        return beta\n",
    "    def crf_viterbi(self, sentence_features):\n",
    "        \"\"\"\n",
    "        This is the viterbi algorithm for the linear-chain CRF\n",
    "        \"\"\"\n",
    "        n = len(sentence_features)\n",
    "        #viterbi matrix for the algorithm\n",
    "        dp = [defaultdict(float) for _ in range(n)]\n",
    "        #This is the backpointer matrix for the viterbi algorithm\n",
    "        backpointer = [defaultdict(str) for _ in range(n)]\n",
    "        \n",
    "        #Initialize the first position like in the HMM\n",
    "        for label in self.labels:\n",
    "            #Get the features for the first word in the sentence and the label\n",
    "            features = self.feature_extraction(sentence_features[0], label)\n",
    "            dp[0][label] = self.compute_score(features)\n",
    "        \n",
    "        #Now go through every word in the sentence\n",
    "        #Fill in the DP table\n",
    "        for i in range(1,n):\n",
    "            for current_label in self.labels:\n",
    "                #Set the best score and prev label to infinity and none\n",
    "                #because they're not known yet\n",
    "                best_score = -float('inf')\n",
    "                best_prev_label = None\n",
    "                \n",
    "                current_features = self.feature_extraction(sentence_features[i], current_label)\n",
    "                state_score = self.compute_score(current_features)\n",
    "                \n",
    "                for prev_label in self.labels:\n",
    "                    trans_features = [f\"trans_{prev_label}_{current_label}\"]\n",
    "                    transition_score = self.compute_score(trans_features)\n",
    "                    #Calculate with the previous word and previous label, state score, and transition score\n",
    "                    score = dp[i-1][prev_label] + state_score + transition_score\n",
    "                    \n",
    "                    #Same as the HMM, check to see if the score beats the best score\n",
    "                    if score > best_score:\n",
    "                        best_score = score #Replace the best score with the new score\n",
    "                        best_prev_label = prev_label #Make the best_prev_label the current previous label\n",
    "                \n",
    "                #Put the best score and best_prev_label into the viterbi matrix and backpointer matrix respectively        \n",
    "                dp[i][current_label] = best_score\n",
    "                backpointer[i][current_label] = best_prev_label\n",
    "            \n",
    "        #Backtracking\n",
    "        bestpath = []\n",
    "        best_final_label = max(dp[n-1].items(), key= lambda x: x[1])[0]\n",
    "        \n",
    "        current_label = best_final_label\n",
    "        for i in range(n-1, -1, -1):\n",
    "            bestpath.append(current_label)\n",
    "            if i > 0:\n",
    "                current_label = backpointer[i][current_label]\n",
    "        return bestpath[::1]\n",
    "    \n",
    "    def predict(self, X_test:list):\n",
    "        \"\"\"\n",
    "        A prediction function that predicts the parts of speech for a given sentence\n",
    "        Args:\n",
    "            X_test (list): a list containing the words for a sentence\n",
    "\n",
    "        Returns:\n",
    "            list: a list that contains the viterbi path from the last word to the first word\n",
    "        \"\"\"\n",
    "        prediction_result = []\n",
    "        for sentence_features in X_test:\n",
    "            pred_tags = self.crf_viterbi(sentence_features)\n",
    "            prediction_result.append(pred_tags)\n",
    "        return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "932e5cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 48 labels\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m crf \u001b[38;5;241m=\u001b[39m ConditionalRandomField()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m crf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[16], line 121\u001b[0m, in \u001b[0;36mConditionalRandomField.train\u001b[0;34m(self, X_train, y_train, learning_rate, iterations)\u001b[0m\n\u001b[1;32m    119\u001b[0m         actual_feature_counts[feat] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m         prev_label \u001b[38;5;241m=\u001b[39m \u001b[43mtrue_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    122\u001b[0m         actual_feature_counts[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrans_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#Update the weights\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "crf = ConditionalRandomField()\n",
    "crf.train(X_train, y_train, learning_rate=0.1, iterations=10)\n",
    "\n",
    "# Predict\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# Evaluate (simple accuracy)\n",
    "correct = 0\n",
    "total = 0\n",
    "for true_tags, pred_tags in zip(y_test, y_pred):\n",
    "    for t, p in zip(true_tags, pred_tags):\n",
    "        if t == p:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00755819",
   "metadata": {},
   "source": [
    "When I made my learning rate and iterattion the default, 0.01 and 10:\n",
    "Accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I need to read in the GMB dataset\n",
    "with open('GMB_dataset.txt', 'r') as gmb_file:\n",
    "    #Need a way to read in the text file with columns\n",
    "    line = gmb_file.readlines()\n",
    "    sentence = gmb_file.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Same as the one for CRF \n",
    "def word_feature(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences for the CRFs\n",
    "CRF_test_sentence1 = [\"She\", \"likes\", \"to\", \"read\",\"books\"]\n",
    "CRF_test_sentence2 = []\n",
    "CRF_test_sentence3 = []\n",
    "CRF_test_sentence4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
