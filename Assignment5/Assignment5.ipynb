{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "        #DEBUGGING TO SEE IF IT WORKS PROPERLY\n",
    "        #print(\"Sample transition probabilities:\")\n",
    "        #print(f\"DT -> NN: {self.transition_probs[self.state_to_idx['DT']][self.state_to_idx['NN']]}\")\n",
    "        #print(f\"NN -> VB: {self.transition_probs[self.state_to_idx['NN']][self.state_to_idx['VB']]}\")\n",
    "\n",
    "        #print(\"\\nSample emission probabilities:\")\n",
    "        #print(f\"P('The'|'DT'): {self.emission_probs[self.state_to_idx['DT']][self.obs_to_idx['The']]}\")\n",
    "        #print(f\"P('cat'|'NN'): {self.emission_probs[self.state_to_idx['NN']][self.obs_to_idx['cat']]}\")\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        tag_idx = self.state_to_idx[tag] #Forgot to add this and it lead to an error\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag_idx] = self.initial_probs[tag_idx] + 1 / (len(first_tags))\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros((len(self.states), len(self.states)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            #Accidentally used + instead of *\n",
    "                            transition_counts[prev_idx][curr_idx] +=  1 / (len(prev_tags) * len(current_tags))\n",
    "                            \n",
    "        #I need to normalize the transition matrix so it's between 0-1\n",
    "        row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "        self.transition_probs = np.divide(transition_counts, row_sums, \n",
    "                                    out=np.zeros_like(transition_counts), \n",
    "                                    where=row_sums!=0)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        #Need a temporary matrix that does all the calculations\n",
    "        #Then put it into the emission porbability matrix\n",
    "        emission_counts = np.zeros((len(self.states), len(self.observations)))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for word, tags in sentence:\n",
    "                if word in self.obs_to_idx:\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    for tag in tags:\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            emission_counts[tag_idx][word_idx] += 1 / len(tags)\n",
    "            \n",
    "        #Normalize the counts into probabilities (I forgot this, which caused an issue in the code)\n",
    "        row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
    "        self.emission_probs = np.divide(emission_counts, row_sums,\n",
    "                                    out=np.zeros_like(emission_counts),\n",
    "                                    where=row_sums!=0)\n",
    "        \n",
    "    def viterbi(self, sentence: list) -> np.ndarray:\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Debug to see how the input is\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "     \n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            \n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "            #Debugging statement to see what the initial viterbi row looks like\n",
    "            #print(f\"Initial viterbi row: {viterbi[0]}\")\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            #See if the current word's index exists\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            \n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                #Need to go through the previous states\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][current_word_idx]\n",
    "                    \n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob #make the current probability the new max probability\n",
    "                        best_prev_state = prev_state #make the current previous state the best previous state\n",
    "                \n",
    "                #After checking all the previous states, store the max probability adn the best previous state\n",
    "                #Into the viterbi and the backpointer prespectively        \n",
    "                viterbi[t][current_state] = max_prob\n",
    "                \n",
    "                # Debug statement to see what viterbi looks like after each time step\n",
    "                #print(f\"Viterbi at time {t}: {viterbi[t]}\")\n",
    "                \n",
    "                backpointer[t][current_state] = best_prev_state\n",
    "                         \n",
    "        #Backtracking now\n",
    "        #Get the last word of the sentence\n",
    "        last_word = len(sentence) - 1\n",
    "        #Get the best state for the last word with the argmax of the viterbi matrix\n",
    "        best_last_state = np.argmax(viterbi[last_word])\n",
    "        #Make a best path array with type int\n",
    "        bestpath = np.zeros(len(sentence), dtype=int)\n",
    "        #Make the best path of the last word the best last state\n",
    "        bestpath[last_word] = best_last_state\n",
    "        #Start from the second to last word and end at the beginning of the sentence\n",
    "        #n-2, n-3, ..., 0\n",
    "        for t in range(len(sentence)-2, -1, -1):\n",
    "            bestpath[t] = backpointer[t+1][bestpath[t+1]]\n",
    "            \n",
    "        #Return the best path and the best path's probability\n",
    "        return bestpath\n",
    "    \n",
    "    def predict(self, sentence: list) -> list:\n",
    "        \"\"\"\n",
    "        Predict the part-of-speech tags for each word in the sentence\n",
    "        Args:\n",
    "            sentence (list): a list of words the HMM predicts\n",
    "        Returns:\n",
    "            a list of tuples (word, and predicted tag)\n",
    "        \"\"\"\n",
    "        #Use the viterbi function\n",
    "        tag_indices = self.viterbi(sentence)\n",
    "        \n",
    "        #Convert indices to actual tag names\n",
    "        predicted_tags = [self.states[idx] for idx in tag_indices]\n",
    "        \n",
    "        #Pair words with predicted tags\n",
    "        return list(zip(sentence, predicted_tags))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: ['The', 'cat', 'sat']\n",
      "HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
      "Input sentence: ['Mark', 'will', 'pay', 'the', 'bill', 'soon']\n",
      "HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
      "Input sentence: ['I', 'know', 'how', 'watch', 'after', 'a', 'dog']\n",
      "HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
      "Input sentence: ['I', 'am', 'so', 'tired', '.']\n",
      "HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['The', 'police', 'department', 'said', 'that', 'the', 'suspect', 'has', 'been', 'apprehended', 'today', ',', 'they', 'hope', 'justice', 'will', 'be', 'served', '.']\n",
      "HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
      "Input sentence: ['Today', 'the', 'studio', 'announced', 'that', 'the', 'new', 'film', 'will', 'be', 'about', 'a', 'girl', 'who', 'is', 'transported', 'to', 'another', 'world', '.']\n",
      "HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"Mark\", \"will\", \"pay\", \"the\", \"bill\", \"soon\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.predict(test_sentence1)\n",
    "print(\"HMM prediction of first sentence: \", predicted_tags1)\n",
    "#Originally: HMM prediction of first sentence:  [('The', 'NPS'), ('cat', 'NPS'), ('sat', 'NPS')] - predicted it as NPs for some reason (Error with probability matrices)\n",
    "#Fixed it issue: HMM prediction of first sentence:  [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD')]\n",
    "predicted_tags2 = hmm.predict(test_sentence2)\n",
    "print(\"HMM prediction of second sentence: \", predicted_tags2)\n",
    "#HMM prediction of second sentence:  [('Mark', 'NNP'), ('will', 'MD'), ('pay', 'VB'), ('the', 'DT'), ('bill', 'NN'), ('soon', 'RB')]\n",
    "predicted_tags3 = hmm.predict(test_sentence3)\n",
    "print(\"HMM prediction of third sentence: \", predicted_tags3)\n",
    "#HMM prediction of third sentence:  [('I', 'PRP'), ('know', 'VBP'), ('how', 'WRB'), ('watch', 'NN'), ('after', 'IN'), ('a', 'DT'), ('dog', 'NN')]\n",
    "predicted_tags4 = hmm.predict(test_sentence4)\n",
    "print(\"HMM prediction of fourth sentence: \", predicted_tags4)\n",
    "#HMM prediction of fourth sentence:  [('I', 'PRP'), ('am', 'VBP'), ('so', 'RB'), ('tired', 'VBN'), ('.', '.')]\n",
    "predicted_long_tags1 = hmm.predict(test_sentence_long)\n",
    "print(\"HMM prediction of first long sentence: \", predicted_long_tags1)\n",
    "#HMM prediction of first long sentence:  [('The', 'DT'), ('police', 'NN'), ('department', 'NN'), ('said', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('suspect', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('apprehended', 'VBN'), ('today', 'RB'), (',', ','), ('they', 'PRP'), ('hope', 'VBP'), ('justice', 'NN'), ('will', 'MD'), ('be', 'VB'), ('served', 'VBN'), ('.', '.')]\n",
    "predicted_long_tags2 = hmm.predict(test_sentence_long2)\n",
    "print(\"HMM prediction of second long sentence: \", predicted_long_tags2)\n",
    "#HMM prediction of second long sentence:  [('Today', 'RB'), ('the', 'DT'), ('studio', 'NN'), ('announced', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('film', 'NN'), ('will', 'MD'), ('be', 'VB'), ('about', 'IN'), ('a', 'DT'), ('girl', 'NN'), ('who', 'WP'), ('is', 'VBZ'), ('transported', 'VBN'), ('to', 'TO'), ('another', 'DT'), ('world', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering for the extracted brown files\n",
    "#I got help from the geeks2geeks website\n",
    "#https://www.geeksforgeeks.org/nlp/conditional-random-fields-crfs-for-pos-tagging-in-nlp/\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sentence in full_data:\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    #Go through every sentence in the full data list\n",
    "    for i in range(len(sentence)):\n",
    "        #Append the word features into the X_sentence\n",
    "        X_sentence.append(word_features(sentence,i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "        \n",
    "    #Append the sentences into the original list\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "    \n",
    "    \n",
    "#Split the extracted files (80% training, 20% testing)\n",
    "split = int(0.8 * len(X))\n",
    "#Get every word,tag up to 80% of the orignal X and y\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "#Get the remaining 20% of the original X and y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ffb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF \n",
    "class ConditionalRandomField():\n",
    "    def __init__(self):\n",
    "        self.\n",
    "    def\n",
    "    def predict(self, sentence:list):\n",
    "        return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I need to read in the GMB dataset\n",
    "with open('GMB_dataset.txt', 'r') as gmb_file:\n",
    "    #Need a way to read in the text file with columns\n",
    "    line = gmb_file.readlines()\n",
    "    sentence = gmb_file.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "#Same as the one for CRF with\n",
    "def word_feature(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': i == 0, #if the word is a first word\n",
    "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
    "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
    "         #prefix of the word\n",
    "        'prefix-1': word[0],   \n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "         #suffix of the word\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "         #extracting previous word\n",
    "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
    "         #extracting next word\n",
    "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
    "        'has_hyphen': '-' in word,    #if word has hypen\n",
    "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
    "        'capitals_inside': word[1:].lower() != word[1:]\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences for the CRFs\n",
    "CRF_test_sentence1 = [\"She\", \"likes\", \"to\", \"read\",\"books\"]\n",
    "CRF_test_sentence2 = []\n",
    "CRF_test_sentence3 = []\n",
    "CRF_test_sentence4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
