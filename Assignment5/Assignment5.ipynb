{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545e22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Read in the extracted brown files\n",
    "import glob\n",
    "\n",
    "tagged_files = glob.glob(\"_extracted_brown/*.txt\") #Read in the files and creates a list\n",
    "print(type(tagged_files))\n",
    "print(len(tagged_files)) #Should be 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make the files into a list of a list of tuples\n",
    "The tuple contains a str(word) and a set(tag(s)) \n",
    "Tag(s) because some words in the file contain more than one tag\n",
    "'''\n",
    "#I got help from the website where we download the extarcted brown text files\n",
    "#https://kristopherkyle.github.io/Corpus-Linguistics-Working-Group/pos_tagging_1.html\n",
    "\n",
    "#divide into sentences\n",
    "full_data: list = []\n",
    "for file in tagged_files:\n",
    "    with open(file, 'r') as x:\n",
    "        text = x.read().split(\"\\n\\n\")\n",
    "        for sent in text:\n",
    "            sentence = []\n",
    "            for word_line in sent.split(\"\\n\"):\n",
    "                #Strip leading/trailing whitespace\n",
    "                word_line = word_line.strip()\n",
    "                \n",
    "                #Skip empty lines\n",
    "                if not word_line:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if split will work\n",
    "                parts = word_line.split(\" \", 1)\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                \n",
    "                #Continue getting the word and tag(s)\n",
    "                word_, pos = parts\n",
    "                pos_set:set = set(pos.split(\"|\"))\n",
    "                sentence.append((word_, pos_set))\n",
    "            \n",
    "            if sentence:\n",
    "                full_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data type: <class 'list'>\n",
      "Number of sentences: 52108\n",
      "First sentence type: <class 'list'>\n",
      "First sentence length: 17\n",
      "First item type: <class 'tuple'>\n",
      "First item: ('In', {'IN'})\n"
     ]
    }
   ],
   "source": [
    "#Better Sanity Check so I can see the structure\n",
    "print(f\"full_data type: {type(full_data)}\")\n",
    "print(f\"Number of sentences: {len(full_data)}\")\n",
    "\n",
    "if full_data:\n",
    "    first_sentence = full_data[0]\n",
    "    print(f\"First sentence type: {type(first_sentence)}\")\n",
    "    print(f\"First sentence length: {len(first_sentence)}\")\n",
    "    \n",
    "    if first_sentence:\n",
    "        first_item = first_sentence[0]\n",
    "        print(f\"First item type: {type(first_item)}\")\n",
    "        print(f\"First item: {first_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9467bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM Model\n",
    "import numpy as np\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self):\n",
    "        #Initialize everything when I first create the Hidden Markov Model\n",
    "        self.states = None\n",
    "        self.observations = None\n",
    "        \n",
    "        #I need these states/observations to index\n",
    "        #Because I need a way to calculate the probs (numpy understands integer indices, NOT strings!!!)\n",
    "        self.states_to_idx = None\n",
    "        self.states_to_idx = None\n",
    "        \n",
    "        #Make empty initial/tranmission/emission probabilities \n",
    "        #Since it's all learned during training\n",
    "        self.initial_probs = None\n",
    "        self.transition_probs = None\n",
    "        self.emission_probs = None\n",
    "        \n",
    "    def train_HMM(self, training_data: list):\n",
    "        \"\"\"\n",
    "        Trains the HMM on tagged data\n",
    "        Calculates the initial, transmission, and emission probabilities\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Build the states and observations from the training data\n",
    "        #Make them sets, since they don't allow duplication\n",
    "        all_states = set()\n",
    "        all_observations = set()\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #Observations are based on the words\n",
    "                all_observations.add(word)\n",
    "                #The states are the tags\n",
    "                all_states.update(tags)\n",
    "        \n",
    "        #Make the states and observations into lists\n",
    "        self.states = list(all_states)\n",
    "        self.observations = list(all_observations)\n",
    "        \n",
    "        #Make my state/observation index\n",
    "        self.state_to_idx: dict = {state: i for i, state in enumerate(self.states)}\n",
    "        self.obs_to_idx: dict = {obs: i for i, obs in enumerate(self.observations)}\n",
    "        \n",
    "        #initialize the empty matrices\n",
    "        n_states = len(self.states)\n",
    "        n_observations = len(self.observations)\n",
    "        self.initial_probs = np.zeros(n_states)\n",
    "        self.transition_probs = np.zeros((n_states, n_states))\n",
    "        self.emission_probs = np.zeros((n_states, n_observations))\n",
    "        \n",
    "        #Now calculate the all the probabilities\n",
    "        self.calculate_initial_probabilities(training_data)\n",
    "        self.calculate_transition_probabilities(training_data)\n",
    "        self.calculate_emission_probabilities(training_data)\n",
    "        \n",
    "    def calculate_initial_probabilities(self,training_data: list):\n",
    "        \"\"\"\n",
    "        Calculate the intial state probabilities P(tag|start)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            #Check to see if the sentence is empty\n",
    "            if sentence:\n",
    "                #Get the first words and tag(s) in the sentence\n",
    "                first_word,first_tags = sentence[0]\n",
    "                #Handle if the word has multiple tags\n",
    "                for tag in first_tags:\n",
    "                    #If the tag is in the state indec dictionary\n",
    "                    if tag in self.state_to_idx:\n",
    "                        #Fractional count if there's multiple tags\n",
    "                        self.initial_probs[tag] += 1 / len(first_tags)\n",
    "    \n",
    "    def calculate_transition_probabilities(self, training_data:list):\n",
    "        \"\"\"\n",
    "        Create the transition probability of current tag and previous tag\n",
    "        P(tag i | tag i-1)\n",
    "        Args:\n",
    "            training_data (list): a list of a list of tuples with the words and POS tags\n",
    "        \"\"\"\n",
    "        #Create a temporary matrix that will do all the calculations\n",
    "        #Then store that into the self.transition_probability matrix\n",
    "        transition_counts = np.zeros(len(self.states), len(self.states))\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            #i in range of the entire sentence\n",
    "            for i in range(1, len(sentence)):\n",
    "                #Previous word and tags\n",
    "                prev_word, prev_tags = sentence[i-1]\n",
    "                #Current word and current tags\n",
    "                current_word, current_tags = sentence[i]\n",
    "                for previous_tag in prev_tags:\n",
    "                    for current_tag in current_tags:\n",
    "                        #If both the previous tag and the current tag are in the state index dicitonary\n",
    "                        if previous_tag in self.state_to_idx and current_tag in self.state_to_idx:\n",
    "                            prev_idx = self.state_to_idx[previous_tag]\n",
    "                            curr_idx = self.state_to_idx[current_tag]\n",
    "                            transition_counts[prev_idx][curr_idx] += 1 / len(prev_tags) + len(current_tags)\n",
    "    \n",
    "    def calculate_emission_probabilities(self, training_data:list):\n",
    "        \"\"\"\n",
    "        Create the emission probability of the word and tag\n",
    "        P(word | tag)\n",
    "        Args:\n",
    "            training_data (list): \n",
    "        \"\"\"\n",
    "        for sentence in training_data:\n",
    "            for word,tags in sentence:\n",
    "                #If the word exists in the observation index\n",
    "                if word in self.obs_to_idx:\n",
    "                    #Get the index of that specific word\n",
    "                    word_idx = self.obs_to_idx[word]\n",
    "                    #Handle if a word has multiple POS tags\n",
    "                    for tag in tags:\n",
    "                        #If the tag is in the state index dicitonary\n",
    "                        if tag in self.state_to_idx:\n",
    "                            tag_idx = self.state_to_idx[tag]\n",
    "                            #Fractional if there's more than one tag\n",
    "                            self.emission_probs[tag_idx][word_idx] += 1 /len(tags)\n",
    "    \n",
    "    def viterbi(self, sentence: list):\n",
    "        \"\"\" My implementation of the viterbi algorithm from the textbook\n",
    "        It returns the best path from the end of the sentence to the beginning\n",
    "        Args:\n",
    "            Sentence (list): a list of words\n",
    "        \"\"\"\n",
    "        #Intialize the viterbi matrix and the bacpointer matrix\n",
    "        viterbi = np.zeros((len(sentence), len(self.states)))\n",
    "        backpointer = np.empty((len(sentence), len(self.states)))\n",
    "       \n",
    "        #for each state s from 1 to s\n",
    "        first_word = sentence[0]\n",
    "        for state_idx in range(len(self.states)):\n",
    "            #make a viterbi matrix where viterbi[s][1] <- init_prob of that state * emission[state][observation[0]]\n",
    "            #This is if the word is known\n",
    "            if first_word in self.obs_to_idx:\n",
    "                word_idx = self.obs_to_idx[first_word]\n",
    "                #viterbi[first word][state] = initial prob of that state * emission[first word in the sentence]\n",
    "                viterbi[0][state_idx] = self.initial_probs[state_idx] * self.emission_probs[state_idx][word_idx]\n",
    "            \n",
    "            #I need a way to handle unknown words\n",
    "            else:\n",
    "                #If the word is not known, make it 0\n",
    "                viterbi[0][state_idx] = 0\n",
    "            #Backpointer for the first word. There's no previous word so make it something to denote that\n",
    "            backpointer[0][state_idx] = -1\n",
    "            \n",
    "        #Going through my sentence (after the first word)\n",
    "        for t in range(1, len(sentence)):\n",
    "            #Get the index of the current word\n",
    "            current_word = sentence[t]\n",
    "            current_word_idx = self.obs_to_idx.get(current_word)\n",
    "            #Go through every state besides the first word\n",
    "            for current_state in range(len(self.states)):\n",
    "                #Need variables to find which previous states gives us the max probability\n",
    "                max_prob = -1\n",
    "                best_prev_state = -1\n",
    "                for prev_state in range(len(self.states)):\n",
    "                    #The probability of the viterbi[previous word][previous state] * transition probability matrix[previous state][current state] * emission probability matrix[current state][word index]\n",
    "                    prob = viterbi[t-1][prev_state] * self.transition_probs[prev_state][current_state] * self.emission_probs[current_state][word_idx]\n",
    "                    \n",
    "                    \n",
    "        #Backtracking now\n",
    "        bestpathprob = np.max(viterbi[word_idx - 1][])\n",
    "        bestbathpointer = \n",
    "        bestpath = \n",
    "        return bestpath, bestpathprob\n",
    "    def generate_sequence():\n",
    "        sequence = []\n",
    "        print(\"Generated sequence: \")\n",
    "        return sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5039d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in my list to train the model\n",
    "hmm = HiddenMarkovModel()\n",
    "hmm.train_HMM(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A sample test Set for the HMM\n",
    "#A few short sentences\n",
    "test_sentence1 = [\"The\", \"cat\", \"sat\"]\n",
    "test_sentence2 = [\"This\", \"Hidden\", \"Markov\", \"Model\", \"works\", \"well\"]\n",
    "test_sentence3 = [\"I\", \"know\", \"how\", \"watch\", \"after\", \"a\", \"dog\"]\n",
    "test_sentence4 = [\"I\", \"am\", \"so\", \"tired\", \".\"]\n",
    "#A two long ones\n",
    "test_sentence_long = [\"The\", \"police\", \"department\", \"said\", \"that\", \"the\", \"suspect\", \"has\", \"been\", \"apprehended\", \"today\", \",\", \"they\", \"hope\", \"justice\", \"will\", \"be\", \"served\", \".\"]\n",
    "test_sentence_long2 = [\"Today\", \"the\", \"studio\", \"announced\", \"that\", \"the\", \"new\", \"film\", \"will\", \"be\", \"about\", \"a\", \"girl\", \"who\", \"is\", \"transported\", \"to\", \"another\", \"world\", \".\"]\n",
    "\n",
    "predicted_tags1 = hmm.viterbi(test_sentence1)\n",
    "predicted_tags2 = hmm.viterbi(test_sentence2)\n",
    "predicted_tags3 = hmm.viterbi(test_sentence3)\n",
    "predicted_tags4 = hmm.viterbi(test_sentence4)\n",
    "predicted_long_tags1 = hmm.viterbi(test_sentence_long)\n",
    "predicted_long_tags2 = hmm.viterbi(test_sentence_long2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering of the extracted_brown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14cbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8024c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I need to read in the GMB dataset\n",
    "with open('GMB_dataset.txt', 'r') as gmb_file:\n",
    "    sentence = gmb_file.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering necessary for the CRF\n",
    "def word_feature():\n",
    "    features = \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRF \n",
    "class ConditionalRandomField():\n",
    "    def __init__(self):\n",
    "        self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6427e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
